<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>üß† Explainers - My Docs</title>
        <link href="../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../css/fontawesome.min.css" rel="stylesheet">
        <link href="../../css/brands.min.css" rel="stylesheet">
        <link href="../../css/solid.min.css" rel="stylesheet">
        <link href="../../css/v4-font-face.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link id="hljs-light" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" >
        <link id="hljs-dark" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css" disabled>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../..">My Docs</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar-collapse" aria-controls="navbar-collapse" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="nav-item">
                                <a href="../.." class="nav-link">Welcome to EvalxNLP documentation!</a>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle active" aria-current="page" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Reference</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="./" class="dropdown-item active" aria-current="page">üß† Explainers</a>
</li>
                                    
<li>
    <a href="../llm_explanations/" class="dropdown-item">Llm explanations</a>
</li>
                                    
<li>
    <a href="../metrics/" class="dropdown-item">üìä Evaluation Metrics</a>
</li>
                                    
<li>
    <a href="../tasks_datasets/" class="dropdown-item">Tasks datasets</a>
</li>
                                </ul>
                            </li>
                            <li class="nav-item dropdown">
                                <a href="#" class="nav-link dropdown-toggle" role="button" data-bs-toggle="dropdown"  aria-expanded="false">Usage</a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../usage/benchmarking/" class="dropdown-item">Benchmarking</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ms-md-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-bs-toggle="modal" data-bs-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../.." class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../llm_explanations/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-bs-toggle="collapse" data-bs-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-body-tertiary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-bs-level="1"><a href="#explainers" class="nav-link">üß† Explainers</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-bs-level="2"><a href="#categories-of-explainers" class="nav-link">üß© Categories of Explainers</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#perturbation-based-methods" class="nav-link">üîÄ Perturbation-Based Methods</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#gradient-based-methods" class="nav-link">üîÅ Gradient-Based Methods</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#categories-of-explainers_1" class="nav-link">üß© Categories of Explainers</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#perturbation-based-methods_1" class="nav-link">üîÄ Perturbation-Based Methods</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#gradient-based-methods_1" class="nav-link">üîÅ Gradient-Based Methods</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h1 id="explainers">üß† Explainers</h1>
<p>One goal of <strong>EvalxNLP</strong> is to enable users to generate diverse explanations using multiple explainability methods. It integrates eight widely recognized <strong>post-hoc feature attribution (Ph-FA)</strong> explainability methods from the XAI literature.</p>
<p>These methods assign importance scores to individual features based on their contribution to the model's prediction. We focus on Ph-FA methods for their interpretability, relevance to end users, and generally efficient computational cost.</p>
<hr />
<h2 id="categories-of-explainers">üß© Categories of Explainers</h2>
<p>Feature attribution methods in EvalxNLP are categorized into:</p>
<ul>
<li><strong>Perturbation-based methods</strong></li>
<li><strong>Gradient-based methods</strong></li>
</ul>
<p>Below, we provide an overview of the methods included in each category.</p>
<hr />
<h2 id="perturbation-based-methods">üîÄ Perturbation-Based Methods</h2>
<p>Perturbation-based methods explain model predictions by altering or masking input features and observing the resulting changes in the output.</p>
<h3 id="lime">LIME</h3>
<blockquote>
<p>A model-agnostic method that explains individual predictions by training a local surrogate model.</p>
</blockquote>
<p>LIME generates explanations by perturbing the input data and measuring how the model's predictions change. Perturbed samples are weighted by their similarity to the original input. A simple, interpretable model‚Äîtypically linear‚Äîis then trained on this weighted data to approximate the local behavior of the original model. LIME aims to balance fidelity (how closely the surrogate approximates the model) and interpretability (how understandable the explanation is to humans).</p>
<h3 id="shap-partition-shap">SHAP (Partition SHAP)</h3>
<blockquote>
<p>A game-theoretic approach to explain feature contributions using Shapley values.</p>
</blockquote>
<p>SHAP computes the contribution of each input feature by evaluating all possible subsets of features, as proposed in cooperative game theory. Partition SHAP, a variant, improves efficiency by leveraging feature independence, making it scalable for high-dimensional data while preserving theoretical guarantees.</p>
<h3 id="shap-i">SHAP-I</h3>
<blockquote>
<p>An advanced extension of SHAP that captures higher-order feature interactions.</p>
</blockquote>
<p>SHAP-I extends the original SHAP method to support any-order interactions between features. It incorporates efficient algorithms to compute interaction-aware Shapley values, enabling explanation of complex dependencies in high-dimensional input data.</p>
<hr />
<h2 id="gradient-based-methods">üîÅ Gradient-Based Methods</h2>
<p>Gradient-based methods derive feature attributions by analyzing the gradients of the model's output with respect to its inputs.</p>
<h3 id="saliency">Saliency</h3>
<blockquote>
<p>A baseline technique that visualizes input feature importance using raw gradients.</p>
</blockquote>
<p>Saliency methods compute the gradient of the output with respect to each input feature. The absolute values of these gradients highlight which parts of the input most influence the model's prediction.</p>
<h3 id="gxi-gradient-input">GxI (Gradient √ó Input)</h3>
<blockquote>
<p>Enhances gradient information by incorporating input values.</p>
</blockquote>
<p>GxI multiplies the gradient of the output with respect to each input feature by the corresponding input value. This combination provides a more intuitive measure of each feature's contribution, especially for models where feature magnitudes play a critical role.</p>
<h3 id="ig-integrated-gradients">IG (Integrated Gradients)</h3>
<blockquote>
<p>A principled method that attributes importance by integrating gradients along a path from baseline to input.</p>
</blockquote>
<p>Integrated Gradients explain a model‚Äôs prediction by accumulating gradients as the input transitions from a baseline (e.g., a zero vector) to the actual input. This method satisfies desirable properties such as completeness and provides a mathematically grounded attribution of feature importance.</p>
<h3 id="dl-deeplift">DL (DeepLIFT)</h3>
<blockquote>
<p>Assigns attributions by comparing neuron activations to a reference.</p>
</blockquote>
<p>DeepLIFT computes the difference between the activations of neurons for an input and a reference baseline. It assigns credit to each input feature for the change in output, using the Rescale Rule. DeepLIFT is often more stable than raw gradients and is computationally more efficient than IG, especially for large datasets and deep architectures.</p>
<h3 id="gbp-guided-backpropagation">GBP (Guided Backpropagation)</h3>
<blockquote>
<p>Enhances interpretability by filtering gradients.</p>
</blockquote>
<p>Guided Backpropagation modifies the standard backpropagation algorithm to ignore negative gradients during the backward pass. This approach emphasizes input features that positively influence the output, resulting in sharper and more focused explanations.</p>
<p>---1~# üß† Explainers</p>
<p>One goal of <strong>EvalxNLP</strong> is to enable users to generate diverse explanations using multiple explainability methods. It integrates eight widely recognized <strong>post-hoc feature attribution (Ph-FA)</strong> explainability methods from the XAI literature.</p>
<p>These methods assign importance scores to individual features based on their contribution to the model's prediction. We focus on Ph-FA methods for their interpretability, relevance to end users, and generally efficient computational cost.</p>
<hr />
<h2 id="categories-of-explainers_1">üß© Categories of Explainers</h2>
<p>Feature attribution methods in EvalxNLP are categorized into:</p>
<ul>
<li><strong>Perturbation-based methods</strong></li>
<li><strong>Gradient-based methods</strong></li>
</ul>
<p>Below, we provide an overview of the methods included in each category.</p>
<hr />
<h2 id="perturbation-based-methods_1">üîÄ Perturbation-Based Methods</h2>
<p>Perturbation-based methods explain model predictions by altering or masking input features and observing the resulting changes in the output.</p>
<h3 id="lime_1">LIME</h3>
<blockquote>
<p>A model-agnostic method that explains individual predictions by training a local surrogate model.</p>
</blockquote>
<p>LIME generates explanations by perturbing the input data and measuring how the model's predictions change. Perturbed samples are weighted by their similarity to the original input. A simple, interpretable model‚Äîtypically linear‚Äîis then trained on this weighted data to approximate the local behavior of the original model. LIME aims to balance fidelity (how closely the surrogate approximates the model) and interpretability (how understandable the explanation is to humans).</p>
<h3 id="shap-partition-shap_1">SHAP (Partition SHAP)</h3>
<blockquote>
<p>A game-theoretic approach to explain feature contributions using Shapley values.</p>
</blockquote>
<p>SHAP computes the contribution of each input feature by evaluating all possible subsets of features, as proposed in cooperative game theory. Partition SHAP, a variant, improves efficiency by leveraging feature independence, making it scalable for high-dimensional data while preserving theoretical guarantees.</p>
<h3 id="shap-i_1">SHAP-I</h3>
<blockquote>
<p>An advanced extension of SHAP that captures higher-order feature interactions.</p>
</blockquote>
<p>SHAP-I extends the original SHAP method to support any-order interactions between features. It incorporates efficient algorithms to compute interaction-aware Shapley values, enabling explanation of complex dependencies in high-dimensional input data.</p>
<hr />
<h2 id="gradient-based-methods_1">üîÅ Gradient-Based Methods</h2>
<p>Gradient-based methods derive feature attributions by analyzing the gradients of the model's output with respect to its inputs.</p>
<h3 id="saliency_1">Saliency</h3>
<blockquote>
<p>A baseline technique that visualizes input feature importance using raw gradients.</p>
</blockquote>
<p>Saliency methods compute the gradient of the output with respect to each input feature. The absolute values of these gradients highlight which parts of the input most influence the model's prediction.</p>
<h3 id="gxi-gradient-input_1">GxI (Gradient √ó Input)</h3>
<blockquote>
<p>Enhances gradient information by incorporating input values.</p>
</blockquote>
<p>GxI multiplies the gradient of the output with respect to each input feature by the corresponding input value. This combination provides a more intuitive measure of each feature's contribution, especially for models where feature magnitudes play a critical role.</p>
<h3 id="ig-integrated-gradients_1">IG (Integrated Gradients)</h3>
<blockquote>
<p>A principled method that attributes importance by integrating gradients along a path from baseline to input.</p>
</blockquote>
<p>Integrated Gradients explain a model‚Äôs prediction by accumulating gradients as the input transitions from a baseline (e.g., a zero vector) to the actual input. This method satisfies desirable properties such as completeness and provides a mathematically grounded attribution of feature importance.</p>
<h3 id="dl-deeplift_1">DL (DeepLIFT)</h3>
<blockquote>
<p>Assigns attributions by comparing neuron activations to a reference.</p>
</blockquote>
<p>DeepLIFT computes the difference between the activations of neurons for an input and a reference baseline. It assigns credit to each input feature for the change in output, using the Rescale Rule. DeepLIFT is often more stable than raw gradients and is computationally more efficient than IG, especially for large datasets and deep architectures.</p>
<h3 id="gbp-guided-backpropagation_1">GBP (Guided Backpropagation)</h3>
<blockquote>
<p>Enhances interpretability by filtering gradients.</p>
</blockquote>
<p>Guided Backpropagation modifies the standard backpropagation algorithm to ignore negative gradients during the backward pass. This approach emphasizes input features that positively influence the output, resulting in sharper and more focused explanations.</p>
<hr /></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../../js/bootstrap.bundle.min.js"></script>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js"></script>
        <script src="../../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
