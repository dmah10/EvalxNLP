{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to EvalxNLP documentation! EvalxNLP is a Python framework for benchmarking state-of-the-art feature attribution methods for transformer-based NLP models The framework allows users to: Visualize and compare Transformers-based models output explanations using various Ph-FA methods. Use natural language text explanations from LLMs to interpret importance scores and evaluation metrics for different explainers. Evaluate effectiveness of explainers using a variety of evaluation metrics for faithfulness, plausibility, and complexity. Contents Usage Installation Explaining Benchmarking Generating LLM Explanations Advanced Features Tutorials Sentiment Analysis Hate Speech Detection Technical Reference Explainers Metrics Tasks and Datasets LLM Explanations","title":"Welcome to EvalxNLP documentation!"},{"location":"#welcome-to-evalxnlp-documentation","text":"EvalxNLP is a Python framework for benchmarking state-of-the-art feature attribution methods for transformer-based NLP models The framework allows users to: Visualize and compare Transformers-based models output explanations using various Ph-FA methods. Use natural language text explanations from LLMs to interpret importance scores and evaluation metrics for different explainers. Evaluate effectiveness of explainers using a variety of evaluation metrics for faithfulness, plausibility, and complexity.","title":"Welcome to EvalxNLP documentation!"},{"location":"#contents","text":"","title":"Contents"},{"location":"#usage","text":"Installation Explaining Benchmarking Generating LLM Explanations Advanced Features","title":"Usage"},{"location":"#tutorials","text":"Sentiment Analysis Hate Speech Detection","title":"Tutorials"},{"location":"#technical-reference","text":"Explainers Metrics Tasks and Datasets LLM Explanations","title":"Technical Reference"},{"location":"reference/explainers/","text":"\ud83e\udde0 Explainers One goal of EvalxNLP is to enable users to generate diverse explanations using multiple explainability methods. It integrates eight widely recognized post-hoc feature attribution (Ph-FA) explainability methods from the XAI literature. These methods assign importance scores to individual features based on their contribution to the model's prediction. We focus on Ph-FA methods for their interpretability, relevance to end users, and generally efficient computational cost. \ud83e\udde9 Categories of Explainers Feature attribution methods in EvalxNLP are categorized into: Perturbation-based methods Gradient-based methods Below, we provide an overview of the methods included in each category. \ud83d\udd00 Perturbation-Based Methods Perturbation-based methods explain model predictions by altering or masking input features and observing the resulting changes in the output. LIME A model-agnostic method that explains individual predictions by training a local surrogate model. LIME generates explanations by perturbing the input data and measuring how the model's predictions change. Perturbed samples are weighted by their similarity to the original input. A simple, interpretable model\u2014typically linear\u2014is then trained on this weighted data to approximate the local behavior of the original model. LIME aims to balance fidelity (how closely the surrogate approximates the model) and interpretability (how understandable the explanation is to humans). SHAP (Partition SHAP) A game-theoretic approach to explain feature contributions using Shapley values. SHAP computes the contribution of each input feature by evaluating all possible subsets of features, as proposed in cooperative game theory. Partition SHAP, a variant, improves efficiency by leveraging feature independence, making it scalable for high-dimensional data while preserving theoretical guarantees. SHAP-I An advanced extension of SHAP that captures higher-order feature interactions. SHAP-I extends the original SHAP method to support any-order interactions between features. It incorporates efficient algorithms to compute interaction-aware Shapley values, enabling explanation of complex dependencies in high-dimensional input data. \ud83d\udd01 Gradient-Based Methods Gradient-based methods derive feature attributions by analyzing the gradients of the model's output with respect to its inputs. Saliency A baseline technique that visualizes input feature importance using raw gradients. Saliency methods compute the gradient of the output with respect to each input feature. The absolute values of these gradients highlight which parts of the input most influence the model's prediction. GxI (Gradient \u00d7 Input) Enhances gradient information by incorporating input values. GxI multiplies the gradient of the output with respect to each input feature by the corresponding input value. This combination provides a more intuitive measure of each feature's contribution, especially for models where feature magnitudes play a critical role. IG (Integrated Gradients) A principled method that attributes importance by integrating gradients along a path from baseline to input. Integrated Gradients explain a model\u2019s prediction by accumulating gradients as the input transitions from a baseline (e.g., a zero vector) to the actual input. This method satisfies desirable properties such as completeness and provides a mathematically grounded attribution of feature importance. DL (DeepLIFT) Assigns attributions by comparing neuron activations to a reference. DeepLIFT computes the difference between the activations of neurons for an input and a reference baseline. It assigns credit to each input feature for the change in output, using the Rescale Rule. DeepLIFT is often more stable than raw gradients and is computationally more efficient than IG, especially for large datasets and deep architectures. GBP (Guided Backpropagation) Enhances interpretability by filtering gradients. Guided Backpropagation modifies the standard backpropagation algorithm to ignore negative gradients during the backward pass. This approach emphasizes input features that positively influence the output, resulting in sharper and more focused explanations. ---1~# \ud83e\udde0 Explainers One goal of EvalxNLP is to enable users to generate diverse explanations using multiple explainability methods. It integrates eight widely recognized post-hoc feature attribution (Ph-FA) explainability methods from the XAI literature. These methods assign importance scores to individual features based on their contribution to the model's prediction. We focus on Ph-FA methods for their interpretability, relevance to end users, and generally efficient computational cost. \ud83e\udde9 Categories of Explainers Feature attribution methods in EvalxNLP are categorized into: Perturbation-based methods Gradient-based methods Below, we provide an overview of the methods included in each category. \ud83d\udd00 Perturbation-Based Methods Perturbation-based methods explain model predictions by altering or masking input features and observing the resulting changes in the output. LIME A model-agnostic method that explains individual predictions by training a local surrogate model. LIME generates explanations by perturbing the input data and measuring how the model's predictions change. Perturbed samples are weighted by their similarity to the original input. A simple, interpretable model\u2014typically linear\u2014is then trained on this weighted data to approximate the local behavior of the original model. LIME aims to balance fidelity (how closely the surrogate approximates the model) and interpretability (how understandable the explanation is to humans). SHAP (Partition SHAP) A game-theoretic approach to explain feature contributions using Shapley values. SHAP computes the contribution of each input feature by evaluating all possible subsets of features, as proposed in cooperative game theory. Partition SHAP, a variant, improves efficiency by leveraging feature independence, making it scalable for high-dimensional data while preserving theoretical guarantees. SHAP-I An advanced extension of SHAP that captures higher-order feature interactions. SHAP-I extends the original SHAP method to support any-order interactions between features. It incorporates efficient algorithms to compute interaction-aware Shapley values, enabling explanation of complex dependencies in high-dimensional input data. \ud83d\udd01 Gradient-Based Methods Gradient-based methods derive feature attributions by analyzing the gradients of the model's output with respect to its inputs. Saliency A baseline technique that visualizes input feature importance using raw gradients. Saliency methods compute the gradient of the output with respect to each input feature. The absolute values of these gradients highlight which parts of the input most influence the model's prediction. GxI (Gradient \u00d7 Input) Enhances gradient information by incorporating input values. GxI multiplies the gradient of the output with respect to each input feature by the corresponding input value. This combination provides a more intuitive measure of each feature's contribution, especially for models where feature magnitudes play a critical role. IG (Integrated Gradients) A principled method that attributes importance by integrating gradients along a path from baseline to input. Integrated Gradients explain a model\u2019s prediction by accumulating gradients as the input transitions from a baseline (e.g., a zero vector) to the actual input. This method satisfies desirable properties such as completeness and provides a mathematically grounded attribution of feature importance. DL (DeepLIFT) Assigns attributions by comparing neuron activations to a reference. DeepLIFT computes the difference between the activations of neurons for an input and a reference baseline. It assigns credit to each input feature for the change in output, using the Rescale Rule. DeepLIFT is often more stable than raw gradients and is computationally more efficient than IG, especially for large datasets and deep architectures. GBP (Guided Backpropagation) Enhances interpretability by filtering gradients. Guided Backpropagation modifies the standard backpropagation algorithm to ignore negative gradients during the backward pass. This approach emphasizes input features that positively influence the output, resulting in sharper and more focused explanations.","title":"\ud83e\udde0 Explainers"},{"location":"reference/explainers/#explainers","text":"One goal of EvalxNLP is to enable users to generate diverse explanations using multiple explainability methods. It integrates eight widely recognized post-hoc feature attribution (Ph-FA) explainability methods from the XAI literature. These methods assign importance scores to individual features based on their contribution to the model's prediction. We focus on Ph-FA methods for their interpretability, relevance to end users, and generally efficient computational cost.","title":"\ud83e\udde0 Explainers"},{"location":"reference/explainers/#categories-of-explainers","text":"Feature attribution methods in EvalxNLP are categorized into: Perturbation-based methods Gradient-based methods Below, we provide an overview of the methods included in each category.","title":"\ud83e\udde9 Categories of Explainers"},{"location":"reference/explainers/#perturbation-based-methods","text":"Perturbation-based methods explain model predictions by altering or masking input features and observing the resulting changes in the output.","title":"\ud83d\udd00 Perturbation-Based Methods"},{"location":"reference/explainers/#lime","text":"A model-agnostic method that explains individual predictions by training a local surrogate model. LIME generates explanations by perturbing the input data and measuring how the model's predictions change. Perturbed samples are weighted by their similarity to the original input. A simple, interpretable model\u2014typically linear\u2014is then trained on this weighted data to approximate the local behavior of the original model. LIME aims to balance fidelity (how closely the surrogate approximates the model) and interpretability (how understandable the explanation is to humans).","title":"LIME"},{"location":"reference/explainers/#shap-partition-shap","text":"A game-theoretic approach to explain feature contributions using Shapley values. SHAP computes the contribution of each input feature by evaluating all possible subsets of features, as proposed in cooperative game theory. Partition SHAP, a variant, improves efficiency by leveraging feature independence, making it scalable for high-dimensional data while preserving theoretical guarantees.","title":"SHAP (Partition SHAP)"},{"location":"reference/explainers/#shap-i","text":"An advanced extension of SHAP that captures higher-order feature interactions. SHAP-I extends the original SHAP method to support any-order interactions between features. It incorporates efficient algorithms to compute interaction-aware Shapley values, enabling explanation of complex dependencies in high-dimensional input data.","title":"SHAP-I"},{"location":"reference/explainers/#gradient-based-methods","text":"Gradient-based methods derive feature attributions by analyzing the gradients of the model's output with respect to its inputs.","title":"\ud83d\udd01 Gradient-Based Methods"},{"location":"reference/explainers/#saliency","text":"A baseline technique that visualizes input feature importance using raw gradients. Saliency methods compute the gradient of the output with respect to each input feature. The absolute values of these gradients highlight which parts of the input most influence the model's prediction.","title":"Saliency"},{"location":"reference/explainers/#gxi-gradient-input","text":"Enhances gradient information by incorporating input values. GxI multiplies the gradient of the output with respect to each input feature by the corresponding input value. This combination provides a more intuitive measure of each feature's contribution, especially for models where feature magnitudes play a critical role.","title":"GxI (Gradient \u00d7 Input)"},{"location":"reference/explainers/#ig-integrated-gradients","text":"A principled method that attributes importance by integrating gradients along a path from baseline to input. Integrated Gradients explain a model\u2019s prediction by accumulating gradients as the input transitions from a baseline (e.g., a zero vector) to the actual input. This method satisfies desirable properties such as completeness and provides a mathematically grounded attribution of feature importance.","title":"IG (Integrated Gradients)"},{"location":"reference/explainers/#dl-deeplift","text":"Assigns attributions by comparing neuron activations to a reference. DeepLIFT computes the difference between the activations of neurons for an input and a reference baseline. It assigns credit to each input feature for the change in output, using the Rescale Rule. DeepLIFT is often more stable than raw gradients and is computationally more efficient than IG, especially for large datasets and deep architectures.","title":"DL (DeepLIFT)"},{"location":"reference/explainers/#gbp-guided-backpropagation","text":"Enhances interpretability by filtering gradients. Guided Backpropagation modifies the standard backpropagation algorithm to ignore negative gradients during the backward pass. This approach emphasizes input features that positively influence the output, resulting in sharper and more focused explanations. ---1~# \ud83e\udde0 Explainers One goal of EvalxNLP is to enable users to generate diverse explanations using multiple explainability methods. It integrates eight widely recognized post-hoc feature attribution (Ph-FA) explainability methods from the XAI literature. These methods assign importance scores to individual features based on their contribution to the model's prediction. We focus on Ph-FA methods for their interpretability, relevance to end users, and generally efficient computational cost.","title":"GBP (Guided Backpropagation)"},{"location":"reference/explainers/#categories-of-explainers_1","text":"Feature attribution methods in EvalxNLP are categorized into: Perturbation-based methods Gradient-based methods Below, we provide an overview of the methods included in each category.","title":"\ud83e\udde9 Categories of Explainers"},{"location":"reference/explainers/#perturbation-based-methods_1","text":"Perturbation-based methods explain model predictions by altering or masking input features and observing the resulting changes in the output.","title":"\ud83d\udd00 Perturbation-Based Methods"},{"location":"reference/explainers/#lime_1","text":"A model-agnostic method that explains individual predictions by training a local surrogate model. LIME generates explanations by perturbing the input data and measuring how the model's predictions change. Perturbed samples are weighted by their similarity to the original input. A simple, interpretable model\u2014typically linear\u2014is then trained on this weighted data to approximate the local behavior of the original model. LIME aims to balance fidelity (how closely the surrogate approximates the model) and interpretability (how understandable the explanation is to humans).","title":"LIME"},{"location":"reference/explainers/#shap-partition-shap_1","text":"A game-theoretic approach to explain feature contributions using Shapley values. SHAP computes the contribution of each input feature by evaluating all possible subsets of features, as proposed in cooperative game theory. Partition SHAP, a variant, improves efficiency by leveraging feature independence, making it scalable for high-dimensional data while preserving theoretical guarantees.","title":"SHAP (Partition SHAP)"},{"location":"reference/explainers/#shap-i_1","text":"An advanced extension of SHAP that captures higher-order feature interactions. SHAP-I extends the original SHAP method to support any-order interactions between features. It incorporates efficient algorithms to compute interaction-aware Shapley values, enabling explanation of complex dependencies in high-dimensional input data.","title":"SHAP-I"},{"location":"reference/explainers/#gradient-based-methods_1","text":"Gradient-based methods derive feature attributions by analyzing the gradients of the model's output with respect to its inputs.","title":"\ud83d\udd01 Gradient-Based Methods"},{"location":"reference/explainers/#saliency_1","text":"A baseline technique that visualizes input feature importance using raw gradients. Saliency methods compute the gradient of the output with respect to each input feature. The absolute values of these gradients highlight which parts of the input most influence the model's prediction.","title":"Saliency"},{"location":"reference/explainers/#gxi-gradient-input_1","text":"Enhances gradient information by incorporating input values. GxI multiplies the gradient of the output with respect to each input feature by the corresponding input value. This combination provides a more intuitive measure of each feature's contribution, especially for models where feature magnitudes play a critical role.","title":"GxI (Gradient \u00d7 Input)"},{"location":"reference/explainers/#ig-integrated-gradients_1","text":"A principled method that attributes importance by integrating gradients along a path from baseline to input. Integrated Gradients explain a model\u2019s prediction by accumulating gradients as the input transitions from a baseline (e.g., a zero vector) to the actual input. This method satisfies desirable properties such as completeness and provides a mathematically grounded attribution of feature importance.","title":"IG (Integrated Gradients)"},{"location":"reference/explainers/#dl-deeplift_1","text":"Assigns attributions by comparing neuron activations to a reference. DeepLIFT computes the difference between the activations of neurons for an input and a reference baseline. It assigns credit to each input feature for the change in output, using the Rescale Rule. DeepLIFT is often more stable than raw gradients and is computationally more efficient than IG, especially for large datasets and deep architectures.","title":"DL (DeepLIFT)"},{"location":"reference/explainers/#gbp-guided-backpropagation_1","text":"Enhances interpretability by filtering gradients. Guided Backpropagation modifies the standard backpropagation algorithm to ignore negative gradients during the backward pass. This approach emphasizes input features that positively influence the output, resulting in sharper and more focused explanations.","title":"GBP (Guided Backpropagation)"},{"location":"reference/llm_explanations/","text":"Feature attribution explanations, particularly those involving high-dimensional features, can be difficult for lay users to interpret. To address this, our tool integrates a module that leverages Large Language Models (LLMs) to generate natural language descriptions to help users interpret both: The importance scores from various explanation methods. The evaluation metric scores. Key Benefits Improved Interpretability : Textual descriptions bridge the gap between numerical outputs and actionable insights, helping users better understand model behavior and evaluation outcomes. User-Friendly Accessibility : Natural language explanations lower the barrier for non-technical users, making the framework more approachable without requiring deep expertise in explainability methods or an understanding of the metrics. Enhanced Usability : The combination of visual heatmaps with textual explanations offers a more accessible and holistic view of the model\u2019s decision-making process. The LLM is integrated into the framework via an API , allowing for seamless generation of textual descriptions whenever needed. This ensures the feature is both flexible and scalable , adapting to various tasks and datasets.","title":"Llm explanations"},{"location":"reference/llm_explanations/#key-benefits","text":"Improved Interpretability : Textual descriptions bridge the gap between numerical outputs and actionable insights, helping users better understand model behavior and evaluation outcomes. User-Friendly Accessibility : Natural language explanations lower the barrier for non-technical users, making the framework more approachable without requiring deep expertise in explainability methods or an understanding of the metrics. Enhanced Usability : The combination of visual heatmaps with textual explanations offers a more accessible and holistic view of the model\u2019s decision-making process. The LLM is integrated into the framework via an API , allowing for seamless generation of textual descriptions whenever needed. This ensures the feature is both flexible and scalable , adapting to various tasks and datasets.","title":"Key Benefits"},{"location":"reference/metrics/","text":"\ud83d\udcca Evaluation Metrics EvalxNLP incorporates a diverse and well-recognized set of properties and metrics from prior research to evaluate post-hoc explanation methods. These metrics are designed to assess explanation quality across three major properties: Faithfulness Plausibility Complexity Each metric is custom-implemented , following its original research paper to ensure correctness and theoretical fidelity. (\u2193)/(\u2191) indicates whether lower or higher values are better for that metric. \u2705 Faithfulness Measures how well the explanations align with the model's actual reasoning. Soft Sufficiency \u2193 Measures how well the most important tokens (based on their importance scores) can retain the model's prediction when other tokens are softly perturbed. It assumes that retaining more elements of important tokens should preserve the model's output, while dropping less important tokens should have minimal impact. A Bernoulli mask is generated, where each token is dropped with a probability proportional to its normalized importance score: $$ \\mathrm{mask} \\sim \\mathrm{Bernoulli}\\left(\\mathrm{normalized_importance_scores}\\right) $$ The Suff score is calculated as: $$ \\mathrm{Suff} = 1 - \\max\\left(0, P_{\\mathrm{full}}(y) - P_{\\mathrm{reduced}}(y)\\right) $$ Where: $P_{\\mathrm{full}}(y)$ is the model's predicted probability for the original input $P_{\\mathrm{reduced}}(y)$ is the model's predicted probability for the perturbed input The Suff score is normalized to the range $[0, 1]$ using a baseline Suff score (computed by masking all tokens): $$ \\mathrm{normalized_Suff} = \\frac{\\max\\left(0, \\mathrm{Suff} - \\mathrm{baseline_Suff}\\right)}{1 - \\mathrm{baseline_Suff}} $$ The final score is the average across all instances, with higher values indicating that the model's predictions are less affected by the perturbation of important tokens. Soft Comprehensiveness \u2191 It evaluates how much the model's prediction changes when important tokens are softly perturbed using Bernoulli mask. It assumes that heavily perturbing important tokens should significantly affect the model's output, indicating their importance to the prediction. For each instance, the importance scores of tokens are normalized to the range $[0, 1]$: $$ \\mathrm{normalized_importance_scores} = \\frac{\\mathrm{importance_scores} - \\min(\\mathrm{importance_scores})}{\\max(\\mathrm{importance_scores}) - \\min(\\mathrm{importance_scores})} $$ A Bernoulli mask is then generated, where each token is dropped with a probability proportional to $1 - \\mathrm{normalized_importance_scores}$: $$ \\mathrm{mask} \\sim \\mathrm{Bernoulli}\\left(1 - \\mathrm{normalized_importance_scores}\\right) $$ This mask is applied to the token embeddings, creating a perturbed input. The $\\mathrm{Comp}$ score is calculated as the difference between the model's confidence in the original prediction and its confidence after perturbation: $$ \\mathrm{Comp} = \\max\\left(0, P_{\\mathrm{original}}(y) - P_{\\mathrm{perturbed}}(y)\\right) $$ where: $P_{\\mathrm{original}}(y)$ is the model's predicted probability for the original input $P_{\\mathrm{perturbed}}(y)$ is the model's predicted probability for the perturbed input The final $\\mathrm{Comp}$ score is the average across all instances, with higher values indicating that the model relies more heavily on the important tokens. FAD curve N-AUC \u2193 Measures the impact of dropping the most salient tokens on model performance, with the steepness of the curve indicating the method's faithfulness. The N-AUC quantifies this steepness, where a higher score reflects better alignment of the attribution method with the model's true feature importance. For each instance, tokens are progressively dropped based on their saliency scores and the model's accuracy is evaluated at different drop percentages (e.g., 0%, 10%, ..., 40%). The saliency scores determine which tokens to replace with a baseline token ( [MASK] ). The N-AUC is computed over a specified percentage range (0% to 20%) using the trapezoidal rule: $$ \\mathrm{N\\text{-}AUC} = \\frac{\\mathrm{AUC}}{\\mathrm{max_AUC}} $$ where: $\\mathrm{AUC}$ is the area under the accuracy curve, computed as: $$ \\mathrm{AUC} = \\int_{x_{\\min}}^{x_{\\max}} y(x) \\, dx $$ where $x$ represents the percentage of tokens dropped, and $y(x)$ represents the model's accuracy at that percentage. $\\mathrm{max_AUC}$ is the maximum possible area, calculated as: $$ \\mathrm{max_AUC} = (x_{\\max} - x_{\\min}) \\times \\max(y) $$ where $x_{\\min}$ and $x_{\\max}$ are the lower and upper bounds of the percentage range, and $\\max(y)$ is the highest accuracy value in the range. This metric quantifies how much model performance degrades when salient tokens are removed, with lower N-AUC indicating greater reliance on the dropped tokens. AUTPC \u2193 Area Under the Token Perturbation Curve evaluates the faithfulness of saliency explanations by progressively masking the most important tokens (based on their saliency scores) and measuring the drop in the model's performance. Masking involves replacing tokens with a baseline value (typically [MASK] or a zero vector). Starting with 0% masking and incrementally increasing to 100%, the model\u2019s accuracy is computed at each threshold. The resulting performance curve, plotting accuracy against the percentage of tokens masked, is used to calculate the area under the curve (AUC). This AUTPC value, normalized to the range [0, 1], provides a single metric summarizing how significantly the model relies on the highlighted tokens, with higher values indicating more faithful explanations. A smaller AUC indicates that removing critical features degrades performance faster, demonstrating more faithful explanations. Results are standardized by the number of features for comparability. \ud83d\udca1 Plausibility Measures how well explanations align with human-annotated rationales. IOU-F1 Score \u2191 It evaluates the alignment between predicted rationales and ground truth rationales at the span level using the Intersection over Union (IOU) metric. For the i-th instance with predicted rationale $S_i^p$ and ground truth rationale $S_i^g$, the IOU is calculated as: $$ S_i = \\frac{|S_i^p \\cap S_i^g|}{|S_i^p \\cup S_i^g|} $$ A rationale is considered a match when $S_i \\geq 0.5$. The IOU-F1 score aggregates these matches across all $N$ instances: $$ \\mathrm{IOU\\text{-}F1} = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}(S_i \\geq 0.5) $$ where $\\mathbb{I}$ is the indicator function: $$ \\mathbb{I}(S_i \\geq 0.5) = \\begin{cases} 1 & \\text{if } S_i \\geq 0.5 \\ 0 & \\text{otherwise} \\end{cases} $$ This metric ranges from 0 to 1, with higher values indicating better alignment between predicted and ground truth rationales. The 0.5 threshold ensures that only substantially overlapping spans are counted as matches. Token-level F1 Score \u2191 The Token F1-score measures the alignment between predicted rationales and ground truth rationales by computing the F1-score, which balances precision and recall. For each instance, the predicted rationale $S_i^p$ is compared to the ground truth rationale $S_i^g$, and the F1-score is calculated based on the overlap of tokens. The F1-score for the i-th instance is defined as: $$ F1_i = \\frac{2 \\times P_i \\times R_i}{P_i + R_i} $$ where: - Precision $P_i$: $$ P_i = \\frac{|S_i^p \\cap S_i^g|}{|S_i^p|} $$ - Recall $R_i$: $$ R_i = \\frac{|S_i^p \\cap S_i^g|}{|S_i^g|} $$ The final Token F1-score is the average F1-score across all $N$ instances: $$ \\mathrm{Token\\text{-}F1} = \\frac{1}{N} \\sum_{i=1}^N F1_i $$ For both the Token-F1 score and IOU-F1 score, the top $K$ tokens with positive influence are selected, where $K$ is the average length of the human rationale for the dataset. AUPRC \u2191 For each instance, the saliency scores are compared to the ground truth rationale mask. The precision-recall curve is computed, and then the area under this curve is calculated. The final AUPRC score is the average of these values across all instances, providing a single metric that quantifies the alignment between predicted saliency and human-annotated rationales, with higher scores indicating better performance. \ud83d\udd00 Complexity Measures how interpretable the explanation is by checking sparsity and token focus. Complexity \u2193 It measures the complexity of token-level attributions using Shannon entropy, which quantifies how evenly the importance scores are distributed across tokens. For each instance, the fractional contribution of each token is computed as: $$ f_j = \\frac{|a_j|}{\\sum_{k=1}^n |a_k|} $$ where: - $a_j$ is the saliency score of the $j$-th token - $n$ is the total number of tokens The complexity score for the instance is then calculated as the entropy of the fractional contributions: $$ \\mathrm{Complexity} = -\\frac{1}{n}\\sum_{j=1}^n f_j \\cdot \\log(f_j + \\epsilon) $$ where $\\epsilon$ is a small constant (e.g., $10^{-8}$) to avoid numerical instability. The final complexity score is the average across all instances, with higher values indicating more evenly distributed attributions (higher complexity) and lower values indicating more concentrated attributions (lower complexity). Sparseness \u2191 It measures the sparsity of model attributions using the Gini index, which quantifies how concentrated the importance scores are across features. For each instance, the absolute values of the attributions are sorted in ascending order, and the Gini index is computed as: $$ \\mathrm{Sparseness} = 1 - 2 \\cdot \\frac{\\sum_{j=1}^n (n - j + 0.5) \\cdot |a_j|}{\\left(\\sum_{j=1}^n |a_j|\\right) \\cdot n} $$ where: - $a_j$ is the $j$-th sorted attribution value - $n$ is the total number of features (tokens) - $|a_j|$ is the absolute value of the $j$-th attribution The sparseness score ranges from 0 to 1, where: - 0 indicates dense attributions (importance is evenly distributed across features) - 1 indicates sparse attributions (importance is concentrated on a few features) The final sparseness score is the average across all instances, providing a single metric to evaluate how focused the model is on specific features.","title":"\ud83d\udcca Evaluation Metrics"},{"location":"reference/metrics/#evaluation-metrics","text":"EvalxNLP incorporates a diverse and well-recognized set of properties and metrics from prior research to evaluate post-hoc explanation methods. These metrics are designed to assess explanation quality across three major properties: Faithfulness Plausibility Complexity Each metric is custom-implemented , following its original research paper to ensure correctness and theoretical fidelity. (\u2193)/(\u2191) indicates whether lower or higher values are better for that metric.","title":"\ud83d\udcca Evaluation Metrics"},{"location":"reference/metrics/#faithfulness","text":"Measures how well the explanations align with the model's actual reasoning.","title":"\u2705 Faithfulness"},{"location":"reference/metrics/#soft-sufficiency","text":"Measures how well the most important tokens (based on their importance scores) can retain the model's prediction when other tokens are softly perturbed. It assumes that retaining more elements of important tokens should preserve the model's output, while dropping less important tokens should have minimal impact. A Bernoulli mask is generated, where each token is dropped with a probability proportional to its normalized importance score: $$ \\mathrm{mask} \\sim \\mathrm{Bernoulli}\\left(\\mathrm{normalized_importance_scores}\\right) $$ The Suff score is calculated as: $$ \\mathrm{Suff} = 1 - \\max\\left(0, P_{\\mathrm{full}}(y) - P_{\\mathrm{reduced}}(y)\\right) $$ Where: $P_{\\mathrm{full}}(y)$ is the model's predicted probability for the original input $P_{\\mathrm{reduced}}(y)$ is the model's predicted probability for the perturbed input The Suff score is normalized to the range $[0, 1]$ using a baseline Suff score (computed by masking all tokens): $$ \\mathrm{normalized_Suff} = \\frac{\\max\\left(0, \\mathrm{Suff} - \\mathrm{baseline_Suff}\\right)}{1 - \\mathrm{baseline_Suff}} $$ The final score is the average across all instances, with higher values indicating that the model's predictions are less affected by the perturbation of important tokens.","title":"Soft Sufficiency \u2193"},{"location":"reference/metrics/#soft-comprehensiveness","text":"It evaluates how much the model's prediction changes when important tokens are softly perturbed using Bernoulli mask. It assumes that heavily perturbing important tokens should significantly affect the model's output, indicating their importance to the prediction. For each instance, the importance scores of tokens are normalized to the range $[0, 1]$: $$ \\mathrm{normalized_importance_scores} = \\frac{\\mathrm{importance_scores} - \\min(\\mathrm{importance_scores})}{\\max(\\mathrm{importance_scores}) - \\min(\\mathrm{importance_scores})} $$ A Bernoulli mask is then generated, where each token is dropped with a probability proportional to $1 - \\mathrm{normalized_importance_scores}$: $$ \\mathrm{mask} \\sim \\mathrm{Bernoulli}\\left(1 - \\mathrm{normalized_importance_scores}\\right) $$ This mask is applied to the token embeddings, creating a perturbed input. The $\\mathrm{Comp}$ score is calculated as the difference between the model's confidence in the original prediction and its confidence after perturbation: $$ \\mathrm{Comp} = \\max\\left(0, P_{\\mathrm{original}}(y) - P_{\\mathrm{perturbed}}(y)\\right) $$ where: $P_{\\mathrm{original}}(y)$ is the model's predicted probability for the original input $P_{\\mathrm{perturbed}}(y)$ is the model's predicted probability for the perturbed input The final $\\mathrm{Comp}$ score is the average across all instances, with higher values indicating that the model relies more heavily on the important tokens.","title":"Soft Comprehensiveness \u2191"},{"location":"reference/metrics/#fad-curve-n-auc","text":"Measures the impact of dropping the most salient tokens on model performance, with the steepness of the curve indicating the method's faithfulness. The N-AUC quantifies this steepness, where a higher score reflects better alignment of the attribution method with the model's true feature importance. For each instance, tokens are progressively dropped based on their saliency scores and the model's accuracy is evaluated at different drop percentages (e.g., 0%, 10%, ..., 40%). The saliency scores determine which tokens to replace with a baseline token ( [MASK] ). The N-AUC is computed over a specified percentage range (0% to 20%) using the trapezoidal rule: $$ \\mathrm{N\\text{-}AUC} = \\frac{\\mathrm{AUC}}{\\mathrm{max_AUC}} $$ where: $\\mathrm{AUC}$ is the area under the accuracy curve, computed as: $$ \\mathrm{AUC} = \\int_{x_{\\min}}^{x_{\\max}} y(x) \\, dx $$ where $x$ represents the percentage of tokens dropped, and $y(x)$ represents the model's accuracy at that percentage. $\\mathrm{max_AUC}$ is the maximum possible area, calculated as: $$ \\mathrm{max_AUC} = (x_{\\max} - x_{\\min}) \\times \\max(y) $$ where $x_{\\min}$ and $x_{\\max}$ are the lower and upper bounds of the percentage range, and $\\max(y)$ is the highest accuracy value in the range. This metric quantifies how much model performance degrades when salient tokens are removed, with lower N-AUC indicating greater reliance on the dropped tokens.","title":"FAD curve N-AUC \u2193"},{"location":"reference/metrics/#autpc","text":"Area Under the Token Perturbation Curve evaluates the faithfulness of saliency explanations by progressively masking the most important tokens (based on their saliency scores) and measuring the drop in the model's performance. Masking involves replacing tokens with a baseline value (typically [MASK] or a zero vector). Starting with 0% masking and incrementally increasing to 100%, the model\u2019s accuracy is computed at each threshold. The resulting performance curve, plotting accuracy against the percentage of tokens masked, is used to calculate the area under the curve (AUC). This AUTPC value, normalized to the range [0, 1], provides a single metric summarizing how significantly the model relies on the highlighted tokens, with higher values indicating more faithful explanations. A smaller AUC indicates that removing critical features degrades performance faster, demonstrating more faithful explanations. Results are standardized by the number of features for comparability.","title":"AUTPC \u2193"},{"location":"reference/metrics/#plausibility","text":"Measures how well explanations align with human-annotated rationales.","title":"\ud83d\udca1 Plausibility"},{"location":"reference/metrics/#iou-f1-score","text":"It evaluates the alignment between predicted rationales and ground truth rationales at the span level using the Intersection over Union (IOU) metric. For the i-th instance with predicted rationale $S_i^p$ and ground truth rationale $S_i^g$, the IOU is calculated as: $$ S_i = \\frac{|S_i^p \\cap S_i^g|}{|S_i^p \\cup S_i^g|} $$ A rationale is considered a match when $S_i \\geq 0.5$. The IOU-F1 score aggregates these matches across all $N$ instances: $$ \\mathrm{IOU\\text{-}F1} = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}(S_i \\geq 0.5) $$ where $\\mathbb{I}$ is the indicator function: $$ \\mathbb{I}(S_i \\geq 0.5) = \\begin{cases} 1 & \\text{if } S_i \\geq 0.5 \\ 0 & \\text{otherwise} \\end{cases} $$ This metric ranges from 0 to 1, with higher values indicating better alignment between predicted and ground truth rationales. The 0.5 threshold ensures that only substantially overlapping spans are counted as matches.","title":"IOU-F1 Score \u2191"},{"location":"reference/metrics/#token-level-f1-score","text":"The Token F1-score measures the alignment between predicted rationales and ground truth rationales by computing the F1-score, which balances precision and recall. For each instance, the predicted rationale $S_i^p$ is compared to the ground truth rationale $S_i^g$, and the F1-score is calculated based on the overlap of tokens. The F1-score for the i-th instance is defined as: $$ F1_i = \\frac{2 \\times P_i \\times R_i}{P_i + R_i} $$ where: - Precision $P_i$: $$ P_i = \\frac{|S_i^p \\cap S_i^g|}{|S_i^p|} $$ - Recall $R_i$: $$ R_i = \\frac{|S_i^p \\cap S_i^g|}{|S_i^g|} $$ The final Token F1-score is the average F1-score across all $N$ instances: $$ \\mathrm{Token\\text{-}F1} = \\frac{1}{N} \\sum_{i=1}^N F1_i $$ For both the Token-F1 score and IOU-F1 score, the top $K$ tokens with positive influence are selected, where $K$ is the average length of the human rationale for the dataset.","title":"Token-level F1 Score \u2191"},{"location":"reference/metrics/#auprc","text":"For each instance, the saliency scores are compared to the ground truth rationale mask. The precision-recall curve is computed, and then the area under this curve is calculated. The final AUPRC score is the average of these values across all instances, providing a single metric that quantifies the alignment between predicted saliency and human-annotated rationales, with higher scores indicating better performance.","title":"AUPRC \u2191"},{"location":"reference/metrics/#complexity","text":"Measures how interpretable the explanation is by checking sparsity and token focus.","title":"\ud83d\udd00 Complexity"},{"location":"reference/metrics/#complexity_1","text":"It measures the complexity of token-level attributions using Shannon entropy, which quantifies how evenly the importance scores are distributed across tokens. For each instance, the fractional contribution of each token is computed as: $$ f_j = \\frac{|a_j|}{\\sum_{k=1}^n |a_k|} $$ where: - $a_j$ is the saliency score of the $j$-th token - $n$ is the total number of tokens The complexity score for the instance is then calculated as the entropy of the fractional contributions: $$ \\mathrm{Complexity} = -\\frac{1}{n}\\sum_{j=1}^n f_j \\cdot \\log(f_j + \\epsilon) $$ where $\\epsilon$ is a small constant (e.g., $10^{-8}$) to avoid numerical instability. The final complexity score is the average across all instances, with higher values indicating more evenly distributed attributions (higher complexity) and lower values indicating more concentrated attributions (lower complexity).","title":"Complexity \u2193"},{"location":"reference/metrics/#sparseness","text":"It measures the sparsity of model attributions using the Gini index, which quantifies how concentrated the importance scores are across features. For each instance, the absolute values of the attributions are sorted in ascending order, and the Gini index is computed as: $$ \\mathrm{Sparseness} = 1 - 2 \\cdot \\frac{\\sum_{j=1}^n (n - j + 0.5) \\cdot |a_j|}{\\left(\\sum_{j=1}^n |a_j|\\right) \\cdot n} $$ where: - $a_j$ is the $j$-th sorted attribution value - $n$ is the total number of features (tokens) - $|a_j|$ is the absolute value of the $j$-th attribution The sparseness score ranges from 0 to 1, where: - 0 indicates dense attributions (importance is evenly distributed across features) - 1 indicates sparse attributions (importance is concentrated on a few features) The final sparseness score is the average across all instances, providing a single metric to evaluate how focused the model is on specific features.","title":"Sparseness \u2191"},{"location":"reference/tasks_datasets/","text":"The framework currently supports the following text classification tasks: \ud83e\udde0 Supported Tasks \ud83d\udcac Sentiment Analysis Determines the sentiment expressed in a text, such as positive , negative , or neutral . Commonly used in applications like product reviews, social media analysis, and customer feedback. \ud83d\udeab Hate Speech Detection Identifies and classifies text containing hate speech , offensive language , or harmful content . Essential for moderating online platforms and ensuring safe digital environments. \ud83d\udd17 Natural Language Inference (NLI) Determines the logical relationship between two sentences (e.g., premise and hypothesis). Tasks include identifying entailment , contradiction , or neutrality between sentences. Useful for applications like question answering, summarization, and reasoning tasks. \ud83d\udcda Dataset Support EvalxNLP includes a representative dataset for each of the supported tasks and allows users to extend the framework with additional classification datasets. All datasets are rationale-annotated , meaning they include human-annotated rationales that highlight the most important words or sentences for a given class label. These rationales enable the evaluation of alignment between model explanations and human understanding. \ud83c\udfac MovieReviews Task: Sentiment Analysis Description: Contains 1,000 positive and 1,000 negative movie reviews. Each review includes phrase-level human-annotated rationales that justify the sentiment label. \ud83d\udce2 HateXplain Task: Hate Speech Detection Description: Comprises 20,000 posts from Gab and Twitter, annotated with one of three labels: hate speech , offensive , or normal . \ud83d\udcc4 e-SNLI Task: Natural Language Inference Description: Contains 549,367 examples split into training, validation, and test sets. Each example includes a premise and a hypothesis , annotated with one of three labels: entailment , contradiction , or neutral .","title":"Tasks datasets"},{"location":"reference/tasks_datasets/#supported-tasks","text":"","title":"\ud83e\udde0 Supported Tasks"},{"location":"reference/tasks_datasets/#sentiment-analysis","text":"Determines the sentiment expressed in a text, such as positive , negative , or neutral . Commonly used in applications like product reviews, social media analysis, and customer feedback.","title":"\ud83d\udcac Sentiment Analysis"},{"location":"reference/tasks_datasets/#hate-speech-detection","text":"Identifies and classifies text containing hate speech , offensive language , or harmful content . Essential for moderating online platforms and ensuring safe digital environments.","title":"\ud83d\udeab Hate Speech Detection"},{"location":"reference/tasks_datasets/#natural-language-inference-nli","text":"Determines the logical relationship between two sentences (e.g., premise and hypothesis). Tasks include identifying entailment , contradiction , or neutrality between sentences. Useful for applications like question answering, summarization, and reasoning tasks.","title":"\ud83d\udd17 Natural Language Inference (NLI)"},{"location":"reference/tasks_datasets/#dataset-support","text":"EvalxNLP includes a representative dataset for each of the supported tasks and allows users to extend the framework with additional classification datasets. All datasets are rationale-annotated , meaning they include human-annotated rationales that highlight the most important words or sentences for a given class label. These rationales enable the evaluation of alignment between model explanations and human understanding.","title":"\ud83d\udcda Dataset Support"},{"location":"reference/tasks_datasets/#moviereviews","text":"Task: Sentiment Analysis Description: Contains 1,000 positive and 1,000 negative movie reviews. Each review includes phrase-level human-annotated rationales that justify the sentiment label.","title":"\ud83c\udfac MovieReviews"},{"location":"reference/tasks_datasets/#hatexplain","text":"Task: Hate Speech Detection Description: Comprises 20,000 posts from Gab and Twitter, annotated with one of three labels: hate speech , offensive , or normal .","title":"\ud83d\udce2 HateXplain"},{"location":"reference/tasks_datasets/#e-snli","text":"Task: Natural Language Inference Description: Contains 549,367 examples split into training, validation, and test sets. Each example includes a premise and a hypothesis , annotated with one of three labels: entailment , contradiction , or neutral .","title":"\ud83d\udcc4 e-SNLI"},{"location":"usage/benchmarking/","text":"EvalxNLP allows users to evaluate the quality and reliability of generated explanations and compare multiple explainers across plausibility and faithfulness metrics. \ud83e\uddea Evaluating a Single Sentence Step 1: Evaluate Explanations Use the evaluate_single_sentence() method from the XAIFramework class to evaluate explanations for a single sentence. You may also provide a human rationale to enable plausibility metric evaluation. Human Rationale Human rationales are binary lists indicating which tokens are important for the target label. A value of 1 marks a token as important , and 0 as not important . If no rationale is provided, plausibility metrics will be skipped . example = \"Worst experience I've ever had!!\" label = \"negative\" human_rationale = [0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0] # Binary rationale metrics = xai_framework.evaluate_single_sentence( sentence=example, target_label=label, human_rationale=human_rationale ) Step 2: View Results Visualize the evaluation metrics in a tabular format using the create_pivot_table() function: xai_framework.create_pivot_table(metrics) Metrics are color-coded by property. Darker hues indicate stronger performance. The best-performing score for each metric is bolded for easier comparison. \ud83e\uddea Evaluating a Dataset Step 1: Compute Evaluation Metrics Use the compute_evaluation_metrics() method to calculate explanation quality across a dataset. It takes a list of explanation objects as input (e.g., from get_feature_importance_for_dataset() ). metrics = xai_framework.compute_evaluation_metrics(exp_scores) Step 2: Visualize Results Visualize the dataset-level evaluation using the same pivot table function: xai_framework.create_pivot_table(metrics)","title":"Benchmarking"},{"location":"usage/benchmarking/#evaluating-a-single-sentence","text":"","title":"\ud83e\uddea Evaluating a Single Sentence"},{"location":"usage/benchmarking/#step-1-evaluate-explanations","text":"Use the evaluate_single_sentence() method from the XAIFramework class to evaluate explanations for a single sentence. You may also provide a human rationale to enable plausibility metric evaluation.","title":"Step 1: Evaluate Explanations"},{"location":"usage/benchmarking/#human-rationale","text":"Human rationales are binary lists indicating which tokens are important for the target label. A value of 1 marks a token as important , and 0 as not important . If no rationale is provided, plausibility metrics will be skipped . example = \"Worst experience I've ever had!!\" label = \"negative\" human_rationale = [0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0] # Binary rationale metrics = xai_framework.evaluate_single_sentence( sentence=example, target_label=label, human_rationale=human_rationale )","title":"Human Rationale"},{"location":"usage/benchmarking/#step-2-view-results","text":"Visualize the evaluation metrics in a tabular format using the create_pivot_table() function: xai_framework.create_pivot_table(metrics) Metrics are color-coded by property. Darker hues indicate stronger performance. The best-performing score for each metric is bolded for easier comparison.","title":"Step 2: View Results"},{"location":"usage/benchmarking/#evaluating-a-dataset","text":"","title":"\ud83e\uddea Evaluating a Dataset"},{"location":"usage/benchmarking/#step-1-compute-evaluation-metrics","text":"Use the compute_evaluation_metrics() method to calculate explanation quality across a dataset. It takes a list of explanation objects as input (e.g., from get_feature_importance_for_dataset() ). metrics = xai_framework.compute_evaluation_metrics(exp_scores)","title":"Step 1: Compute Evaluation Metrics"},{"location":"usage/benchmarking/#step-2-visualize-results","text":"Visualize the dataset-level evaluation using the same pivot table function: xai_framework.create_pivot_table(metrics)","title":"Step 2: Visualize Results"}]}