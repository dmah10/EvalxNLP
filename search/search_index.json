{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to EvalxNLP documentation! EvalxNLP is a Python framework for benchmarking state-of-the-art feature attribution methods for transformer-based NLP models The framework allows users to: Visualize and compare Transformers-based models output explanations using various Ph-FA methods. Use natural language text explanations from LLMs to interpret importance scores and evaluation metrics for different explainers. Evaluate effectiveness of explainers using a variety of evaluation metrics for faithfulness, plausibility, and complexity. Contents Usage Installation Explaining Benchmarking Generating LLM Explanations Advanced Features Tutorials Sentiment Analysis Hate Speech Detection Technical Reference Explainers Metrics Tasks and Datasets LLM Explanations","title":"Welcome to EvalxNLP documentation!"},{"location":"#welcome-to-evalxnlp-documentation","text":"EvalxNLP is a Python framework for benchmarking state-of-the-art feature attribution methods for transformer-based NLP models The framework allows users to: Visualize and compare Transformers-based models output explanations using various Ph-FA methods. Use natural language text explanations from LLMs to interpret importance scores and evaluation metrics for different explainers. Evaluate effectiveness of explainers using a variety of evaluation metrics for faithfulness, plausibility, and complexity.","title":"Welcome to EvalxNLP documentation!"},{"location":"#contents","text":"","title":"Contents"},{"location":"#usage","text":"Installation Explaining Benchmarking Generating LLM Explanations Advanced Features","title":"Usage"},{"location":"#tutorials","text":"Sentiment Analysis Hate Speech Detection","title":"Tutorials"},{"location":"#technical-reference","text":"Explainers Metrics Tasks and Datasets LLM Explanations","title":"Technical Reference"},{"location":"reference/explainers/","text":"\ud83e\udde0 Explainers One goal of EvalxNLP is to enable users to generate diverse explanations using multiple explainability methods. It integrates eight widely recognized post-hoc feature attribution (Ph-FA) explainability methods from the XAI literature. These methods assign importance scores to individual features based on their contribution to the model's prediction. We focus on Ph-FA methods for their interpretability, relevance to end users, and generally efficient computational cost. \ud83e\udde9 Categories of Explainers Feature attribution methods in EvalxNLP are categorized into: Perturbation-based methods Gradient-based methods Below, we provide an overview of the methods included in each category. \ud83d\udd00 Perturbation-Based Methods Perturbation-based methods explain model predictions by altering or masking input features and observing the resulting changes in the output. LIME A model-agnostic method that explains individual predictions by training a local surrogate model. LIME generates explanations by perturbing the input data and measuring how the model's predictions change. Perturbed samples are weighted by their similarity to the original input. A simple, interpretable model\u2014typically linear\u2014is then trained on this weighted data to approximate the local behavior of the original model. LIME aims to balance fidelity (how closely the surrogate approximates the model) and interpretability (how understandable the explanation is to humans). SHAP (Partition SHAP) A game-theoretic approach to explain feature contributions using Shapley values. SHAP computes the contribution of each input feature by evaluating all possible subsets of features, as proposed in cooperative game theory. Partition SHAP, a variant, improves efficiency by leveraging feature independence, making it scalable for high-dimensional data while preserving theoretical guarantees. SHAP-I An advanced extension of SHAP that captures higher-order feature interactions. SHAP-I extends the original SHAP method to support any-order interactions between features. It incorporates efficient algorithms to compute interaction-aware Shapley values, enabling explanation of complex dependencies in high-dimensional input data. \ud83d\udd01 Gradient-Based Methods Gradient-based methods derive feature attributions by analyzing the gradients of the model's output with respect to its inputs. Saliency A baseline technique that visualizes input feature importance using raw gradients. Saliency methods compute the gradient of the output with respect to each input feature. The absolute values of these gradients highlight which parts of the input most influence the model's prediction. GxI (Gradient \u00d7 Input) Enhances gradient information by incorporating input values. GxI multiplies the gradient of the output with respect to each input feature by the corresponding input value. This combination provides a more intuitive measure of each feature's contribution, especially for models where feature magnitudes play a critical role. IG (Integrated Gradients) A principled method that attributes importance by integrating gradients along a path from baseline to input. Integrated Gradients explain a model\u2019s prediction by accumulating gradients as the input transitions from a baseline (e.g., a zero vector) to the actual input. This method satisfies desirable properties such as completeness and provides a mathematically grounded attribution of feature importance. DL (DeepLIFT) Assigns attributions by comparing neuron activations to a reference. DeepLIFT computes the difference between the activations of neurons for an input and a reference baseline. It assigns credit to each input feature for the change in output, using the Rescale Rule. DeepLIFT is often more stable than raw gradients and is computationally more efficient than IG, especially for large datasets and deep architectures. GBP (Guided Backpropagation) Enhances interpretability by filtering gradients. Guided Backpropagation modifies the standard backpropagation algorithm to ignore negative gradients during the backward pass. This approach emphasizes input features that positively influence the output, resulting in sharper and more focused explanations. ---1~# \ud83e\udde0 Explainers One goal of EvalxNLP is to enable users to generate diverse explanations using multiple explainability methods. It integrates eight widely recognized post-hoc feature attribution (Ph-FA) explainability methods from the XAI literature. These methods assign importance scores to individual features based on their contribution to the model's prediction. We focus on Ph-FA methods for their interpretability, relevance to end users, and generally efficient computational cost. \ud83e\udde9 Categories of Explainers Feature attribution methods in EvalxNLP are categorized into: Perturbation-based methods Gradient-based methods Below, we provide an overview of the methods included in each category. \ud83d\udd00 Perturbation-Based Methods Perturbation-based methods explain model predictions by altering or masking input features and observing the resulting changes in the output. LIME A model-agnostic method that explains individual predictions by training a local surrogate model. LIME generates explanations by perturbing the input data and measuring how the model's predictions change. Perturbed samples are weighted by their similarity to the original input. A simple, interpretable model\u2014typically linear\u2014is then trained on this weighted data to approximate the local behavior of the original model. LIME aims to balance fidelity (how closely the surrogate approximates the model) and interpretability (how understandable the explanation is to humans). SHAP (Partition SHAP) A game-theoretic approach to explain feature contributions using Shapley values. SHAP computes the contribution of each input feature by evaluating all possible subsets of features, as proposed in cooperative game theory. Partition SHAP, a variant, improves efficiency by leveraging feature independence, making it scalable for high-dimensional data while preserving theoretical guarantees. SHAP-I An advanced extension of SHAP that captures higher-order feature interactions. SHAP-I extends the original SHAP method to support any-order interactions between features. It incorporates efficient algorithms to compute interaction-aware Shapley values, enabling explanation of complex dependencies in high-dimensional input data. \ud83d\udd01 Gradient-Based Methods Gradient-based methods derive feature attributions by analyzing the gradients of the model's output with respect to its inputs. Saliency A baseline technique that visualizes input feature importance using raw gradients. Saliency methods compute the gradient of the output with respect to each input feature. The absolute values of these gradients highlight which parts of the input most influence the model's prediction. GxI (Gradient \u00d7 Input) Enhances gradient information by incorporating input values. GxI multiplies the gradient of the output with respect to each input feature by the corresponding input value. This combination provides a more intuitive measure of each feature's contribution, especially for models where feature magnitudes play a critical role. IG (Integrated Gradients) A principled method that attributes importance by integrating gradients along a path from baseline to input. Integrated Gradients explain a model\u2019s prediction by accumulating gradients as the input transitions from a baseline (e.g., a zero vector) to the actual input. This method satisfies desirable properties such as completeness and provides a mathematically grounded attribution of feature importance. DL (DeepLIFT) Assigns attributions by comparing neuron activations to a reference. DeepLIFT computes the difference between the activations of neurons for an input and a reference baseline. It assigns credit to each input feature for the change in output, using the Rescale Rule. DeepLIFT is often more stable than raw gradients and is computationally more efficient than IG, especially for large datasets and deep architectures. GBP (Guided Backpropagation) Enhances interpretability by filtering gradients. Guided Backpropagation modifies the standard backpropagation algorithm to ignore negative gradients during the backward pass. This approach emphasizes input features that positively influence the output, resulting in sharper and more focused explanations.","title":"\ud83e\udde0 Explainers"},{"location":"reference/explainers/#explainers","text":"One goal of EvalxNLP is to enable users to generate diverse explanations using multiple explainability methods. It integrates eight widely recognized post-hoc feature attribution (Ph-FA) explainability methods from the XAI literature. These methods assign importance scores to individual features based on their contribution to the model's prediction. We focus on Ph-FA methods for their interpretability, relevance to end users, and generally efficient computational cost.","title":"\ud83e\udde0 Explainers"},{"location":"reference/explainers/#categories-of-explainers","text":"Feature attribution methods in EvalxNLP are categorized into: Perturbation-based methods Gradient-based methods Below, we provide an overview of the methods included in each category.","title":"\ud83e\udde9 Categories of Explainers"},{"location":"reference/explainers/#perturbation-based-methods","text":"Perturbation-based methods explain model predictions by altering or masking input features and observing the resulting changes in the output.","title":"\ud83d\udd00 Perturbation-Based Methods"},{"location":"reference/explainers/#lime","text":"A model-agnostic method that explains individual predictions by training a local surrogate model. LIME generates explanations by perturbing the input data and measuring how the model's predictions change. Perturbed samples are weighted by their similarity to the original input. A simple, interpretable model\u2014typically linear\u2014is then trained on this weighted data to approximate the local behavior of the original model. LIME aims to balance fidelity (how closely the surrogate approximates the model) and interpretability (how understandable the explanation is to humans).","title":"LIME"},{"location":"reference/explainers/#shap-partition-shap","text":"A game-theoretic approach to explain feature contributions using Shapley values. SHAP computes the contribution of each input feature by evaluating all possible subsets of features, as proposed in cooperative game theory. Partition SHAP, a variant, improves efficiency by leveraging feature independence, making it scalable for high-dimensional data while preserving theoretical guarantees.","title":"SHAP (Partition SHAP)"},{"location":"reference/explainers/#shap-i","text":"An advanced extension of SHAP that captures higher-order feature interactions. SHAP-I extends the original SHAP method to support any-order interactions between features. It incorporates efficient algorithms to compute interaction-aware Shapley values, enabling explanation of complex dependencies in high-dimensional input data.","title":"SHAP-I"},{"location":"reference/explainers/#gradient-based-methods","text":"Gradient-based methods derive feature attributions by analyzing the gradients of the model's output with respect to its inputs.","title":"\ud83d\udd01 Gradient-Based Methods"},{"location":"reference/explainers/#saliency","text":"A baseline technique that visualizes input feature importance using raw gradients. Saliency methods compute the gradient of the output with respect to each input feature. The absolute values of these gradients highlight which parts of the input most influence the model's prediction.","title":"Saliency"},{"location":"reference/explainers/#gxi-gradient-input","text":"Enhances gradient information by incorporating input values. GxI multiplies the gradient of the output with respect to each input feature by the corresponding input value. This combination provides a more intuitive measure of each feature's contribution, especially for models where feature magnitudes play a critical role.","title":"GxI (Gradient \u00d7 Input)"},{"location":"reference/explainers/#ig-integrated-gradients","text":"A principled method that attributes importance by integrating gradients along a path from baseline to input. Integrated Gradients explain a model\u2019s prediction by accumulating gradients as the input transitions from a baseline (e.g., a zero vector) to the actual input. This method satisfies desirable properties such as completeness and provides a mathematically grounded attribution of feature importance.","title":"IG (Integrated Gradients)"},{"location":"reference/explainers/#dl-deeplift","text":"Assigns attributions by comparing neuron activations to a reference. DeepLIFT computes the difference between the activations of neurons for an input and a reference baseline. It assigns credit to each input feature for the change in output, using the Rescale Rule. DeepLIFT is often more stable than raw gradients and is computationally more efficient than IG, especially for large datasets and deep architectures.","title":"DL (DeepLIFT)"},{"location":"reference/explainers/#gbp-guided-backpropagation","text":"Enhances interpretability by filtering gradients. Guided Backpropagation modifies the standard backpropagation algorithm to ignore negative gradients during the backward pass. This approach emphasizes input features that positively influence the output, resulting in sharper and more focused explanations. ---1~# \ud83e\udde0 Explainers One goal of EvalxNLP is to enable users to generate diverse explanations using multiple explainability methods. It integrates eight widely recognized post-hoc feature attribution (Ph-FA) explainability methods from the XAI literature. These methods assign importance scores to individual features based on their contribution to the model's prediction. We focus on Ph-FA methods for their interpretability, relevance to end users, and generally efficient computational cost.","title":"GBP (Guided Backpropagation)"},{"location":"reference/explainers/#categories-of-explainers_1","text":"Feature attribution methods in EvalxNLP are categorized into: Perturbation-based methods Gradient-based methods Below, we provide an overview of the methods included in each category.","title":"\ud83e\udde9 Categories of Explainers"},{"location":"reference/explainers/#perturbation-based-methods_1","text":"Perturbation-based methods explain model predictions by altering or masking input features and observing the resulting changes in the output.","title":"\ud83d\udd00 Perturbation-Based Methods"},{"location":"reference/explainers/#lime_1","text":"A model-agnostic method that explains individual predictions by training a local surrogate model. LIME generates explanations by perturbing the input data and measuring how the model's predictions change. Perturbed samples are weighted by their similarity to the original input. A simple, interpretable model\u2014typically linear\u2014is then trained on this weighted data to approximate the local behavior of the original model. LIME aims to balance fidelity (how closely the surrogate approximates the model) and interpretability (how understandable the explanation is to humans).","title":"LIME"},{"location":"reference/explainers/#shap-partition-shap_1","text":"A game-theoretic approach to explain feature contributions using Shapley values. SHAP computes the contribution of each input feature by evaluating all possible subsets of features, as proposed in cooperative game theory. Partition SHAP, a variant, improves efficiency by leveraging feature independence, making it scalable for high-dimensional data while preserving theoretical guarantees.","title":"SHAP (Partition SHAP)"},{"location":"reference/explainers/#shap-i_1","text":"An advanced extension of SHAP that captures higher-order feature interactions. SHAP-I extends the original SHAP method to support any-order interactions between features. It incorporates efficient algorithms to compute interaction-aware Shapley values, enabling explanation of complex dependencies in high-dimensional input data.","title":"SHAP-I"},{"location":"reference/explainers/#gradient-based-methods_1","text":"Gradient-based methods derive feature attributions by analyzing the gradients of the model's output with respect to its inputs.","title":"\ud83d\udd01 Gradient-Based Methods"},{"location":"reference/explainers/#saliency_1","text":"A baseline technique that visualizes input feature importance using raw gradients. Saliency methods compute the gradient of the output with respect to each input feature. The absolute values of these gradients highlight which parts of the input most influence the model's prediction.","title":"Saliency"},{"location":"reference/explainers/#gxi-gradient-input_1","text":"Enhances gradient information by incorporating input values. GxI multiplies the gradient of the output with respect to each input feature by the corresponding input value. This combination provides a more intuitive measure of each feature's contribution, especially for models where feature magnitudes play a critical role.","title":"GxI (Gradient \u00d7 Input)"},{"location":"reference/explainers/#ig-integrated-gradients_1","text":"A principled method that attributes importance by integrating gradients along a path from baseline to input. Integrated Gradients explain a model\u2019s prediction by accumulating gradients as the input transitions from a baseline (e.g., a zero vector) to the actual input. This method satisfies desirable properties such as completeness and provides a mathematically grounded attribution of feature importance.","title":"IG (Integrated Gradients)"},{"location":"reference/explainers/#dl-deeplift_1","text":"Assigns attributions by comparing neuron activations to a reference. DeepLIFT computes the difference between the activations of neurons for an input and a reference baseline. It assigns credit to each input feature for the change in output, using the Rescale Rule. DeepLIFT is often more stable than raw gradients and is computationally more efficient than IG, especially for large datasets and deep architectures.","title":"DL (DeepLIFT)"},{"location":"reference/explainers/#gbp-guided-backpropagation_1","text":"Enhances interpretability by filtering gradients. Guided Backpropagation modifies the standard backpropagation algorithm to ignore negative gradients during the backward pass. This approach emphasizes input features that positively influence the output, resulting in sharper and more focused explanations.","title":"GBP (Guided Backpropagation)"}]}