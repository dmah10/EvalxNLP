EvalxNLP provides powerful tools to interpret explanation scores and evaluation metrics using natural language generated by Large Language Models (LLMs). These human-readable interpretations enhance the transparency and usability of XAI methods.

---

The framework supports integration with [Together AI's](https://docs.together.ai/) API, using the `meta-llama/Llama-3.3-70B-Instruct-Turbo-Free` model by default.

#### ‚úÖ Prerequisites

- Create an API key from [Together AI](https://docs.together.ai/).
- Pass your API key and model name to the `LLMExplanationGenerator` and/or `EvaluationMetricsExplainer` class(es)

### üìò Understanding Explanation Scores

Use  `LLMExplanationGenerator` class to interpret importance scores generated from different explainers.
`generate_and_save_explanations()` allows to obtain explanations and save descriptions in JSON, HTML, or both formats. Explanations are saved in the `results/explanations/` folder.

You can also visualize the output within notebook cell using `display_explanations()` function.

```python
from LLMExplanationGenerator import LLMExplanationGenerator
import pandas as pd
from IPython.display import display, HTML

model_name = "meta-llama/Llama-3.3-70B-Instruct-Turbo-Free"
api_key = "YOUR_API_KEY_HERE"

explainer = LLMExplanationGenerator(model_name=model_name, api_key=api_key)

explanations, saved_files = explainer.generate_and_save_explanations(
    exps=exps,
    output_format="both"  # Choose from "json", "html", or "both"
)

# üì∫ Display in notebook
explainer.display_explanations(explanations)
```

### üìä Understanding Evaluation Metrics

Use `EvaluationMetricsExplainer` class to interpret evaluation metrics.
`explain_results()` generates explanations and `save_results()` saves the output in JSON, HTML, or both formats to `results/explanations/` folder.
You can also visualize the output within notebook cell using `display_results()` function.

```python
from EvaluationMetricsExplainer import EvaluationMetricsExplainer

api_key = "YOUR_API_KEY_HERE"
explainer = EvaluationMetricsExplainer(api_key=api_key)

# Provide results from evaluation functions
results = explainer.explain_results(metrics)

# Save results to files
json_path, html_path = explainer.save_results(results)

# üñ•Ô∏è Display in notebook
explainer.display_results(results)
```

üìç The generated reports include metric definitions, score interpretations, and comparisons across explainers in structured HTML or JSON format.
