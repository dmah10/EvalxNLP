{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to Use EvalxNLP Framework for Sentiment Analysis ‚Äì A Beginner-Friendly Guide üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### What is EvalxNLP?\n",
        "\n",
        "Imagine you have a sentiment analysis model that predicts whether a sentence is positive, neutral, or negative. But how does it know what makes a sentence negative? Does it focus on the right words?\n",
        "\n",
        "That‚Äôs where EvalxNLP helps! ü¶ä\n",
        "\n",
        "‚úÖ EvalxNLP explains model predictions by showing which words had the most impact.\n",
        "\n",
        "‚úÖ Helps you trust the model‚Äôs decisions.\n",
        "\n",
        "‚úÖ Useful for debugging why your model makes mistakes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\EvalxNLP\\venv32\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from XAIbenchmark import XAIFramework\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Single Sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 1: Load a pre-trained Sentiment Model, and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 2: Optionally initialize the explainers, and evaluators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from explainers import InputXGradientExplainer, IntegratedGradientsExplainer, DeepLiftExplainer, LimeExplainer, SHAPExplainer, SHAPIQExplainer, GuidedBackpropExplainer\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "ig= IntegratedGradientsExplainer(model,tokenizer,device)\n",
        "gb= GuidedBackpropExplainer(model,tokenizer,device)\n",
        "dl= DeepLiftExplainer(model,tokenizer,device)\n",
        "ixg= InputXGradientExplainer(model,tokenizer,device,multiply_by_inputs=True)\n",
        "g= InputXGradientExplainer(model,tokenizer,device,multiply_by_inputs=False)\n",
        "lime= LimeExplainer(model,tokenizer)\n",
        "shap= SHAPExplainer(model,tokenizer)\n",
        "shapiq= SHAPIQExplainer(model,tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from evaluators import AUCTPEvaluator, SoftComprehensivenessEvaluator, SoftSufficiencyEvaluator, FADEvaluator, SparsenessEvaluator, ComplexityEvaluator, IOUF1Evaluator,TokenF1Evaluator, AUPRCEvaluator\n",
        "\n",
        "sc= SoftComprehensivenessEvaluator(model,tokenizer,device)\n",
        "ss= SoftSufficiencyEvaluator(model,tokenizer,device)\n",
        "fad= FADEvaluator(model,tokenizer,device)\n",
        "sp= SparsenessEvaluator(model,tokenizer,device)\n",
        "cx= ComplexityEvaluator(model,tokenizer,device)\n",
        "auctp= AUCTPEvaluator(model,tokenizer,device)\n",
        "iou_f1= IOUF1Evaluator(model,tokenizer,device)\n",
        "token_f1= TokenF1Evaluator(model,tokenizer,device)\n",
        "auprc= AUPRCEvaluator(model,tokenizer,device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 3: Inialize XAIBenchmark Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "xai_framework = XAIFramework(model, tokenizer,explainers=[g], evaluators=[cx,sc,ss,fad,sp,auctp],device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note:** If you don‚Äôt pass any explainers or evaluators, it defaults to all explainers/evaluators."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 4: Predict Sentiment of a Sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'negative': 0.07390535622835159,\n",
              " 'neutral': 0.10088995099067688,\n",
              " 'positive': 0.8252046704292297}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Define the senece to be explained\n",
        "# sentence= \"Great movie for a great nap!\"\n",
        "# sentence= \"Worst experience I've ever had!\"\n",
        "sentence= \"A masterpiece of how not to make a movie.\"\n",
        "xai_framework.classify_text(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 5: Use EvalxNLP to Explain Why"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can use the `XAIBenchmark` class to explain for all the explainers defined during the initiation of the `XAIBenchmark` class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Explanation(\n",
              "  text=['A masterpiece of how not to make a movie.'],\n",
              "  tokens=['<s>', '‚ñÅA', '‚ñÅmaster', 'piece', '‚ñÅof', '‚ñÅhow', '‚ñÅnot', '‚ñÅto', '‚ñÅmake', '‚ñÅa', '‚ñÅmovie', '.', '</s>'],\n",
              "  scores=[0.05983935 0.02848242 0.18051037 0.1956277  0.04314518 0.08561721\n",
              " 0.11212566 0.04384509 0.06295516 0.0221678  0.0799308  0.02522218\n",
              " 0.06053117],\n",
              "  explainer=Saliency,\n",
              "  target=positive,\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "exps= xai_framework.explain(input_data=sentence,target_label=\"positive\")\n",
        "exps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or you can use an individual explainer's `compute_feature_importance` method to get importance scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:10<00:00,  2.43it/s]\n"
          ]
        }
      ],
      "source": [
        "ig_exps= ig.compute_feature_importance(sentence, target=\"positive\")\n",
        "gb_exps= gb.compute_feature_importance(sentence, target=\"positive\")\n",
        "dl_exps= dl.compute_feature_importance(sentence, target=\"positive\")\n",
        "ixg_exps= ixg.compute_feature_importance(sentence, target=\"positive\")\n",
        "lime_exps= lime.compute_feature_importance(sentence, target=\"positive\")\n",
        "shap_exps= shap.compute_feature_importance(sentence, target=\"positive\")\n",
        "shapiq_exps= shapiq.compute_feature_importance(sentence, target=\"positive\")\n",
        "g_exps= g.compute_feature_importance(sentence, target=\"positive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 5: Visualize the Explanation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABDQAAAL8CAYAAAAfheWZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACGbElEQVR4nO3dCZxN9fvA8ecOmTH2sZPsKUWyC6XILkqFyJLlTyFLWVrsZf1JloiyhSyVtRIhUkJkqRBFluxjHTWWuf/X89W93TubWe5x58z9vH+v8zP33HPvfOfbOfee85zn+3wdTqfTKQAAAAAAADYS5O8GAAAAAAAAJBYBDQAAAAAAYDsENAAAAAAAgO0Q0AAAAAAAALZDQAMAAAAAANgOAQ0AAAAAAGA7BDQAAAAAAIDtENAAAAAAAAC2Q0ADAAAAAADYDgENAICtORwOqVGjhthZoUKFzAIAAICEI6ABAEiwQ4cOmQBCfAsX5jG1bdvW9I32nx3of8OQkJB4t9G/55577pHbyW79CAAArJXW4vcHAKRCRYsWlVatWsX6XNasWW97e+xuzZo1/m4CAACA7RDQAAAkWrFixWTQoEH+bkaqChABAAAgcRhyAgCwzIgRI8wQgc6dO8f5XJcuXdzrNEii67755hv58MMPpVSpUmboQ/78+aVnz55y6dKlBP3e3377Tfr06SNly5aV7Nmzm/e4++67pV+/fnL58uUY22sNDv29165dM23QIRfBwcHmNe+9916M7f/66y8ZOHCgVK5cWXLlymW21de8+OKLcurUKa9tdf2sWbPMz4ULF3YPzfGs+xFXDY2IiAjze3Roh/4NYWFh0qBBA/nuu+9ibOvZd/PmzZMyZcpI+vTpJW/evPLyyy/L33//LbeD0+mU6dOnS9WqVSVz5swSGhoq5cuXN+us7kfX42PHjslzzz0nOXLkkEyZMpk+++OPP8w2e/bskSZNmpi+1OeefvppOXnyZIy2aXsbN27sHn6j29epU0fWrVsXY1vtc/3d+t9g48aNpg363pqt1LRpUzlw4IBP+hYAAHgjQwMAYBkNKqxevVref/99qVu3rrmQVFu2bJEBAwZIyZIlZezYsTFep+t0GEazZs3MxejXX38t48aNkx9++EE2bNggd9xxR7y/97PPPjMBkUcffdRcXEZFRZnXjhw5UtavXx/ne7Ro0cK0rV69epImTRpZuHChvPTSS2bbjh07urfT1//vf/+TmjVrSqVKlczzP/30k0yePFm++uor2b59u2TJksVs26NHD5k5c6bs3LnTBBZcQ3JuVWvkn3/+kccee8y0RwMz+j564b1gwQLzOz7++GN55plnYrxu4sSJsnLlSnMxrq/Xn8ePHy9nzpyRuXPnitXBjJYtW5q2FS9e3AQV0qVLZ/aB9u3by6+//ipjxoyxtB/PnTsn1apVkzx58kibNm1McGvFihWyd+9eWbp0qVSvXl3KlSsnL7zwgmzbtk0+/fRTCQ8Pl7Vr13q9j/53f+CBB6RWrVqSM2dOEyRZsmSJeaz7l/ZvdLqPDR8+3Ozr3bp1k19++UUWL14s3377rXmuSJEiFvU8AAABygkAQAIdPHjQqV8dRYsWdQ4cODDW5csvv/R6zdGjR53Zs2d3hoWFmZ8vXrxoXh8cHOzcuXOn17b6en3/dOnSeT0XFRXlfO6558xzY8aM8XqNrnvkkUdi/M7IyMgY7R88eLDZfs6cOV7r9fW6vlKlSs4LFy641+/du9eZNm1aZ4kSJby2P3nypPPSpUsx3n/WrFnmfYYNG+a1vk2bNma99l9sChYsaJbY2tqyZUvz97ts377d9E/WrFlNX0bvuyxZsph2u1y5csV59913O4OCgpzHjh2L9ffH1p40adLE+d/Y9bui98vUqVPN+nbt2jmvXr3qXq//LRo1amSe+/HHHy3rR31Ol549e3qt79Kli1mvfTZu3Dj3eu3X+vXrm+e2bdvm9Zo//vgjxvv/9ddfznz58jmLFy/utX7dunXu3z1lyhSv5/Sxrm/YsGGsbQYAAElHQAMAkOiARnzLyy+/HON1S5YsMc/VqFHD2apVK/Pzu+++G2M714Vyhw4dYjx36NAhc5F9//333zKgEZezZ8+a7du2bRtrQGPt2rUxXuN6zjN4EBe9QM6cObP5O5Mb0ChSpIjzjjvucB45ciTG9h07djTvN3v27Bh9N2DAgBjbu55btmzZLf8GV3tu9d85toBG6dKlnRkyZDBBlOh27dplXtO7d2/L+lGfy5gxozMiIsJr/YYNG9yBOM/gkNI+1OemT5/uTIhu3bqZ7XV/jB7Q0MDRjRs3vLbXxxoAcTgczlOnTiXodwAAgIRhyAkAING0loAOZUgoTc/XOhpTpkwxj+vXry/du3ePc3sdFhBdwYIFpUCBAiaN/+rVq2YoQ1z02nbGjBlmiMLPP/8sFy5cMMNOPGs3xEaHIkR35513mn/Pnz9v6iK46LADHUqjwyJ0mMONGzdu+f4JdfHiRVPz4d5773X/fk86lGbatGmyY8cOef755xP1NySU1rPQYS9x0ZoRnq5cuSK7d++WfPnymaE90Wl9EqVDPzz5uh91qIvW7fCkdURU6dKlY7Tb9Vz036X9r8NHdCiKDjeJjIz0el63133Sk9YNCQryLk+mj3X9/v37zXAZHbICAAB8g4AGAOC2ePLJJ90Bja5du8a7be7cueNcf+jQIVMcVIt9xkWDJVpLQgMgTzzxhLlo1Qt0NXjw4BgXpy5axDK6tGlvflV6Xmhr3YdXXnnF1FaoXbu2CRhoAU6ltT7iev/EBDRcf29sXBfhru2S8jf4mgYjNJCkF//ax3HRQqdW9mN8f398z7kCLkqLeFasWNH0rwaPGjVqZF6rwQktAKp1WGJrW3z7rdLAGgAA8B0CGgAAy2lmgBbVzJAhg7mo1oKJWvzRM+PBU2yzTrjW6x32uF6ndHaMSZMmmbvxmzZt8rpbf+LEiXgvthPi+vXrMnToUBNU0AwJnZ3DRS/oR40aJcnluvCOqx/07/DcLiVwtUUzRH788ccU0Y9J9c4775gAzUcffSStWrXyek4zjTSgkdj9VrkKnAIAAN9g2lYAgOU6deokhw8flnfffVdGjx4tv//+u5lFIi46K0R0f/75pxw5ckTuu+++eIeb6FABvSDW1P7oQw9ie9/E0tlC9E57lSpVvC7ClV7IxzY9qs6YkpgMCQ0O6IwYmimgGQ/RaZaA0qlZUwoNMukQGZ0WNSFDW25HPyaV7p8q+kwmul/FNmWuiz7nObRJ6ePvv//eBOJ01hQAAOA7BDQAAJbS6VMXLVpkphjVqTt1uEnDhg3N3e958+bF+prZs2fLrl27vC4kX3vtNXMh27Zt23h/n6uugV5Eel5cHj16VPr375/sv0cvvnVYhNZ80LoRLnpHXzNPYhMWFmb+1YBMQumUozoMQtt8s97lTdovWhtE7/a7psFNKXSoj/aJZuN4Di1xOXjwoBkydDv7MSlc+9DGjRu91o8YMcLUZImLThGrtU086WNdr9MP69AaAADgOww5AQAkmmYODBo0KM7n+/XrJyEhIeZC7uWXXza1LKZOnep+fvr06WZISJcuXcwd+sKFC8coOqrrmzdvbi4C16xZY+7aV65cOc6LXRcdwtC0aVP59NNPpXz58lKzZk2T8r9ixQrzs+vue1JpHYUXX3zR1H/QO+5aX0FrLXz55ZfmQliLYkb32GOPyZgxY0ymirZNh97ottELenrq06ePfP755ybwo1kP2nYdTrNgwQIzXEMvlOMbeuMP//d//yc//PCDzJo1y2QraJaM9of2vxYD3bx5swliFSpU6Lb1Y1LosBItKqu/49lnnzX1WvTv0uCLBib0v0tsdL/VoM4XX3xhMom0gO3y5cslR44cJjsJAAD4WAJnQwEAIEHTtupy7tw5Z2RkpLNs2bLOoKAg5/r162O816pVq8xUlpUrV3Zeu3bNa3pRnQZz2rRpzvvuu88ZHBzszJs3r5kONrapU2ObtvXSpUtmetBChQqZ1+u0mUOHDnVevXo11u1dU7PGJrapQvV93nrrLfO++v533XWX+X36e2ObglWNGjXKbK9TsUZvQ1yvuXz5svPNN98004GmS5fOmTVrVme9evWc3377bYxtPfsuuhkzZpjn9N+E0Lbo3xWf2KZtdVmwYIGzVq1azmzZspm/N3/+/GYK1v/973/O06dPW9aPcU3h69pv9b9ldK4pV7X/oq+vWrWqM1OmTKbf69ev79y2bVus/ez5HvrfRtug09fq1LNPPvmkc//+/fH2JQAASBqH/p+vgyQAACSFZn1o0c5169ZJjRo1/N0cIEG0ponOhjJw4MB4M5cAAIBvUUMDAAAAAADYDgENAAAAAABgOwQ0AAAAAACA7VBDAwAAAAAA2A4ZGgAAAAAAwHYIaAAAAAAAANshoAEAAAAAAGyHgAYAAAAAALAdAhoAAAAAAMB2CGgAAAAAAADbIaABAAAAAABsh4AGAAAAAACwHQIaAAAAAADAdghoAAAAAAAA2yGgAQAAAAAAbIeABgAAAAAAsB0CGgAAAAAAwHYIaAAAAAAAANshoAEAAAAAAGwnrb8bkFJcCg/3dxNs6/qRY/5ugn3duOHvFthWmgL5/d0E23JGRvq7CfZ29Zq/W2BbjkwZ/d0E+7p23d8tsC1H+hB/N8G2oiIi/N0E2woKDfV3E2wtU1iYv5sAmyBDAwAAAAAA2A4BDQAAAAAAYDsENAAAAAAAgO0Q0AAAAAAAALZDQAMAAAAAANgOAQ0AAAAAAGA7BDQAAAAAAIDtENAAAAAAAAC2Q0ADAAAAAADYDgENAAAAAABgOwQ0AAAAAACA7RDQAAAAAAAAtkNAAwAAAAAA2A4BDQAAAAAAYDsENAAAAAAAgO0Q0AAAAAAAALZDQAMAAAAAANgOAQ0AAAAAAGA7BDQAAAAAAIDtENAAAAAAAAC2Q0ADAAAAAADYDgENAAAAAABgOwQ0AAAAAACA7RDQAAAAAAAAtkNAAwAAAAAA2A4BDQAAAAAAYDsENAAAAAAAgO0Q0AAAAAAAALZDQAMAAAAAANgOAQ0AAAAAAGA7BDQAAAAAAIDtENAAAAAAAAC2Q0ADAAAAAADYDgENAAAAAABgOwQ0AAAAAACA7RDQAAAAAAAAtkNAAwAAAAAA2A4BDQAAAAAAYDsENAAAAAAAgO0Q0AAAAAAAALZDQAMAAAAAANgOAQ0AAAAAAGA7BDQAAAAAAIDtENAAAAAAAAC2Q0ADAAAAAADYDgENAAAAAABgOwQ0AAAAAACA7RDQAAAAAAAAtkNAAwAAAAAA2A4BDQAAAAAAYDsENAAAAAAAgO0Q0AAAAAAAALZDQAMAAAAAANgOAQ0AAAAAAGA7BDQAAAAAAIDtENAAAAAAAAC2Q0ADAAAAAADYDgENAAAAAABgOwQ0AAAAAACA7RDQAAAAAAAAtkNAAwAAAAAA2A4BDQAAAAAAYDsENAAAAAAAgO0Q0AAAAAAAALZDQAMAAAAAANgOAQ0AAAAAAGA7BDQAAAAAAIDtENAAAAAAAAC2Q0ADAAAAAADYDgENAAAAAABgOwQ0AAAAAACA7RDQAAAAAAAAtkNAAwAAAACAALFhwwZp1KiR5MuXTxwOhyxZsuSWr/nmm2+kbNmyEhwcLMWKFZOZM2fG2GbSpElSqFAhCQkJkUqVKsmWLVvEagQ0AAAAAAAIEBEREfLAAw+YAERCHDx4UBo0aCCPPvqo7NixQ3r06CEdOnSQr776yr3NggULpFevXjJw4EDZvn27ef86derIqVOnLPxLRBxOp9Np6W+wiUvh4f5ugm1dP3LM302wrxs3/N0C20pTIL+/m2BbzshIfzfB3q5e83cLbMuRKaO/m2Bf1677uwW25Ugf4u8m2FZURIS/m2BbQaGh/m6CrWUKC/N3EwKGw+GQxYsXS5MmTeLcpm/fvvL555/Lzz//7F7XvHlzOX/+vKxcudI81oyMChUqyMSJE83jqKgoKVCggHTr1k369etnWfvJ0AAAAAAAwMYiIyPl4sWLXouu84VNmzZJrVq1vNZp9oWuV1evXpVt27Z5bRMUFGQeu7axSlpL3x0AAAAAAMj+anUse++5tarI4MGDvdbp8I9BgwYl+71PnDghuXPn9lqnjzVo8vfff8u5c+fkxo0bsW6zd+9esRIBDQAAAAAAbKx///6mhoUnLeCZ2hHQAAAAAADAag7rKj4EBwdbFsDIkyePnDx50mudPs6cObOkT59e0qRJY5bYttHXWokaGgAAAAAAIFZVqlSRNWvWeK1bvXq1Wa/SpUsn5cqV89pGi4LqY9c2ViFDAwAAAAAAqzkckhJcvnxZDhw44DUtq07HGhYWJnfddZcZvnLs2DGZPXu2eb5z585m9pI+ffrICy+8IGvXrpWFCxeamU9cdLhLmzZtpHz58lKxYkUZN26cmR62Xbt2lv4tBDQAAAAAAAgQP/74ozz66KPux67aGxqQmDlzphw/flwOHz7sfr5w4cImeNGzZ09599135c4775QPPvjAzHTi0qxZMzl9+rQMGDDAFBEtU6aMmdI1eqFQX3M4nU6npb/BJi6Fh/u7CbZ1/cgxfzfBvm7c8HcLbCtNgfz+boJtOX00hVfAunrN3y2wLUemjP5ugn1du+7vFtiWI32Iv5tgW1EREf5ugm0FhYb6uwm2liksTFKj/TUaWPbexb/5L1sikJChAQAAAACAxRwWFgUNVPQoAAAAAACwHTI0AAAAAACwWlDKKAqampChAQAAAAAAbIcMDQAAAAAAAmTa1tSEDA0AAAAAAGA7ZGgAAAAAAGC1IPIJfI0eBQAAAAAAtkOGBgAAAAAAVqOGhs8R0AAAAAAAwGIOAho+x5ATAAAAAABgO2RoAAAAAABgNYqC+hw9CgAAAAAAbIcMDQAAAAAArEYNDZ8jQwMAAAAAANgOGRoAAAAAAFgtiAwNXyNDAwAAAAAA2A4ZGgAAAAAAWM1BPoGvEdAAAAAAAMBiDoac+BwhIgAAAAAAYDtkaAAAAAAAYDWmbfU5MjQAAAAAAIDtkKEBAAAAAIDVKArqc/QoAAAAAACwHTI0AAAAAACwGrOc+BwZGgAAAAAAwHbI0AAAAAAAwGrMcuJzBDQAAAAAALCYI4gBEr5GjwIAAAAAANshQwMAAAAAAKsx5MTnyNAAAAAAAAC2Q4YGAAAAAABWo4aGz9GjAAAAAADAdsjQAAAAAADAatTQ8DkyNAAAAAAAgO2QoQEAAAAAgNXI0PA5AhoAAAAAAFjMQVFQn6NHAQAAAACA7ZChAQAAAACA1Rhy4nNkaAAAAAAAANshQwMAAAAAAKsFkaHha2RoAAAAAAAA2yFDAwAAAAAAqznIJ/A1ehQAAAAAANgOGRoAAAAAAFiNGho+R4YGAAAAAACwHTI0AAAAAACwmoMMDV8jQwMAAAAAAIs5HEGWLUkxadIkKVSokISEhEilSpVky5YtcW5bo0YNcTgcMZYGDRq4t2nbtm2M5+vWrStWIkMDAAAAAIAAsmDBAunVq5dMmTLFBDPGjRsnderUkX379kmuXLlibP/ZZ5/J1atX3Y/Pnj0rDzzwgDzzzDNe22kAY8aMGe7HwcHBlv4dZGgAAAAAAHA7ioJatSTS2LFjpWPHjtKuXTspWbKkCWyEhobK9OnTY90+LCxM8uTJ415Wr15tto8e0NAAhud22bJlEysR0AAAAAAAwMYiIyPl4sWLXouui41mWmzbtk1q1arlXhcUFGQeb9q0KUG/78MPP5TmzZtLhgwZvNZ/8803JsOjRIkS0qVLF5PJYSUCGgAAAAAA3I6ioBYtw4cPlyxZsngtui42Z86ckRs3bkju3Lm91uvjEydO3PLP0FobP//8s3To0CHGcJPZs2fLmjVrZOTIkbJ+/XqpV6+e+V1WoYYGAAAAAAA21r9/f1MTw5NV9Ss0O6NUqVJSsWJFr/WaseGiz5cuXVqKFi1qsjZq1qxpSVvI0AAAAAAAwGpBQZYtwcHBkjlzZq8lroBGjhw5JE2aNHLy5Emv9fpY617EJyIiQubPny/t27e/5Z9bpEgR87sOHDggViGgAQAAAABAgEiXLp2UK1fODA1xiYqKMo+rVKkS72sXLVpkanO0atXqlr/n6NGjpoZG3rx5xSoENAAAAAAAsHENjcTS4SnTpk2TWbNmyZ49e0wBT82+0FlPVOvWrc0wltiGmzRp0kSyZ8/utf7y5cvy6quvyg8//CCHDh0ywZHGjRtLsWLFzHSwVqGGBgAAAAAAFnMkYXpVqzRr1kxOnz4tAwYMMIVAy5QpIytXrnQXCj18+LCZ+cTTvn37ZOPGjbJq1aoY76dDWHbt2mUCJOfPn5d8+fJJ7dq1ZejQoZbV8lAOp9PptOzdbeRSeLi/m2Bb148c83cT7MvCir+pXZoC+f3dBNtyxjGFFxLo6jV/t8C2HJky+rsJ9nXtur9bYFuO9CH+boJtRUVE+LsJthUUGurvJthaprAwSY0Od+9j2XvfNX6UBCIyNAAAAAAAsJqDig++Ro8CAAAAAADbIUMDAAAAAACrJaF4J+JHhgYAAAAAALAdMjQAAAAAALBaCprlJLUgQwMAAAAAANgOGRoAAAAAAFiNWU58joAGAAAAAAAWczDkxOcIEQEAAAAAANshQwMAAAAAAKsxbavPkaEBAAAAAABshwwNAAAAAACsFkQ+ga/RowAAAAAAwHbI0AAAAAAAwGrU0PA5MjQAAAAAAIDtkKEBAAAAAIDVyNDwOQIaAAAAAABYzEFRUJ+jRwEAAAAAgO2QoQEAAAAAgNUYcuJzZGgAAAAAAADbIUMDAAAAAACrBZGh4WtkaAAAAAAAANshQwMAAAAAAKs5yCfwNXoUAAAAAADYDhkaAAAAAABYjRoaPkdAAwAAAAAAqzFtq88x5AQAAAAAANgOGRoAAAAAAFjMQVFQn6NHAQAAAACA7ZChAQAAAACA1SgK6nNkaAAAAAAAANshQwMAAAAAAKsxy4nPkaEBAAAAAABshwwNAAAAAACsFkQ+ga8R0AAAAAAAwGoMOfE5QkQAAAAAAMB2yNAAAAAAAMBiDqZt9TkyNAAAAAAAgO2QoQEAAAAAgNUc5BP4Gj0KAAAAAABshwwNAAAAAACsxiwnPkeGBgAAAAAAsB0yNAAAAAAAsBqznPgcAQ0AAAAAAKxGUVCfo0cBAAAAAIDtkKEBAAAAAIDFHAw58TkyNAAAAAAAgO0Q0AAAAAAA4HZM22rVkgSTJk2SQoUKSUhIiFSqVEm2bNkS57YzZ84Uh8PhtejrPDmdThkwYIDkzZtX0qdPL7Vq1ZL9+/eLlQhoAAAAAAAQQBYsWCC9evWSgQMHyvbt2+WBBx6QOnXqyKlTp+J8TebMmeX48ePu5c8///R6ftSoUTJ+/HiZMmWKbN68WTJkyGDe859//rHs7yCgAQAAAACA1YKCrFsSaezYsdKxY0dp166dlCxZ0gQhQkNDZfr06XG+RrMy8uTJ415y587tlZ0xbtw4eeONN6Rx48ZSunRpmT17tvz111+yZMkSsQoBDQAAAAAAbCwyMlIuXrzotei62Fy9elW2bdtmhoS4BAUFmcebNm2K83dcvnxZChYsKAUKFDBBi19++cX93MGDB+XEiRNe75klSxYzlCW+90wuAhoAAAAAANg4Q2P48OEmgOC56LrYnDlzRm7cuOGVYaH0sQYlYlOiRAmTvbF06VKZM2eOREVFyUMPPSRHjx41z7tel5j39AWmbQUAAAAAwGpJLN6ZEP379zc1MTwFBwf77P2rVKliFhcNZtx7773y/vvvy9ChQ8VfCGgAAAAAAGBjwcHBCQ5g5MiRQ9KkSSMnT570Wq+PtTZGQtxxxx3y4IMPyoEDB8xj1+v0PXSWE8/3LFOmjFiFIScAAAAAAFjMEeSwbEmMdOnSSbly5WTNmjXudTqERB97ZmHER4es7N692x28KFy4sAlqeL6n1vHQ2U4S+p5JQYYGAAAAAAABpFevXtKmTRspX768VKxY0cxQEhERYWY9Ua1bt5b8+fO763AMGTJEKleuLMWKFZPz58/L6NGjzbStHTp0cM+A0qNHDxk2bJgUL17cBDjefPNNyZcvnzRp0sSyv4OABgAAAAAAVnOknAESzZo1k9OnT8uAAQNM0U4dFrJy5Up3Uc/Dhw+bmU9czp07Z6Z51W2zZctmMjy+//57M+WrS58+fUxQpFOnTiboUa1aNfOeISEhlv0dDqdOGAu5FB7u7ybY1vUjx/zdBPu6ccPfLbCtNAXy+7sJtuWMYwovJNDVa/5ugW05MmX0dxPs69p1f7fAthzprTuRTu2iIiL83QTbCgoN9XcTbC1TWJikRic+mG3Ze+fp0FoCERkaAAAAAADYeJaTQJVycl4AAAAAAAASiAwNAAAAAACslsjZSHBrBDQAAAAAAAigoqCpBT0KAAAAAABshwwNAAAAAAAs5mDIic+RoQEAAAAAAGyHDA0AAAAAAKzGtK0+R4YGAAAAAACwHTI0AAAAAACwWhD5BL5GjwIAAAAAANshQwMAAAAAAKtRQ8PnCGgAAAAAAGA1Aho+x5ATAAAAAABgO2RoAAAAAABgMQdFQX2OHgUAAAAAALZDhgYAAAAAAFajhobPkaEBAAAAAABshwwNAAAAAACsFkSGhq+RoQEAAAAAAGyHDA0AAAAAAKzmIJ/A1whoAAAAAABgNYac+BwhIgAAAAAAYDtkaAAAAAAAYDEH07b6HBkaAAAAAADAdsjQAAAAAADAahQF9Tl6FAAAAAAA2A4ZGgAAAAAAWI1ZTnyODA0AAAAAAGA7ZGgAAAAAAGA1ZjnxOQIaAAAAAABYLYgBEr5GjwIAAAAAANshQwMAAAAAAKsx5MTnyNAAAAAAAAC2Q4YGAAAAAAAWczBtq8+RoQEAAAAAAGyHDA0AAAAAAKzmIJ/A1+hRAAAAAABgO2RoAAAAAABgNWpo+BwBDQAAAAAArMa0rT7HkBMAAAAAAGA7ZGgAAAAAAGA1ioL6HD0KAAAAAABshwwNAAAAAAAs5qAoqM+RoQEAAAAAAGyHDA0AAAAAAKzGLCc+R4YGAAAAAAABZtKkSVKoUCEJCQmRSpUqyZYtW+Lcdtq0aVK9enXJli2bWWrVqhVj+7Zt24rD4fBa6tata+nfQEADAAAAAACrBQVZtyTSggULpFevXjJw4EDZvn27PPDAA1KnTh05depUrNt/88030qJFC1m3bp1s2rRJChQoILVr15Zjx455bacBjOPHj7uXjz/+WKzkcDqdTkt/g01cCg/3dxNs6/oR750YiXDjhr9bYFtpCuT3dxNsyxkZ6e8m2NvVa/5ugW05MmX0dxPs69p1f7fAthzpQ/zdBNuKiojwdxNsKyg01N9NsLVMYWGSGp3duMmy985erUqitteMjAoVKsjEiRPN46ioKBOk6Natm/Tr1++Wr79x44bJ1NDXt27d2p2hcf78eVmyZIncLmRoAAAAAABgY5GRkXLx4kWvRdfF5urVq7Jt2zYzbMQlKCjIPNbsi4S4cuWKXLt2TcKiBZ80kyNXrlxSokQJ6dKli5w9e1asREADAAAAAACr6bStFi3Dhw+XLFmyeC26LjZnzpwxGRa5c+f2Wq+PT5w4kaA/pW/fvpIvXz6voIgON5k9e7asWbNGRo4cKevXr5d69eqZ32UVZjkBAAAAAMDG+vfvb2pieAoODrbkd40YMULmz59vsjG0oKhL8+bN3T+XKlVKSpcuLUWLFjXb1axZ05K2ENAAAAAAAMBiDod1AySCg4MTHMDIkSOHpEmTRk6ePOm1Xh/nyZMn3teOGTPGBDS+/vprE7CIT5EiRczvOnDggGUBDYacAAAAAAAQINKlSyflypUzQ0NctCioPq5SJe7ioqNGjZKhQ4fKypUrpXz58rf8PUePHjU1NPLmzStWIaABAAAAAIDVHA7rlkTS4SnTpk2TWbNmyZ49e0wBz4iICGnXrp15Xmcu0WEsLloT480335Tp06dLoUKFTK0NXS5fvmye139fffVV+eGHH+TQoUMmONK4cWMpVqyYmQ7WKgw5AQAAAAAggDRr1kxOnz4tAwYMMIGJMmXKmMwLV6HQw4cPm5lPXCZPnmxmR3n66ae93mfgwIEyaNAgM4Rl165dJkCiU7dqwdDatWubjA6rankoh9PpdFr27jZyKTzc302wretHjvm7CfZlYcXf1C5Ngfz+boJtOeOYwgsJdPWav1tgW45MGf3dBPu6dt3fLbAtR/r/CtYhcaIiIvzdBNsKCg31dxNsLVO0qUBTi/AtP1r23mEVbz0EJDUiQwMAAAAAAKtZWBQ0UNGjAAAAAADAdsjQAAAAAADAakGJL96J+JGhAQAAAAAAbIcMDQAAAAAALOZIwvSqiB8ZGgAAAAAAwHbI0AAAAAAAwGpB5BP4Gj0KAAAAAABshwwNAAAAAACsRg0NnyOgAQAAAACA1Qho+BxDTgAAAAAAgO2QoQEAAAAAgNUoCpqyAhqXLl2S8+fPS4ECBdzr/vrrL5kyZYpERkZK06ZNpWLFir5oJwAAAAAAgG8CGp06dZKDBw/KDz/8YB5fvHhRKleuLEePHpWgoCB59913ZeXKlVKjRo3k/BoAAAAAAGzNQQ0Nn0tWzsvGjRulYcOG7sdz5swxGRrff/+9nDt3TkqXLi3Dhg3zRTsBAAAAAAB8E9A4c+aM5M+f3/142bJlUq1aNZOlkSlTJmndurXs3LkzOb8CAAAAAAD7C3JYtwSoZAU0smbNKidOnDA///333/Ltt99K7dq13c+nTZtWrly5kvxWAgAAAAAA+KqGxkMPPSTvvfee3HPPPaZWxj///CONGzd2P//bb795ZXAAAAAAABCQHMxykqICGiNGjJA6deqY2UxU79695b777jM/37hxQxYtWiR169b1TUsBAAAAALCrAB4akiIDGsWLF5d9+/bJr7/+KlmyZJFChQq5n9OhJhMnTpQHHnjAF+0EAAAAAABIfkBDAxatWrUy2RktW7aM8bwWBfUcfgIAAAAAQMBi2lafS/IgntDQUPn6668p+gkAAAAAAG67ZFUl0SlaN23a5LvWAAAAAACQWouCWrUEqGT95VojQ6dqfeONN+To0aO+axUAAAAAAEA8HE6n0ylJpHUyrl+/LlevXjWP06ZNK8HBwd6/wOGQCxcuiC+1adNG2rdvLw8//LDP3vNSeLjP3ivQXD9yzN9NsK8bN/zdAttKU4ApoZPKGRnp7ybY29Vr/m6BbTkyZfR3E+zr2nV/t8C2HOlD/N0E24qKiPB3E2wrKDTU302wtUxhYZIaXfjjoGXvnaVIYQlEyZrlRAuCasDidtMASa1ataRgwYLSrl07E+DIn5+LGwAAAAAAAkWyMjT86fTp0/LRRx/JrFmzzLSxGuDQrA2dWeWOO+5I9PuRoZF0ZGgkAxkaSUaGRtKRoZFMZGgkGRkayUCGRpKRoZF0ZGgkHRkayZNqMzQOHrLsvbMULiSByLbVQ3LmzCm9evWSnTt3yubNm6VYsWLy/PPPS758+aRnz56yf/9+fzcRAAAAAICbgoKsWwJUsv/yw4cPS+fOnaVEiRKSLVs22bBhg1l/5swZ6d69u/z0009ipePHj8vq1avNkiZNGqlfv77s3r1bSpYsKe+8846lvxsAAAAAANiwhoYO9ahevbpERUVJpUqV5MCBA6ZIqMqRI4ds3LhRIiIi5MMPPxRfunbtmixbtkxmzJghq1atktKlS0uPHj3kueeek8yZM5ttFi9eLC+88ILJ1gAAAAAAwK/8UH8ytUtWQKNPnz6SNWtW+eGHH0xx0Fy5cnk936BBA1mwYIH4Wt68eU0QpUWLFrJlyxYpU6ZMjG0effRR0zYAAAAAAJD6JCugocNLBgwYYOpZnD17Nsbzd911lxw75vuCkTqU5JlnnpGQkLiLPGkw4+BB66bFAQAAAAAgwYLI0EhRNTQ0SyI0ngq+OhNJcHCw+NoTTzwhV65cibE+PDxcLl686PPfBwAAAAAAUlFAo2zZsvL555/H+pzW0pg/f75UrlxZfK158+bmvaNbuHCheQ4AAAAAgJTE4QiybAlUyfrL+/fvLytXrpQuXbrIzz//bNadPHlSvv76a6ldu7bs2bNH+vXrJ76m07RqjYzoatSoYZ4DAAAAAACpW7JqaNSrV09mzpwpL7/8skydOtWsa9WqlTidTjPbyOzZs+Xhhx8WX4uMjHTPphJ99pO///7b578PAAAAAIBkYZaTlBXQUM8//7w89dRTZvpUnbZV62oULVpU6tSpI5kyZRIrVKxY0QRQJkyY4LV+ypQpUq5cOUt+JwAAAAAASUZR0JQX0FAZMmSQJ598Um6XYcOGSa1atWTnzp1Ss2ZNs27NmjWydetWE1gBAAAAAACpm08CGpcuXZI///xTzp07Z4abROfrYSdVq1aVTZs2yejRo00h0PTp00vp0qXlww8/lOLFi/v0dwEAAAAAkGwBXLwzRQY0zp49K127dpVPP/1Ubty4YdZpQMPx79gg18+u53ypTJkyMnfuXJ+/LwAAAAAASOUBjY4dO8ry5cule/fuUr16dcmWLZvcLlqrQ2t2nDp1yvzsyYpCpAAAAAAAJBk1NFJWQEPrVfTs2VNGjRolt9MPP/wgzz33nBnmEn2Ii1UZIQAAAAAAIJUENEJDQ6VQoUJyu3Xu3FnKly8vn3/+ueTNm9c9xAUAAAAAgJSI69YUFtBo1aqVLF68WF588UW5nfbv3y+ffPKJFCtW7Lb+XgAAAAAAkAoCGk8//bSsX79e6tatK506dZICBQpImjRpYmxXtmxZ8aVKlSqZ+hkENAAAAAAAthDELCcpKqBRrVo198+rV6+O8bxVs5x069ZNevfuLSdOnJBSpUrJHXfc4fW8TuGaki385BP5aO5cORseLsWLFZNXe/WS+++7L87tv16zRiZPnSrHT5yQAnfeKd1eekmqPfSQ+/n3P/hAVq1eLSdPnTJ9cW+JEvJi587xvqddfbLyS5mzfJmEnz8vxQoWlN4vtJf7isU+Ve8fR47I1AXzZe/BP+TE6dPSo01bad6godc2N6JuyAcLF8rKb78175kjLJs0eKSGtGv6dKpLCftk1VcyZ/lyCb9wXordVVB6t20n98URFFyyZo18+e0G+ePoEfO4ROHC0qVZC6/t9fie9skiWbp2jVyOiJBSJUpInxc6yF1580pqs/DTT2XOxx/fPGaLFpVXe/aU+0qWjHP7r9eulSkffPDfMduli1StUiXWbYePHi2fLV0qPbt3l+eefVZSm0VLlsichQvdffdKt25y3z33xLn91+vXy/szZrj7rmvHjlK1UiX38/o+E6dNk83btsmly5flwdKl5ZWuXeWuO++U1GjR8mUy55NP5Oy5c1K8SBF5pcuLcl+JErFu+/ufh2TqRx/J3v375fipU9Kz0/9Jiyef9Npm++7d5v32HtgvZ8LDZdSbA6SGx/dJasJxm3QLFy+WOQvm/9t3xeTV7t3lvnvvjXP7r7/5RqZM//C/vuv0f1K1cmWvbQ7++adMmPq+bN+505wXFi5YUEYNHiJ5cueW1MSc482Z8985Xu/eCTvHO37c3Bj0PMe7fv26vDdliny3aZMcO3ZMMmbMKBUrVJBuL74oOXPmlNTG198XV/7+WyZNmybrv/tOLly8KPny5JFnn3pKmjZqJKkR1xc2kcquL1KCZIWIZsyY4V6mT58eY3Gt97WmTZvKnj175IUXXpAKFSqYKVwffPBB978p2aqvv5Z3xo+Xju3by5yZM+Xu4sWlW8+eEh4eHuv2O3ftktcHDpTGjRrJ3FmzpMbDD8srffvKgd9/d29TsEAB6dO7t8yfM0c+mDLF1BV56eWX5dy5c5KarP7+O3l39izp8PQzMmvkKClesJD0eGuYhF+4EOv2/0RGSv7cueWl51pK9qxZY93moyVL5LPVq+SV9u3l43fGyUstW8mcZUtl4ZdfSGqyetP38u5Hs6VD06Yy6+0RUrxgQekx4u04+277nl/k8YcekklvDJBpg4dK7uzZ5eXhb8kpj/30o+XLZOHKL6Vv+w7ywdC3JH1wiHnPyKtXJTVZtWaNjJs4UTq0aycfffihOUno1quXhMdxfO3cvVveGDxYGjdsKHOmT5dHqleXV/r3lwN//BFj23Xr18vuX36RnDlySGq0et06GTdlinRo3VpmT5liTlC79+0bZ9/t+uUXeXPYMHmiXj356P335ZGqVeXVAQPk94MH3UE0fXzs+HEZM2SIzHn/fcmbK5d0ffVV+fvvvyW1Wb1+vYybOk06tGwlsydMlOKFi0j3N143wdfYRP4TKfnz5JGX2r0g2eOYdeyff/6R4kUKy6svviSpGcdt0q1au1bGTX5POrRpKx9NnWaO2259Xo27737+Wd4YOkQa128gc6Z9II9UqyavvPmGHDj4X98dPXZMOnbvJoUK3CXvvzNOPv7gQ2n/fGtJly6dpCZ68ffOu+9Kxw4dZM6sWTfP8Xr0iP8cb8AA73O8Pn3c53h6vO7dt8/sx/p+o0eMMAXxe736qqQ2vv6+UOMmT5ZNW7fK4P79ZcGMGdK8aVMZM368bPj+e0ltuL5AIEtWQKNNmzYJWnzt4MGDMZY//vjD/W9KNvfjj6XJE0/IEw0bSpHChaV/nz4SEhwsy1asiHX7+QsXSpVKlaR1q1ZSuFAh6fJ//yf3lChhorAudevUkUoVK8qd+fNL0SJFpOfLL0tERITsP3BAUpOPVyyXxjVrScNHH5PCdxaQvh07SUi6YFmxbm2s25fUE9jnW8vjVavFyOJx2f3bPnm4fAWpWrac5MuVSx6rXEUqln5Afk1tfff559L4sZrSsMajUvjOO00QIiRdOlnxzbpYtx/Stbs8XbuO3F2okBTKn19e69RZopxO+fHn3e4LywVffiHtnnzK9J8GSAa++JKcOXdONvy4VVKTefPnS5NGjeSJBg1uHrOvviohISFxH7OLFplj9vnnnrt5zHbsKPfcfbcs+vRTr+1OnT4tY8aNk6EDBkjatMlKlkux5n3yiTSpX18a1a0rRQoVkn49epjPu+UrV8a6/fzPPpPKFSrI882ambu3ndu1k3uKF5eFS5aY5w8fPSo/79kjfXv0kJL33GNOtvRnDaJ9tTb2zwE7m7f4M2lSr640ql1bihQsKP26dbvZf6u+inX7kiVKSPcOHaV2jRqSLo7PvIcqVJAubdrKo1WrSmrGcZt08xYtkiYNGpgLRT1u+/fqdbPv4gj0z//0U6lSsaI837y5OW67vNDeHLeLFi92b/Pehx/IQ5UqSffOnaVE8eLmfEUvQMPiCLzZlTnHa9z4v3O8vn3j3+8WLJAqlSvfPMfTTMho53iakfHehAnyeK1aUqhgQSl1//3S55VXZM/evSZLOTXx9feFK+jRoHZtKVemjMnOeLJhQxMo+WXvXkltuL6wWYaGVUsSTJo0yUzyoZ9VWtZhy5Yt8W6/aNEiueeee8z2OlLiiy+8vxv0GmHAgAEmAJY+fXqpVauWqX9pJVsO4ilYsGC8S0p17do1E2mvVKGCe11QUJBJH9z188+xvkbX6/Oe9ANodxzb6+9YvGSJ+RLU6Gxqce36Ndn3xx9SoVRpr76rUKqUCUokVam7S8jWn3fL4b/+Mo/3HzokO/ftlSopPNMnMa5dvy77Dv4hFe4v5d1395eS3Qn8gNFslxvXr0vmjBnN479OnZKz5897vWfG0FC5r2ixBL+nHZhj9rffpGL58t7HbPny5g5tbPTYrOCxvaoc7ZiNioqSgUOHSqsWLcxJQmrk6rsKHjWUzH5Xtqzs/vXXWF+j6yuWK+e1rrL29b/b63uqYI+7uvqeGrDUu8Sprv/275cKZR707r8yD8ruPXv82raUjuM2uX23z+s4NH1Xtpzs/iWu4/YXqRD9uK1Q0b299tt3P/wgd91ZQLq9+qrUfrKJtO3SRb7Z+K2kJvGe4+2+eTMgQed4lSvL7ji2V5cvXzZDYjNmyiSphRXfF6r0fffJhk2bTCBSL7J+/OknExivFO1YtzuuL5BUCxYskF69esnAgQNl+/bt8sADD0idOnXk1KlTsW7//fffS4sWLaR9+/by008/SZMmTczys8d+M2rUKBk/frxMmTJFNm/eLBkyZDDvqRlnVknU7QUd4qEfolOnTjXFP/Xxrej2H374ofjaRx99ZDpKszI2bdpkAhnjxo0zEe7GjRtLSnT+/HkzbjQsLMxrvT4+9Oefsb7m7NmzsW6v6z19u3GjvDZggNlZcmTPLpPefVeyxjHMwo7OX7wkN6KiJCxrFq/12bJmlUN/HUvy+7Zu8qRE/P23NOv5svnw1xOvzs1bSN3qD0tqcf7ixZt9lyVa32XJIof+DeTcyqR5cyVHtjB3AOPshZsp79HfUx9roCO1OH/hQuKP2fDwGOn+ehdS17vMmjvXfIY2f+YZSa1M3+l+F0tf/HnkZm2W6LSPYtvelTJb6K67JE+uXDLpgw+kf8+ekj4kxNzV05NVrQeRmriP22zen+P6+M9/a9sgdhy3vjhuo/Vdtmxy6PDhePou5vZnz93su/Dz50wtg1kfzzPZG13/r5Ns2rJF+gwYIJPHvmPunqcGcZ7jad8dOpTwczztu2jneC6RkZEyYdIkqfP445IxQwZJLaz4vlBaX+ntsWOlYfPm5tjV87zXevWSsim81l5icX1hMymoKOjYsWOlY8eO0q5dO/NYr60///xzUzKiX79+MbZ/9913zWQgr/477G3o0KGmjubEiRPNazVwqNfjb7zxhvt6fPbs2ZI7d25ZsmSJNG/e3P8BjbVr17ov+vSDQR/fqnCiFYUVJ0+ebFJZevToIW+99Za76KgeYNqJtwpo6BeCLp6uRkZKcHCw2FX5cuVk3qxZ5kth8dKl0v+NN2TmBx/E+LCCtzWbvpevNn4rQ7q/LIULFDAZGu/MnGEu3hvUqOHv5qUIs5cuka83fS+T3hzodWccSaOpwpreruP0U1vhWatpiv/IwYNl2JgxUqtJE0mjd/DKlZOHKlY0X6KAVThuk84ZdfPYfOShqvLcv8GgEsWKm+EAny1flmoCGlbTAqH9Xn/dfNb169vX382xBR1+osMU/zd0qCk++9Pu3TJ6/HjJmT17jOwOxI7rC3uJjOUaV69vY7vGvXr1qmzbtk369+/vXqfX+TpERJMFYqPrNaPDk2ZfaLBCaaKBDofT93DJkiWLGcqir7UqoJGoEJFGmLVGhasegT6OrZ5F9NoWvjZhwgSZNm2avP76617TxJbXVLN40vRchg8fbjrXc/nfuHFiNQ24aHujF+jRx9mzZ4/1Nbo+IdvrGCWtjq3jKwf82y9Lly+X1CJr5kzm4iX8vHcRy3Pnz8dZ8DMhJsz5SFo3bmLqbOjMH/UefsTMhDJ7yWeSWmTNnPlm30UrAHruwoVb9t3cFctl9rKl8m7/102dDJfsWW6+Lvp76uPk/PdIabJmyZL4Y1bvcEQrmKVFzXS9+mnXLlNQq1HTplL5kUfMohXG3504UZ54+mlJVX2n+108fRGdro9te88Tp3vvvlvmTp0qa5culS8WLZLxI0aY6vX5U9nsOu7j9px3xpM+jqvgJ27iuPXFcRueqOP2bGzb/5u14frvUbiQ95DgwncVlBMnY09rtqM4z/G0LxJzjhfL9q5ghl4oTJowIVVlZ1j1faFDZd/78EPp0aWLVH/oIVM749kmTaRWjRoyZ9EiSU24vrAXpzgsW4bHco2r62Jz5swZkxSg2ROe9HFcNXp0fXzbu/5NzHv6QsrJeUkEDZTENpuJRp+0WM2taCTqwoULXkvvHj3EahoI0oI7W3780b1Os122/vijlL7//lhfo+v1eU+bt2wxHyzx0QKOV/8db54a3JH2DilRpIipd+HVdz/vNnUwkkq/8BzRUr/0S1X7L7W4I21aKVE4lr775WcpFc84yI+WLZXpn30q4/r1l3uLFvV6TguoauDC8z0jrlyRX34/EO972o05Zu++W7Zu2+bdd9u2Sak4pi0rFdsxu3Wr+5itX6eOudsxZ8YM96KzJei4/PFjx0qq67uffvLqOx3DXCqOqTN1/dbt273W6fSssW2v43h1yJmOh97z22/ycCorcmn6r3hx2bpjh3f/7dghpeKZPhMct8nvuxJex6Hpu+3ad3Edt/fFctz+6N5e31OL+EYfOnD46BHJm4qmbHWf423d6t13W7dK6VL/1ZuKcY7nsb37HM9je1cw4/CRI6ZAqF78pzZWfF9ov+kSFC2jSs/xnFFRkppwfYH4rnE9MzBSK1sGNLROxg6PkzyXlStXyr0JONHTwEfmzJm9lts13KRlixayZNkyWfH553Lw0CEZPmqU/P3PP9KoYUPz/IDBg2Xie++5t2/+7LPy/Q8/yJx580xGjM4J/evevfLsv3eEdKrCSZMnmyI+Ooe5psUOHjZMTp8+LbUee0xSkxYNG8myNV/L5998IwePHpVRH0wzAYkGNR41zw+eOF7emzfXq5Dob4cOmkW/1E6Hh5ufj5w47t6mWrnyMvOzT+W77dtMoctvtmyWj1eskEcqVJTUpEWDBrJs3Vr5fP16OXjsqIya/sHNvnvk5rCawe9NlPc+nufeXrMypi5aKK//XxfJmzOXqYuhy5V/C/poynWzevVl5pLFsuHHH+XA4cMyePIkyZEtm5n1JDV5rnlzWbJ8uaz48ktzzI4YM8Ycd40aNDDPa5HAiVOmuLfX8fWbNm+WOR9/bMauTv3wQ3NcPtO0qXleT0aLFSnitehQCr0rojUiUpPnnn5aln7+uaz46is5+OefMnLcOPN517BOHfP8wBEjTD0Ml+ZPPWWm2Ju7cKEZrz911iwTrNC7ai5fr18v23bskGN//SXrv/tOuvXpY2ZL0GJwqc1zTz4lS1d+KStWr5aDhw/LyIkT5O/If6Th47XN8wPHjJZJM6Z7FW377fffzaLFgE+fPWN+PuJRK0drGbi2UX+dPGF+PhFHATC74rhNOh0WsmTFClmxcqU5bke8887N85S69czzA99+WyZOm+reXqfC1JoYcxYukEOH/5SpM2fInn375Jknn3Rv83yz5mZazsUrVsiRY0dl4eLP5Nvvv5enm6TMmmc+Occ7ePC/c7x/97sY53jNmt08x5s79+Y53rRp8uuePe5zPD136dO/v+zZs0eGDR5s6kycOXvWLK4iyamFr78vNIul7AMPyPipU29+Zxw/bvbpL1avNlMLpzZcX9iHjsKzaglOxDVujhw5TMbNyZMnvdbr4zx58sT6Gl0f3/aufxPznr6Q7DnHvvzyS1NQRCujahQotnHMrhoXvqJjd1566SVToEZ/n04v8/HHH5uUmg88PuxSotq1apm01SkffGAK72il4AnvvONOqTtx8qQZv+TyQOnS8tbgwfLe1KkyacoUk/Y1ZuRIKfbvHXPdVk++VnzxhRnfpqlFJe+9V6ZNnpzqqrA//lBVUyhv2sL55uK6eKFC8s5rr7uHOJw4c0Ycjv/67nT4OWnd57+52ucuX2aWB0uWlMmDhph1vV9oL1MXzJfRH0yTcxcuSo6wbNLk8celfSpKIVaPV3noZt99svBm3xUsJO/06+/Rd2e9+u6z1avNBdFr47zvPLZv+rR0fPrmGOjnGz1hgiIjPpgql69ckdIlSphsjtRWZ6N2zZqm4JZ+2WsRsruLFZPx//uf1zHrmeXzQKlSMmzgQJk8bZo5bgvceaeMGT7cXAAFmscffdQMbZo6c6ZJ57+7aFF5d8QId9+dPHXK6+6ZVqQf+vrrMmX6dHlv+nQpkD+/jB4yRIoWLuzeRj83x02ebFKLc4SFSf3ataV9q1aSGj3+yCM3+2/OR3I2XPuviLw7dJh7yEn0/jsdflZadX3J/XjOp5+apWypUjJl1Gizbs/+36SLx/j7cVNvXpg2qFVLBvZ+RVILjtukq/3YY3L+wnl5f+aMm31XtJiMHznqv747pX333373wP33y7A33pTJ0z+U9z74wBy3Y4YOk2KF/+u7R6tXl/49e8nMeXPlfxPGy10FCsjIwUOkjMfMZalB7ccfN0Nhp0yb5n2O928av6Zcex6z5hxvyBB57/33/zvHGzXKfY6nMw1s+PbmbDDPPf+81++aMmmSqW+QWljxfTHsjTfMPjng7bfl4qVLpo5G5xdekKaNGklqw/WFfaSULPB06dJJuXLlZM2aNWamEldmjz7u2rVrrK+pUqWKeV7rWLpoUVBd70o60MCFblPm3/pIFy9eNLOddOnSxbK/xeFMRiW1Tz/9VJ599lm57777pHr16qZY53PPPWeCDEuXLpXixYubDtKpYHxt7ty5MmjQIPn937tM+fLlk8GDB5tpZJLiUiqrkH87XT+S9FlGAp6Pg32BJE2B/P5ugm05oxWMQiJdTV13Rm8nR6abUz8jCa5d93cLbMuRPsTfTbCtqAQM5UbsgkJD/d0EW8uUSguPhp+/aNl7h2XNnOhpW9u0aSPvv/++VKxY0UyusXDhQtm7d6+pe9G6dWvJnz+/uw6HTtv6yCOPyIgRI6RBgwYyf/58efvtt01iw/3/DlcaOXKkeX7WrFkmwPHmm2/Krl275Ndff5WQkJCUl6Ghf5z+8Rs3bjRRQQ1o6FSujz32mElfqly5svlDrNCyZUuzXLlyxczJnStXLkt+DwAAAAAAyZVCEjSMZs2amWFEOnuoZpBpVoWWcHAV9Tx8+LBXZs9DDz0k8+bNM9OyvvbaayZ5QWc4cQUzVJ8+fUxNy06dOplMyWrVqpn3tCqYkewMjdDQUBPUePnll02DtbKwDkHR6VvUkCFDTOTnl19+kZSODI2kI0MjGcjQSDIyNJKODI1kIkMjycjQSAYyNJKMDI2kI0Mj6cjQSJ7UmqFx9px1GRrZsyUuQyO1SFaGhgY0dPyNa8ogLTqihWNcNLqjRZF8oWzZsmY8TrZs2cwMJ/HNA69pLwAAAAAApBROSUEpGqlEsgIaJUqUMONhXDRN5aOPPpJWrVqZysyaknKXj6p/N27c2F2l1VW4BAAAAAAABKZkDTkZM2aMjB8/Xvbv32+CDStWrDCBh/Tp05sMCh0/M336dGnbtq2kdAw5STqGnCQDQ06SjCEnSceQk2RiyEmSMeQkGRhykmQMOUk6hpwkHUNOkie1Djk5FX7BsvfOFZZFAlGyAhqx+fbbb+Wzzz4z89pq9dNHH31U7ICARtIR0EgGAhpJRkAj6QhoJBMBjSQjoJEMBDSSjIBG0hHQSDoCGslDQCPxcgVoQCNZQ05io9O36uJrWjsjvroZnsIJTgAAAAAAUhAf5xLAioCGVXReXAAAAAAA7IiAhp8DGoULF05wloSLbv/7779LcrVp0ybZ7wEAAAAAAAIwoPHII48kOqBhtX/++UeuXr3qtS5z5sCcgxcAAAAAkDJFkaDh34DGzJkzJSXQ2VP69u0rCxculLNnz8Z4/gaFFgEAAAAASNWCxIb69Okja9eulcmTJ5vpYj/44AMZPHiw5MuXT2bPnu3v5gEAAAAAEKOGhlVLoEp2UdDIyEiZNm2afPHFF3Lo0CGzrlChQlK/fn3p0KGDhIT4fqqs5cuXm8BFjRo1pF27dmZWlWLFiknBggVl7ty50rJlS5//TgAAAAAAkEoyNI4ePSplypSR7t27y86dOyVnzpxm0Z91nT6n2/iaTstapEgRd70M1zSt1apVkw0bNvj89wEAAAAAkNwaGlYtgSpZAY2XXnpJ/vzzT1PL4tixY7J+/Xqz6M8LFiyQw4cPm218TYMZBw8eND/fc8895ve7MjeyZs3q898HAAAAAABS0ZCTNWvWSM+ePeXpp5+O8dwzzzwj27dvlwkTJoiv6TATzQLRWVf69esnjRo1kokTJ8q1a9dk7NixPv99AAAAAAAkRyDXukiRAY1MmTJJrly54nw+T548Zhtf0yCKS61atWTv3r2ybds2U0ejdOnSPv99AAAAAAAkBwGNFDbkRDMldCrXK1euxHju8uXLMmPGDGnfvr34yqZNm2TFihVe61zFQTt37myyNLRIKQAAAAAASN2SlaGhRT8///xzU8eiTZs2JkNC7d+/3wQawsLCTMbEZ5995vW6p556Kkm/b8iQISZ40bBhQ/N49+7dJmDStm1bKVmypIwaNcpM3Tpo0KDk/FkAAAAAAPhUIBfvtIrDmYy8l6CgWyd4OBwOr9QafXzjxo0k/b68efOawp/ly5c3j19//XVThHTjxo3m8aJFi2TgwIHy66+/Jvq9L/07UwoS7/qRY/5ugn0l8ViASJoC+f3dBNtyksmWPFev+bsFtuXIlNHfTbCva9f93QLbcqQP8XcTbCsqIsLfTbCtoNBQfzfB1jKFhUlqdPCvM5a9d+F8OSQQJStDY926dXI7nTt3TnLnzu1+rMGMevXquR9XqFBBjhw5clvbBAAAAADArVBDI4UFNHSWkdtJgxk6XWuBAgXk6tWrZhaVwYMHu5+/dOmS3HHHHbe1TQAAAAAAwGZFQbWGxa188skn4iv169c307R+++230r9/fwkNDZXq1au7n9+1a5cULVrUZ78PAAAAAABf0AQNq5ZAlayAhtayGD58uERFRcV4Ljw8XJo1a2YWXxk6dKikTZvWZIZMmzbNLOnSpXM/P336dKldu7bPfh8AAAAAAEiFQ050ZhMtzLl06VKZNWuWlChRwqxfsmSJdOnSxQwBGTdunK/aKjly5JANGzbIhQsXJGPGjJImTRqv57UoqK4HAAAAACAliZIATqVIiRkaU6dOlS+//FKOHj0qDz74oIwcOVJatWplpmXVoR87duyQbt26ia9lyZIlRjBD6TSxnhkbAAAAAACkBAw5SWEZGqpOnTryyy+/mH9fe+01s06zNoYMGWKmaAUAAAAAAEhRGRoqIiJC+vTpI1u2bJHSpUtL+vTpTS0LzdwAAAAAAAA3p221aglUyQporFu3TkqVKmXqZ2hx0G3btslPP/0khQoVkkaNGkmHDh1MHQ0AAAAAAIAUE9CoVauWZMuWzQQy+vbtK0FBQVK8eHHZuHGjqacxb948E/AAAAAAACCQRTmdli2BKlkBjTfffFM2b94s9913n9d6rZ3xyiuvmEBH7ty5k9tGAAAAAACA5AU0tFZGeHi4+XnQoEGSNm3cdUVDQkKkc+fOif0VAAAAAACkKsxykgICGlWqVJGVK1e6H2twIzQ0VNavXx9j2++//97U0QAAAAAAAPDrtK3RK6jq43/++Udu3Ljhy3YBAAAAAJBqBHKtixQT0AAAAAAAAIlDPCOFFQUFAAAAAADwBzI0AAAAAACwWPTyDfBTQOPQoUOyfft28/OFCxfMv/v375esWbN6bXfw4EEfNBEAAAAAAMCbw5nIMFFQUJA4HA6vdfoW0dd5rrdDwdBL/05Fi8S7fuSYv5tgXzY4NlKqNAXy+7sJtuWMjPR3E+zt6jV/t8C2HJky+rsJ9nXtur9bYFuO9CH+boJtRUVE+LsJthUUGurvJthaprAwSY12HLDuuqlMscA8N050hsaMGTOsaQkAAAAAAIBVAY02bdok9iUAAAAAAAQ0Smj4HrOcAAAAAAAA22GWEwAAAAAALMYsJ75HhgYAAAAAALAdMjQAAAAAALBYFAkaPkdAAwAAAAAAizmFiIavMeQEAAAAAADYDhkaAAAAAABYLIqioD5HhgYAAAAAAIghPDxcWrZsKZkzZ5asWbNK+/bt5fLly/Fu361bNylRooSkT59e7rrrLunevbtcuHDBazuHwxFjmT9/viQWGRoAAAAAAFjMjgkaLVu2lOPHj8vq1avl2rVr0q5dO+nUqZPMmzcv1u3/+usvs4wZM0ZKliwpf/75p3Tu3Nms++STT7y2nTFjhtStW9f9WAMmieVwMhmucSk83N9NsK3rR475uwn2deOGv1tgW2kK5Pd3E2zLGRnp7ybY29Vr/m6BbTkyZfR3E+zr2nV/t8C2HOlD/N0E24qKiPB3E2wrKDTU302wtUxhYZIa/bDnsGXvXfneu3z+nnv27DFBia1bt0r58uXNupUrV0r9+vXl6NGjki9fvgS9z6JFi6RVq1YSEREhadPezKnQjIzFixdLkyZNktVGhpwAAAAAAGAxzSWwarHCpk2bTNaEK5ihatWqJUFBQbJ58+YEv48ON9EhK65ghstLL70kOXLkkIoVK8r06dOT9Hcw5AQAAAAAABuLjIw0i6fg4GCzJNWJEyckV65cXus0KBEWFmaeS4gzZ87I0KFDzTAVT0OGDJHHHntMQkNDZdWqVfLiiy+a2hxabyMxyNAAAAAAAMBiUU7rluHDh0uWLFm8Fl0Xm379+sValNNz2bt3b7L/3osXL0qDBg3MsJVBgwZ5Pffmm29K1apV5cEHH5S+fftKnz59ZPTo0Yn+HWRoAAAAAABgMSvLV/bv31969erltS6u7IzevXtL27Zt432/IkWKSJ48eeTUqVNe669fv25mMtHn4nPp0iVT8DNTpkymVsYdd9wR7/aVKlUymRyaZZKYrBICGgAAAAAA2FhwIoaX5MyZ0yy3UqVKFTl//rxs27ZNypUrZ9atXbtWoqKiTAAivsyMOnXqmPYsW7ZMQkJuXZx5x44dki1btkQPkSGgAQAAAACAxew2v+i9995rsiw6duwoU6ZMMdO2du3aVZo3b+6e4eTYsWNSs2ZNmT17tinuqcGM2rVry5UrV2TOnDnmsS5Kgyhp0qSR5cuXy8mTJ6Vy5com2KFTwr799tvyyiuvJLqNBDQAAAAAAEAMc+fONUEMDVro7CZNmzaV8ePHu5/XIMe+fftMAENt377dPQNKsWLFvN7r4MGDUqhQITP8ZNKkSdKzZ08zDEe3Gzt2rAmcJJbDaeVAHhu5FB7u7ybY1vUjx/zdBPu6ccPfLbCtNAXy+7sJtuWMVgEbiXT1mr9bYFuOTBn93QT7unbd3y2wLUf6W6c6I3ZRERH+boJtBYWG+rsJtpYpLExSo/W7Dlr23o+ULiyBiFlOAAAAAACA7TDkBAAAAAAAizE4wvfI0AAAAAAAALZDhgYAAAAAABaLIkHD5whoAAAAAABgMYac+B5DTgAAAAAAgO2QoQEAAAAAgMVI0PA9MjQAAAAAAIDtkKEBAAAAAIDFokjR8DkyNAAAAAAAgO2QoQEAAAAAgMWcQoaGr5GhAQAAAAAAbIcMDQAAAAAALEYJDd8joAEAAAAAgMUoCup7DDkBAAAAAAC2Q4YGAAAAAAAWI0HD98jQAAAAAAAAtkOGBgAAAAAAFnOSouFzZGgAAAAAAADbIUMDAAAAAACLMcuJ75GhAQAAAAAAbIcMDQAAAAAALEaChu8R0AAAAAAAwGIMOfE9hpwAAAAAAADbIUMDAAAAAACLkaDhe2RoAAAAAAAA2yFDAwAAAAAAizlJ0fA5MjQAAAAAAIDtkKEBAAAAAIDFokjQ8DkyNAAAAAAAgO2QoQEAAAAAgMWooeF7BDQAAAAAALCYUwho+BpDTgAAAAAAgO2QoQEAAAAAgMUoCup7ZGgAAAAAAADbIUMDAAAAAACLURTU98jQAAAAAAAAtkOGBgAAAAAAFqOGhu+RoQEAAAAAAGyHDA0AAAAAACxGDQ3fI6ABAAAAAIDFCGj4HkNOAAAAAACA7ZChAQAAAACAxSgK6ntkaAAAAAAAANshQwMAAAAAAItRQ8P3yNAAAAAAAAC2Q4YGAAAAAAAWI0HD98jQAAAAAAAAMYSHh0vLli0lc+bMkjVrVmnfvr1cvnxZ4lOjRg1xOBxeS+fOnb22OXz4sDRo0EBCQ0MlV65c8uqrr8r169clscjQAAAAAADAYlE2TNFo2bKlHD9+XFavXi3Xrl2Tdu3aSadOnWTevHnxvq5jx44yZMgQ92MNXLjcuHHDBDPy5Mkj33//vXn/1q1byx133CFvv/12otpHQAMAAAAAAIvZrSjonj17ZOXKlbJ161YpX768WTdhwgSpX7++jBkzRvLlyxfnazWAoQGL2KxatUp+/fVX+frrryV37txSpkwZGTp0qPTt21cGDRok6dKlS3AbGXICAAAAAICNRUZGysWLF70WXZccmzZtMsNMXMEMVatWLQkKCpLNmzfH+9q5c+dKjhw55P7775f+/fvLlStXvN63VKlSJpjhUqdOHdPmX375JVFtJKABAAAAAIDFopzWLcOHD5csWbJ4LbouOU6cOGHqW3hKmzathIWFmefi8txzz8mcOXNk3bp1Jpjx0UcfSatWrbze1zOYoVyP43vf2DDkBAAAAAAAG+vfv7/06tXLa11wcHCs2/br109Gjhx5y+EmSaU1Nlw0EyNv3rxSs2ZN+f3336Vo0aLiSwQ0AAAAAACwcVHQ4ODgOAMY0fXu3Vvatm0b7zZFihQxNTBOnTrltV5nItGZT+KqjxGbSpUqmX8PHDhgAhr62i1btnhtc/LkSfNvYt5XEdAAAAAAACBA5MyZ0yy3UqVKFTl//rxs27ZNypUrZ9atXbtWoqKi3EGKhNixY4f5VzM1XO/71ltvmWCJa0iLzqKiU8OWLFkyUX8LNTQAAAAAALgNs5xYtVjh3nvvlbp165opWDWj4rvvvpOuXbtK8+bN3TOcHDt2TO655x53xoUOK9EZSzQIcujQIVm2bJmZkvXhhx+W0qVLm21q165tAhfPP/+87Ny5U7766it544035KWXXkpwlokLAQ0AAAAAABDrbCUasNAaGDpda7Vq1WTq1Knu569duyb79u1zz2KiU67qdKwatNDX6fCWpk2byvLly92vSZMmjaxYscL8q9kaWjBUgx5DhgyRxHI47TYZrkUuhYf7uwm2df3IMX83wb5u3PB3C2wrTYH8/m6CbTmTOYVXwLt6zd8tsC1Hpoz+boJ9Xbvu7xbYliN9iL+bYFtRERH+boJtBYWG+rsJtpYpLExSo0krfrTsvV9q+N/UqoGEGhoAAAAAANi4KGigYsgJAAAAAACwHTI0AAAAAACwGAkavkeGBgAAAAAAsB0yNAAAAAAAsJhTSNHwNTI0AAAAAACA7ZChAQAAAACAxZjlxPfI0AAAAAAAALZDhgYAAAAAABYjQcP3CGgAAAAAAGAxhpz4HkNOAAAAAACA7ZChAQAAAACAxUjQ8D0yNAAAAAAAgO2QoQEAAAAAgMWcpGj4HBkaAAAAAADAdsjQAAAAAADAYlEkaPgcGRoAAAAAAMB2yNAAAAAAAMBi1NDwPQIaAAAAAABYjICG7zHkBAAAAAAA2A4ZGgAAAAAAWIyioL5HhgYAAAAAALAdMjQAAAAAALAYNTR8jwwNAAAAAABgO2RoAAAAAABgMWpo+B4ZGgAAAAAAwHbI0AAAAAAAwGJOIUXD1whoAAAAAABgMYqC+h5DTgAAAAAAgO2QoQEAAAAAgMUoCup7ZGgAAAAAAADbIUMDAAAAAACLUUPD98jQAAAAAAAAtkOGBgAAAAAAFiNBw/fI0AAAAAAAALZDhgYAAAAAABaLIkXD5whoAAAAAABgMYqC+h5DTgAAAAAAgO2QoQEAAAAAgMWiSNDwOTI0AAAAAACA7ZChAQAAAACAxaih4XtkaAAAAAAAANshQwMAAAAAAIuRoOF7ZGgAAAAAAADbIUMDAAAAAACLRZGi4XMENAAAAAAAsBjxDN9jyAkAAAAAALAdAhoAAAAAAFjMaeH/rBIeHi4tW7aUzJkzS9asWaV9+/Zy+fLlOLc/dOiQOByOWJdFixa5t4vt+fnz5ye6fQw5AQAAAAAAMWgw4/jx47J69Wq5du2atGvXTjp16iTz5s2LubGIFChQwGzvaerUqTJ69GipV6+e1/oZM2ZI3bp13Y81YJJYBDQAAAAAALCY3YqC7tmzR1auXClbt26V8uXLm3UTJkyQ+vXry5gxYyRfvnwxXpMmTRrJkyeP17rFixfLs88+KxkzZvRarwGM6NsmFkNOAAAAAACwscjISLl48aLXouuSY9OmTSbo4ApmqFq1aklQUJBs3rw5Qe+xbds22bFjhxmqEt1LL70kOXLkkIoVK8r06dPFmYSADwENAAAAAAAsptfrVi3Dhw+XLFmyeC26LjlOnDghuXLl8lqXNm1aCQsLM88lxIcffij33nuvPPTQQ17rhwwZIgsXLjRDWZo2bSovvviiyf5ILIacAAAAAABgY/3795devXp5rQsODo512379+snIkSNvOdwkuf7++29Ta+PNN9+M8ZznugcffFAiIiJMnY3u3bsn6ncQ0AAAAAAAwGJJGVKRUMHBwXEGMKLr3bu3tG3bNt5tihQpYupbnDp1ymv99evXzcwnCal98cknn8iVK1ekdevWt9y2UqVKMnToUDNMJqF/hyKgAQAAAACAxaJSSE3QnDlzmuVWqlSpIufPnzd1MMqVK2fWrV27VqKiokwAIiHDTZ544okE/S6ts5EtW7ZEBTMUAQ0AAAAAAOBFa1/otKodO3aUKVOmmGlbu3btKs2bN3fPcHLs2DGpWbOmzJ492xT3dDlw4IBs2LBBvvjiC+83FZHly5fLyZMnpXLlyhISEmLqaLz99tvyyiuvSGIR0AAAAAAAwMZDTqwyd+5cE8TQoIXObqIFPMePH+9+XoMc+/btM0NLPOmsJXfeeafUrl07xnvecccdMmnSJOnZs6fpk2LFisnYsWNN4CSxHE479qoFLoWH+7sJtnX9yDF/N8G+btzwdwtsK02B/P5ugm05kzmFV8C7es3fLbAtRybv+eeRCNeu+7sFtuVIH+LvJthWVESEv5tgW0Ghof5ugq1lCguT1KjL5K8se+/JXepIICJDAwAAAAAAi0WRS+BzQb5/SwAAAAAAAGuRoQEAAAAAgMVI0PA9MjQAAAAAAIDtkKEBAAAAAIDFmI/D9whoAAAAAABgsSjiGT7HkBMAAAAAAGA7ZGgAAAAAAGAxhpz4HhkaAAAAAADAdsjQAAAAAADAYmRo+B4ZGgAAAAAAwHbI0AAAAAAAwGLMcuJ7ZGgAAAAAAADbIUMDAAAAAACLOYUUDV8joAEAAAAAgMUYcuJ7DDkBAAAAAAC2Q4YGAAAAAAAWY9pW3yNDAwAAAAAA2A4ZGgAAAAAAWIwEDd8jQwMAAAAAANgOGRoAAAAAAFgsihQNnyNDAwAAAAAA2A4ZGgAAAAAAWIxZTnyPgAYAAAAAABYjnuF7DDkBAAAAAAC2Q4YGAAAAAAAWoyio75GhAQAAAAAAbIcMDQAAAAAALEaChu+RoQEAAAAAAGyHDA0AAAAAACxGDQ3fI0MDAAAAAADYDhkaAAAAAABYzEmGhs8R0AAAAAAAwGLEM3yPIScAAAAAAMB2yNAAAAAAAMBiUUKKhq+RoQEAAAAAAGyHDA0AAAAAACxGDQ3fI0MDAAAAAADYDhkaAAAAAABYjGlbfY8MDQAAAAAAYDtkaAAAAAAAYLEoMjR8joAGAAAAAAAWI57heww5AQAAAAAAtkOGBgAAAAAAFqMoqO+RoQEAAAAAAGyHDA0AAAAAACwWRYKGz5GhAQAAAAAAYnjrrbfkoYcektDQUMmaNaskdGjNgAEDJG/evJI+fXqpVauW7N+/32ub8PBwadmypWTOnNm8b/v27eXy5cuSWAQ0AAAAAACwmF7oW7VY5erVq/LMM89Ily5dEvyaUaNGyfjx42XKlCmyefNmyZAhg9SpU0f++ecf9zYazPjll19k9erVsmLFCtmwYYN06tQp0e1jyAkAAAAAAIhh8ODB5t+ZM2dKQmhwZdy4cfLGG29I48aNzbrZs2dL7ty5ZcmSJdK8eXPZs2ePrFy5UrZu3Srly5c320yYMEHq168vY8aMkXz58klCkaEBAAAAAMBtqKFh1RIZGSkXL170WnTd7Xbw4EE5ceKEGWbikiVLFqlUqZJs2rTJPNZ/dZiJK5ihdPugoCCT0ZEYZGgAAAAAAGCxtUObWfbegwYNcmdTuAwcONCsv500mKE0I8OTPnY9p//mypXL6/m0adNKWFiYe5uEIkMDAAAAAAAb69+/v1y4cMFr0XWx6devnzgcjniXvXv3ih2QoQEAAAAAgI0FBwebJSF69+4tbdu2jXebIkWKJKkdefLkMf+ePHnSzHLioo/LlCnj3ubUqVNer7t+/bqZ+cT1+oQioAEAAAAAQIDImTOnWaxQuHBhE5RYs2aNO4Ch9Ty0NoZrppQqVarI+fPnZdu2bVKuXDmzbu3atRIVFWVqbSQGQ04AAAAAAEAMhw8flh07dph/b9y4YX7W5fLly+5t7rnnHlm8eLH5WYer9OjRQ4YNGybLli2T3bt3S+vWrc3MJU2aNDHb3HvvvVK3bl3p2LGjbNmyRb777jvp2rWrmQElMTOcKDI0AAAAAABADAMGDJBZs2a5Hz/44IPm33Xr1kmNGjXMz/v27TM1O1z69OkjERER0qlTJ5OJUa1aNTNNa0hIiHubuXPnmiBGzZo1zewmTZs2lfHjx0tiOZw6USzkUni4v5tgW9ePHPN3E+zrxg1/t8C20hTI7+8m2JbTD1N4pSpXr/m7BbblyJTR302wr2vX/d0C23Kk/+8EGokTFRHh7ybYVlBoqL+bYGuZwsL83QTYBENOAAAAAACA7RDQAAAAAAAAtkNAAwAAAAAA2A4BDQAAAAAAYDsENAAAAAAAgO0Q0AAAAAAAALZDQAMAAAAAANgOAQ0AAAAAAGA7BDQAAAAAAIDtENAAAAAAAAC2Q0ADAAAAAADYDgENAAAAAABgOwQ0AAAAAACA7RDQAAAAAAAAtkNAAwAAAAAA2A4BDQAAAAAAYDsENAAAAAAAgO0Q0AAAAAAAALZDQAMAAAAAANgOAQ0AAAAAAGA7BDQAAAAAAIDtENAAAAAAAAC2Q0ADAAAAAADYDgENAAAAAABgOwQ0AAAAAACA7RDQAAAAAAAAtkNAAwAAAAAA2A4BDQAAAAAAYDsENAAAAAAAgO0Q0AAAAAAAALZDQAMAAAAAANgOAQ0AAAAAAGA7BDQAAAAAAIDtENAAAAAAAAC2Q0ADAAAAAADYDgENAAAAAABgOwQ0AAAAAACA7RDQAAAAAAAAtkNAAwAAAAAA2A4BDQAAAAAAYDsENAAAAAAAgO0Q0AAAAAAAALZDQAMAAAAAANgOAQ0AAAAAAGA7BDQAAAAAAIDtENAAAAAAAAC2Q0ADAAAAAADYDgENAAAAAABgOwQ0AAAAAACA7RDQAAAAAAAAtkNAAwAAAAAA2A4BDQAAAAAAYDsENAAAAAAAgO0Q0AAAAAAAALZDQAMAAAAAANgOAQ0AAAAAAGA7BDQAAAAAAIDtENAAAAAAAAC2Q0ADAAAAAADYDgENAAAAAABgOwQ0AAAAAACA7RDQAAAAAAAAtkNAAwAAAAAA2A4BDQAAAAAAYDsENAAAAAAAgO0Q0AAAAAAAALZDQAMAAAAAANgOAQ0AAAAAAGA7BDQAAAAAAIDtENAAAAAAAAC243A6nU5/NwJxi4yMlOHDh0v//v0lODjY382xFfoueei/pKPvko6+Szr6Lunou+Sh/5KOvks6+i7p6DukJgQ0UriLFy9KlixZ5MKFC5I5c2Z/N8dW6Lvkof+Sjr5LOvou6ei7pKPvkof+Szr6Lunou6Sj75CaMOQEAAAAAADYDgENAAAAAABgOwQ0AAAAAACA7RDQSOG0UM/AgQMp2JME9F3y0H9JR98lHX2XdPRd0tF3yUP/JR19l3T0XdLRd0hNKAoKAAAAAABshwwNAAAAAABgOwQ0AAAAAACA7RDQAAAAAAAAtkNAAwAAAAAA2A4BDQBIoEGDBsnvv//u72YAAAAAYJYT2FFUVJQEBRGLw+1Vr149E8zYs2ePpEmTxqzTj0+Hw+HvpiGA8PkHAADwH86K/IhYUtL6zHUy/+GHH8pPP/3k7yYhAPzyyy9y5MgRWbJkiQlmrFmzxqwnmJEwfNYlzc8//ywnTpwwP7/yyiuyZcsWghmJwH6HlLL/sS8CgHU4M7rN9u3bZy6MPC+G+KJL+J1JV5+98847MnDgQPoOt0WBAgXk8uXL8tZbb0n//v3l8ccfd19o4tZcx+0nn3wiq1at8ndzUjz9XNNMoBo1asisWbOkS5cuMnbsWEmfPr2/m2bL74t//vlHrly54u8m2a7/kDwXL16Ua9euyfXr182+SJ/eWmx9xHkegFthyMlttHr1aqlbt640btxYChYsaC7IQ0NDJV26dKQRJ/Ku5bvvviv169eXJ5980t/NsYX49i/2vfi5+mfr1q1StWpVCQkJkV27dkmhQoXMiWratGn93cQUT79mzpw5YwJB1apVk4kTJzJcJwH0c07rtugF+YoVK6RmzZr0WwJ4fqaNGDFCvvvuO5Nl1axZM3n00Ueldu3a/m6ibfpPP+v05xs3bsgDDzzg76bZxv/+9z/56quvTCCtdOnSMmzYMAkLC+P7Nh6effPjjz/KpUuXJFeuXFKkSBGCuQDixafqbXT8+HHJkiWL9OjRQzZt2iStWrWSdu3ayYEDB0wUH7e2dOlSeeSRR+Tzzz+XDBkymHXE5BJ+kqDDdF5//XV54YUXzJ1yvYPEyVX8XP2zY8cOE3zU/W3AgAFmnQYz9EQf8d9p0z7LmTOnjBw5UiZPnizLly/nojwern1KT+TvuOMOyZw5swmoaXYf/ZbwY1Y/60aPHm0yXZ555hnzvfvGG2/I/Pnz/d1EWwzr1L7S85SnnnpKGjRoIP369ZPIyEh/NzHFe+2112TUqFHSpEkTefbZZ81+17BhQxPU1b4lUyP+/U77T/ute/fuJgjetWtX2b59u7+bCCAF40rmNmrdurXcf//9snv3bvn+++/N46xZs5o7lvqBvXjxYq/tuVCPSbNbnn76aTl16pR8/fXXcuHCBU7wb8F1ktCnTx9zoqDBs/DwcOnWrZsMGTKEYFocXMef6+SzZMmS5q7Rp59+KsuWLZPnnnvOrNeaGgQ14t7vPvjgA5k3b56cO3dO6tSpYwK6GtjQ4Xfw5trXXEVna9WqZYIYeuxqVsv06dPl2LFjcb4O/9m/f78JnM2dO1d69+5t9jkdqvjggw+avty5c6e/m5giub5PNbNFg4+6bNu2zVxg6kX63r17/d3EFH/TRfc7/Y548cUXpXDhwuamlR63GlhzBTX4zoh9vxs/frzMmDFDZs+ebc6VW7RoIYsWLZKIiAh/NxFASqZDTmC969evm38nT57sbNasmfPatWvu5zJmzOisUKGCMzg42PnUU085+/fv78eWphw3btzwenz16lX3z+3atXMWLlzY+cEHHzgvXbrkh9bZyxdffGH6a9u2be7HadOmdc6fP9/fTUvx+97x48fNPvbPP/+Yx3///bdz8eLFzixZsjhbtGjh3s7zmMZNBw4ccKZJk8aZJ08eZ+3atc3jzZs3O+vXr++cMGGC6efox3mg8uyHX375xfnjjz86//jjD/e6YcOGOfPnz2/+PXLkiFnXuHFj508//eSX9qY00fej/fv3O7Nnz+5csmSJ1/qtW7eaz8KFCxfe5hbah36WPf30086PPvrIPP7ss8+cWbNmdU6ZMsU8joyM9HMLUy79bu3Tp4/5ecWKFWYfnDhxonPVqlXmO6NKlSrOEydO+LuZKcKuXbtirHvuueecb731lnu/0z7T82al38Gc7yWd5/cJkNqQoXGbeN5xW7lypXzxxRfmsY5J1TtG+lhTinVMvk4NGeh33DyHSUydOtUMkdCMFk0fVnqnsnr16uaO0cKFC03BRsRN7wppzYeyZcvKggULzFhyvROi/+qdj82bN3PHKJbU16FDh5pU4QoVKphMqm+//dbU0GjUqJHMnDnTHMuakq2opREzq0zHjL/88sum9ogOn9BjVmsZ6HZ6XGvWBinY3vucFp3VLDSt9aA1glz7lw6f0Du+mvGiGX3apzrryX333SeBTjP2PL8vDh48aIYk5suXz3yf6meba98sX7685MmTx3zm4abox9/ff/9tskizZ88u33zzjfnuHT58uPzf//2fXL161Xwu6mchYp/eu2fPnqYP9fxEh0289NJLUqlSJfMZqEMnNEst0GmGo36WedLhTDo0u3LlyvLDDz+Y/U4zqzp37mwySadMmWLWI/HWrVtnMsQ1cwhIlfwdUQnEO0jvvvuus06dOuYuUdWqVZ2nT592b6PR56ioKK/tA5ne6ciXL5/zlVdecY4ePdrpcDic3bp1cz/fpk0b57333uscP36888qVK35ta0oR234zadIkZ5MmTZzr1693ZsqUyTx20SwN7eezZ8/e5pambIMHDzZ312bPnu2cNm2as3nz5s6QkBD3XUvN1NC7v7pPDhw40N/NTVE+/fRT9/6kmQZ33XWXycxYvXq1s2PHjs5nnnnG9NuTTz7p76amKPoZFxYW5vz666+dGzZsMPudZmXo94WLZqW9/PLLzs6dO7uzggI5O2jjxo3ODBkymKyWHj16OHPmzOk8ePCgeU6PSz1m9U6vK8PvwoULzrJly5rvDHgbOnSoySpQvXr1cjZo0MAZGhpq9jnPjLW6det6rcNNrnM3V4aQnrusW7fO3W+anbtmzRrO7f7tH1emjyvjTHXv3t0cw57ftUq/T2rUqOEcO3asX9prd0ePHnV26tTJ+dtvv/m7KYAlCGj4wTfffOPMli2bOUn1PBH1/DL0/DmQT1SLFCliTuzVypUrnenSpTMn+Z70pEsvNukz72DGvHnznFu2bDE/Hz582OxzehH58ccfu7fRi/J69eo5X3jhBfrPQ3h4uBkGFn1f08CaDg379ddf3f2n+2cgX1BGp32jF4y5c+d2Ll++3KybO3eu85577jEnrn/99Ze5wNQLpUceeYT97l96cq+BHh1S4qL71dq1a5133nmn8/XXX4/1OA/0fU/3H71Q1M83Ddbu3r3b63kNgOvFUdu2bZ09e/Z01qxZ03n//fcHfL9p4OLixYvuPtQbK9ovGhhSc+bMMRfk+v167Ngxs+7UqVNmuFi1atXcw2gROw2cVapUyQy103OXWrVqmcV17AZy/3n+7TqcpHr16uZmiyvQof1UvHhx5/nz501/6X6nQbTKlSsHdL8lV6B/5iF1Y8iJH+gsHVpgS9NkXWmw0afiC8RCl660V9e/J06cMKnBmqauBVM1DVuHSXTo0MEUA9VpcJVOZ6iF37TPArmQqmfaet++fc3y2Wefyfnz56VAgQLy3nvvSbZs2WT9+vWmuKXOcqJV2LVY2fvvvx/w/edJp8k8dOiQ5MiRwzx2FU7VIU86/GTChAmmr3T4ie6fOtxEh4sFoujp6nfffbfZt5o2bWqGRrRp08YMudMUYy30psNQdCjF4cOHZc2aNWa/C/QhJ0r7SIdKeBZd1P3q4YcflieeeMIUyHPtY54zEwX6UCfdfzSVWj/ndEaY6EWO9Ttj7Nixpu90ym/dPzXtP5BnKPrkk0/cw+Z0yKH2ofadfq/qUAnVsmVLeeWVV0xxVR1GoQUt9TX6vbx27VqKId9CxowZTTFa7VMdcqKfcTq02DXEzjUMORB5/u1VqlSRv/76yxTs1WFgxYoVMwXL9XuiYMGCUrFiRbP/6bDZDRs2sN8lQ6B/VyCV83dEJdC4ovOafaB3gPVOJZzOy5cvu3/WbAL17bffmsi9FtTSO2+uwlBKC2xpurpG811I43S6hzTpUInt27d7DcPRux06vKRgwYLmztuDDz5oigq6UrG58+Ht8ccfN9krrmKgendD72Tq0J3/+7//83fzUmQhS93nDh065F6n2UAdOnQw2Ro6xO7hhx/2Si8O1P0urs8qLYann3murDSXESNGmOGJDKuLnaaja2aGFhTMkSOH+e6I7Y6k574W6HcrNRNIC0Pr90VERIQZ7qoZkXv37vXqGx3+pMNz+vbt65w+fTrDnBJ5nGvf6nmK65j3LG4eyN5++21zbqe0sLFm8Gk2kA5RdB3Tes6n++eCBQvcxy77HYDYENDwE01V1wvLrl27OgPdsmXLTL0C/cLv0qWLmRFBAxx6YqWprZrir8979p1+8bVq1Yp09Wj0S//55593Dho0yP3Y81+lJ64///yzGVPp6j9OEv7jOvHUYRKa4vrSSy95PacXnP369XMGOs9jT/tDa9lo4PGBBx5wtmzZ0v2cpqvrrDCFChUyQ5502E4g8wxm/PDDDybV+syZM+4Tew006uw5GrRV586dM8MktF4QYvL8bNNhOzo7hwY1Nm3a5F6vtTQ8A22B/L3huf9pzQw9Jv/3v/85d+7caW6yuPbFuF4TqEFIX+Lmy80aLXoO7JrxRfc/V1Dj+++/j/U17HcA4uLQ//N3lkigzuChqcWaXueZBuY5u0egGDx4sEyaNElKlChh+kSHRJQsWdI8pzNyaNpmzZo15fHHH5fQ0FCZPHmynDx50p02HH24TiDTCvQ6JOKhhx4y/aRc/aOpxEeOHDEp154CcZ9L6LCTiRMnyrx580zfaeX1X3/91cyos3PnTtI3/zVmzBgzA4LONpQuXTrZt2+fDBo0SEqXLu2ezUkdPXpU5s+fbyr803ci/fr1MzNy6GeaVvefMWOGmVFHq/jrZ97Zs2dNanXmzJnNMIpt27aZYQF83sVP+0qHS+iwpzfeeMMMSdR0dT1mAznNP67P/GHDhplZS3QGnY8++khy5cplZsRKnz69Gcajn306bExnOEHieB6rOvxTh5+0a9dOApmrT/T8TYck6iwwzZs3N8/psDqdeU3PBXXYyWOPPebv5gKwCQIafqInqq6TK71ov3LlSsB90Xl+2WvAQqeV0rGmenGk409dZs2aJUuXLjU1M8qVK2dOuLRmhp7ce/ZjoIktEKEBDT0R+PPPP02QqGjRou7n9GRBL6LeffddE0jDrfdNvdDUC0m9ENdAhu57egHgGn8fqPue5/6m04rq9NM6rajSWgU6paNOuafTLWvAMnpf6TaBFtTw/LzT/unUqZOZhjBr1qymho1ORa2L1hrR41enG924caPcddddpo9ddVoCrd+SqkuXLuYzL2fOnCbYpt8XgRy8je9v18+0AQMGmEC41mzJnTu32VaDasHBwaaWhvYfkna86/nK888/L3feeacJrGktq0Dsh+ifX88884y5yeI5FavWudE6c1orTadsBYCEIKDh5xOLQP2i8+wD3QVfffVVc0d8+fLl5otMl7x587q31wtLLaKq/ZMhQwbzBRnIJ/ee/bdr1y65dOmS5M+f39xZ04KfekKgd9V0/na9U65917FjR3NRroGhQD2pT4z47oTrXWBO8G8GZjVz5Z577jF3dz3X6503zcrQgr7sb//RrB89DvXOtwZ7XF5++WUT2NBMjRYtWsR4HQG0xB+3p0+fNoV99XEgH7Oe3xeaNaWBCs2matCggfvmgV48vvbaayYQ3r59+xh9Fcjft74IZuixu2XLFnnwwQclEOnnmhaY1c85DZgpDWZoAE1vtGgGkGs//eOPP0xBUD7vACQU305+nInC84tOMxACMZih2Rd6B03T1pXOauKacUODGvpY6R1Lz6ES2o+BenLluQ/pXXG9+6iPNctHZ0PQE1Pdn/SkdNOmTWa9K21969at7irrXGTGz3VC+m+tIa/+CsS7vbFdUOtjnbFk5cqVJutAZ3xxrdfMAg3SajBS09dxc1/SC0rtL02t9rzw0cwp/VkDjxrc1QwXz/7m5D5hPGdr0u+WQD5mlednl1446swmmrm3Y8cOM2uJZkXqcauzYul+p4FIHaKjQ8M8MyUD9fvWF8EM7TsNZpQpU0YCtU/0++G3334zQ2F1KJgGwqtWrWpuvujsJq4hTfo9U6RIEffPfO4BSBB/F/EIJJ6FyHSOdy3Gdccdd5hCcIHYB6+++qopFDhmzBj3zCZKC1reddddzjfeeMO5ZcsWM497qVKlYrw+0I0dO9bMHvHNN9+Yx506dXJmyZLFXeFfC39++umnzgEDBjhnz55Ndfok8tzntD+10n8g9wGFLJNHZ83RmV8yZ87sXLduXYzntc8effRRv7QtteCYjUm/Z++8807znaref/99cw7SqFEjczy7aNFeLcbNd63vzvF09qdAL3qqfaPnHjq7ziOPPGLO8XRWJ53FJCgoyLl69Wq/tBVA6kBA4zbhiy7mxXjOnDndJ1fRjRw50lmiRAlnsWLFnFWqVDHV6wOZzvgS/YShadOm5iRVLVmyxAQzXFPb6kwwsU0PR5Xw5B+3BQoUcIaHhzsDgV5c67SNLn369HFmy5bNmT9/fjOTxPLly816nVHioYcech+zZcuWNUFI1z7IxZE37RfXbByuAKQnZkFIukA/ZmOjwUedanrGjBnm8SeffOLMmjWruWmgU3g/9thjXsE1Vx9y3Cac5zEbyOd4nv2g063quZyep2gg3PN8ZtGiRWYq71q1apm+evnll/3UYgCpAQGN24AvOm9XrlxxPvnkk87Ro0ebxzpH+8KFC5116tQxJ/k6najS/tEvwUCff1z7RC8ko/eh3kXTgNDGjRudGTNmdE6ZMsU8p8EfvesR24USkndhlDZt2oA5bvW40wudXLlymX1MF51WTzOCduzYYaZY1imVdXpbpdNirlmzxkyxrBdOZATFT/tFj20N7GrfRkdQI/EC/ZiNi34n6LF59uxZk1FVpEgR57hx48xzs2bNMsdxjRo1nD/++KO7HwlmJM28efMC/hzPlemTPXt2c16nmaSaxRf9PEYDbR9//LGZwpXvCQDJQUDjNuKL7j8tW7Z0litXzgyF0IsmTU/XIRN33323eRxdIGcW6L7iylA5fvy4191zvfMYGhpq+tHl9OnT5uR00qRJfmlvasDwsJv0JPPZZ5915smTx9xl0+FLnrp3724uhvSzLTaBfNwmpn91/9q5c6e/m2NrHLPxB8I0CK7eeecd8x2rw8LU1KlTzQ2GVq1aEURLJs1I0P0uffr0AX2Op5l7mvnjyso4f/68GUpcoUIF55AhQ8y62AJmBDUAJFVgVcfyIy3c2LJlSwkJCTEFkAKl0rUWYYuNTk2oM7toxWstCvX222+bYqA620loaKgpTuYpUAtDadBR9xWtSK+zI+g0mLr/qN69e5vCi4ULFzbTOuq24eHhpgiZTqfpKrIF3xR1034PtKJu+ndrH+gxqsfmvn373AUXXYUsdSYdLWSpVey1iJunQD1uE9O/c+bMkf79+8t9993n7+bYFsdszAKg+n2qhT614PaJEydMYV79PtYZr3SWHS38qd+zK1asMLOd6CxFroLRSBo9d9FpmDds2BAw53ix0SLu2bNnN9N5qyxZssiLL75oCoF+/fXXplB5bDOIUXgWQFIxbettoicNWrV+1apVUr58eQnUqeK00vyzzz7rXv/XX39Jvnz53K+pVauWFChQwFwcBbroFfnXrFkj7dq1M5XBtVq9nizMnz/fBIP0JFUr1+tMJnpRqfO6a19TJTz5Uyq7LowC+QRVZyrp0qWLfPrpp2YGnRo1ang937ZtWzl8+LCsXbvWb21MTQJxNo6k4piNaeDAgWY2CQ2SaZBb6fTJOnuEzrZRu3Zt872rAQ2dyWTbtm3m+yK+qaqRMBcvXjSzigXysajHoZ6X6Hmv3nBx7Vc6s07ZsmXlu+++kypVqvi7uQBSEQIat1EgfdF5nhhFnypO7wZpZoZemKtLly6ZC/DRo0fLyZMn5ccffwz4kyvPk/QDBw5IcHCwCfTs2bNHGjZsaE7UhwwZIiVLljRBIb3Lq3SaW80E0iDG9evXueORRB9//LHpx0C/MPKkwTLNrPrmm2/MxVG1atW8nuciPPE8++yzzz6TCxcumKAlEi+Qj9nox55m52nWlN480YvHoUOHyt69e2X16tVSvHhx8x2r6/U1GqjUPuP7AokV12f+r7/+agIWmlE6bNgwyZAhg3t9ixYtzPlKqVKl/NBiAKlWkgerAMmYKq5hw4buQnj6r1Zg11k7KCToPba0b9++phCjFteqXr26mc3k999/N0XdtL/iGqdL7YKkYxx03Chk6TvMxuE7gXzMeh5zWqx327ZtZlrMXbt2uddrsU8tzqjTpP/2228x9j++L5BYnvuPFiTXgp9a6N1V1P2zzz5zpkmTxtmxY0fn4sWLzXGp+2ClSpX4ngDgcwQ04Nep4lxFo/7880/3F2QgBzM8v+i1+rcWY9QgxsyZM03VcJ2vXavSa1CjaNGizubNmzu/++47v7Y5NRY006lJt27d6u+mpEgUskw+ZuPwLY7Zm8HvzJkzO++77z5npkyZnBs2bPB6XoMa9erVc4aEhDiPHTvmt3YiddFC0Rrg1kLkpUuXNtN168x16osvvnDef//9ZppvvTGjgTbXVN4ENQD4EkNOYBktTLlx40ZTlE3H1zdt2lS6d+9uhpvMnj1bOnXqJBUrVpRJkya50w8DeZiJJ03r13GoOqSkZ8+e7qE5WltEC71pPQ0t8qZp/6+88ooMHjzY301OVQJpeFhSh58MGjTIDHuiRotvClhqbYNAKmDpa4F2zHruR/p9ocNM9LtUa1XpEE+ti6HFKe+55x73azZt2iSLFi0ywzs5bnE7hjdpQVotAqqLns/oaxneBMDXCGjA0rGUf//9t7nwHjdunCxfvtwUFdQq4NOmTZMvv/zSjK2cNWsWY+896AmABiq00KcGL15//XX3c+fOnTMFGLWehs56ojVJNBjEySn8jRoayZuNI5BqPsB3JkyYYII5WgB6wIABZt2RI0fM7ENbt26Vb7/9VkqUKBHjdRSMRnI+43fu3Gn2oV69epl90HVTSgNpr732mvz2228mqFGsWLE43wMAfIVPFSQbU8X5lhb21AKBuXLlMv/+9NNP7ueyZcsmOXPmNIVCld7R1ZPS6NNlAlbxPFZ1/3TNSMRJ6q37jWAGfP3du3LlSnnzzTdNwWjX94AGvPW7WDMgH330Ufn5559jvJZgBhLL9Rmvhd4ffvhhM3Pf9u3b5fz58+5typUrZ2Y4uffee6V06dKmaHls7wEAvkSGBnyGqeJ8a9euXeaEQadn1WEnGrzQYSd169Y1fTx16lR/NxEBJrYMgzvvvNPcrdNgG24tkGfjgDVTKnft2lUWLlwYY0rlo0ePytNPPy05cuQwNxGApGB4E4CUjoAGkoyp4qyn2RmtWrUyASLt13Tp0snBgwfNNLf6M8Eg+DOYoSeqGqzkojxh9KKzefPmEhISYj4L6TdYPaXy6dOnJXv27NwZR7IxvAlASsU3HHwyllLTDvft2yfBwcFmXdWqVeWtt94yEXvNzNi/f7+5INeioN26dTNBDP2SI5gRP73gWbBggRm6c+HCBXn88cdNX2swQ09iCWbAn7Uf9CSWi/KECw0NNTWE9G4m/QZf0UxHzfzR7IynnnrKBMtcdIgiwzqRXAxvApCSEdCApWMpNajhOZbS8wKcL7mEuf/++02tAp01RvvYVT9DT2IBfxayZFaOxGnYsKEcOnTIBHcBX9JjUoMaelFZvXp1M2TRExkaSA79DtDzkPbt28sXX3xhsjFcdNjhlClT5K677jLnhABwuzHkBInCWEr/Dj/R1E6tSaL1Sjz7GLA6E4tClkDKx5TKsBLDmwCkRAQ0kCSMpfQP7dtXX33V3InLmzevv5uDAEEhS8C+mCoTvqS1z1q0aCHr1683QQ0dYuyJ/Q3A7UZAA4mmu4ymTn/55ZfSrFkzmTNnjjtIoVXVNaihQyNWrVplhkvAt3SWGC0qCNwOFLIE7MHzQlKHB2jdpXbt2vm7WUilQQ0Ncmv27Y4dO8ywYgDwF0KoSDTGUvoXwQzcThSyBOxxo8FzeJhO16pDE8+dO+fvpiEV0mw9vZnVv39/M408APgTGRpIMsZSAoFBh5dlzpzZ380AEAumVEZKwXATAP5AQAPJwlhKAABS1ixEGsxgFiJYheFNAFISAhpINsZSAgCQcqZUJjMDt3O/0+HGO3fulGzZsvm7eQACUFp/NwCpZyxlsWLFGEsJAIDFmFIZKWl409KlSwlmAPAbMjRgGYabAABgHaZUxu3C8CYAKRVXm/BJ4MJFx1LOmDHD/EwwAwAA66ZU1mCGznxFMAP+Gt5EMAOAvzHkBD6dKs41lrJJkyakHwIAYPGUyqtWrSKYAcswvAlASseQEyQZU8UBAOA/TKmM24XhTQBSKgIaSBLGUgIAAATG8KbmzZub4U3fffcdwQwAKQpFDpBojKUEAAAIrOFNGzZsIJgBIMUhQwOJwlhKAACAwMLwJgApFQENJAljKQEAAAAA/sSQEyQaU8UBAAAAAPyNgAYSjbGUAAAAAAB/Y8gJkoSxlAAAAAAAfyKgAQAAAAAAbIchJwAAAAAAwHYIaAAAAAAAANshoAEAAAAAAGyHgAYAAAAAALAdAhoAAAAAAMB2CGgAAAAAAADbIaABAAAAAABsh4AGAAAAAACwHQIaAAAAAADAdghoAAAAAAAA2yGgAQAAAAAAbIeABgAANuBwOKRr167+bgYAAECKQUADAAALgxAJWb755ht/NxUAAMB20vq7AQAApFYfffSR1+PZs2fL6tWrY6y/9957b3PLAAAA7I+ABgAAFmnVqpXX4x9++MEENKKvBwAAQOIx5AQAAD+KiIiQ3r17S4ECBSQ4OFhKlCghY8aMEafTecvXDhs2TIKCgmTChAnudV9++aVUr15dMmTIIJkyZZIGDRrIL7/84vW6tm3bSsaMGeXYsWPSpEkT83POnDnllVdekRs3bnhtO3/+fClXrpx5r8yZM0upUqXk3Xff9WEPAAAAJA0BDQAA/ESDFk888YS88847UrduXRk7dqwJaLz66qvSq1eveF/7xhtvyIABA+T999+Xbt26mXU6lEUDGBqgGDlypLz55pvy66+/SrVq1eTQoUNer9fARZ06dSR79uwmgPLII4/I//73P5k6dap7G80madGihWTLls2834gRI6RGjRry3XffWdQjAAAACceQEwAA/GTZsmWydu1ak2nx+uuvm3UvvfSSPPPMMyYLQmc1KVq0aIzXaSaFBkFmzJghbdq0MesuX74s3bt3lw4dOngFJfR5DZK8/fbbXuv/+ecfadasmQl6qM6dO0vZsmXlww8/lC5duph1n3/+ucnK+OqrryRNmjSW9wcAAEBikKEBAICffPHFFyZQoIEITzoERbM3dPiIJ12nQQ4NdsyZM8cdzHBlU5w/f95kVJw5c8a96PtXqlRJ1q1bF+P3axDDkw5V+eOPP9yPs2bNaobE6HsDAACkNGRoAADgJ3/++afky5fP1KeIbdYTfT76LCmaiTF58mQTuPC0f/9+8+9jjz0W6+/STAtPISEhpm6GJx1acu7cOffjF198URYuXCj16tWT/PnzS+3ateXZZ581w2MAAAD8jYAGAAA2UbVqVdmxY4dMnDjRBBbCwsLcz0VFRbnraOTJkyfGa9Om9f7KT8gQkly5cpnfp0NONFtEFx3m0rp1a5k1a5ZP/iYAAICkIqABAICfFCxYUL7++mu5dOmSV5bG3r173c97KlasmIwaNcoU5tQsiTVr1rhf56q1oUGIWrVq+ayN6dKlk0aNGplFgyaataGFSLX2hrYHAADAX6ihAQCAn9SvX9/MNqIZF5604KfD4TBDPaIrXbq0qb2xZ88eE2T4+++/zXqdsUSHlWjxz2vXrsV43enTpxPdvrNnz3o91ili9feryMjIRL8fAACAL5GhAQCAn2hA4tFHHzUznOi0qg888ICsWrVKli5dKj169Ih1hhNVuXJls40GRJ5++mlZsmSJCWZobY3nn3/ezFbSvHlzUyPj8OHDZrYSHa4SPXByKzpjSnh4uKnLceedd5qaHhMmTJAyZcq463wAAAD4CwENAAD8RDMedOrWAQMGyIIFC0x9ikKFCsno0aPNTCfx0SCDFuxs2rSpCWLMmzdPnnvuOVNkdMSIEeY9NItCi3nq7CXt2rVLdPtatWplpnp97733zAwqWptDp3odNGiQaTsAAIA/OZw6BxwAAAAAAICNcHsFAAAAAADYDgENAAAAAABgOwQ0AAAAAACA7RDQAAAAAAAAtkNAAwAAAAAA2A4BDQAAAAAAYDsENAAAAAAAgO0Q0AAAAAAAALZDQAMAAAAAANgOAQ0AAAAAAGA7BDQAAAAAAIDtENAAAAAAAAC2Q0ADAAAAAACI3fw/Ps760FS8mfUAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1200x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "xai_framework.visualize(exps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "file saved in ..\\results\\llm_explanations\\explanations.html\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div style='\n",
              "                # background: #f5f5f5;\n",
              "                padding: 12px;\n",
              "                margin: 12px 0;\n",
              "                border-radius: 4px;\n",
              "                border-left: 4px solid #4e79a7;\n",
              "                color:white;\n",
              "            '>\n",
              "                <h4 style='margin-top:0;color:white;'>\n",
              "                    SALIENCY <span style='color:white'></span>\n",
              "                </h4>\n",
              "                <span style='color:white'> Sentence: ['A masterpiece of how not to make a movie.'] </span><br>\n",
              "                <span style='color:white'> Model Label: positive </span>\n",
              "                <pre style='white-space:pre-wrap;margin-bottom:0;'>**1. Calculation of Saliency scores:**\n",
              "Saliency scores are calculated by computing the gradient of the model's output with respect to the input tokens. Specifically, for a given token, the score is calculated as the gradient of the model's output probability for the target class ('positive') with respect to the token's embedding. This is done using backpropagation. The resulting gradient values are then normalized to obtain the Saliency scores.\n",
              "\n",
              "**2. Interpretation of Saliency scores:**\n",
              "The Saliency scores represent the contribution of each token to the model's decision. A higher score indicates that the token has a greater impact on the model's output. In this case, the tokens with the highest scores are 'piece' (0.1956277) and 'master' (0.18051037), suggesting that these tokens are most influential in the model's decision to classify the text as 'positive'. Tokens with lower scores, such as '‚ñÅA' (0.02848242), have less impact on the model's decision. The scores can be used to identify the most important tokens that drive the model's prediction.</pre>\n",
              "            </div>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved files: ['..\\\\results\\\\llm_explanations\\\\explanations.json', '..\\\\results\\\\llm_explanations\\\\explanations.html']\n"
          ]
        }
      ],
      "source": [
        "from LLMExplanationGenerator import LLMExplanationGenerator\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "api_key = \"9b305c0291728a7f4ca861254b9bedd3e7785d5360100d93603dd1758931be6d\"\n",
        "explainer = LLMExplanationGenerator(api_key=api_key)\n",
        "\n",
        "# Generate and save explanations (returns both explanations and file paths)\n",
        "explanations, saved_files = explainer.generate_and_save_explanations(\n",
        "    exps=exps,\n",
        "    output_format=\"both\"  # or \"json\"/\"html\"\n",
        ")\n",
        "\n",
        "# Display in notebook\n",
        "explainer.display_explanations(explanations)\n",
        "\n",
        "# Print saved locations\n",
        "print(f\"Saved files: {[str(p) for p in saved_files]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üìå How to Interpret EvalxNLP's Results?\n",
        "\n",
        "This heatmap shows how different explanation methods (feature attribution techniques) assign importance scores to different words (tokens) in a sentence.\n",
        "\n",
        "Each row represents a different explanation method, and each column represents a token (word or punctuation) from the input sentence.\n",
        "\n",
        "üî¥ Red (Positive Score) ‚Üí Word contributes positively to the prediction.\n",
        "\n",
        "üîµ Blue (Negative Score) ‚Üí Word contributes negatively to the prediction.\n",
        "\n",
        "‚ö™ White/Neutral (Close to 0) ‚Üí Word has little to no effect on the prediction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üßê What Can We Learn from This?\n",
        "\n",
        "‚úîÔ∏è Debugging Sentiment Models ‚Äì If a model misclassifies a sentence, check which words influenced the decision.\n",
        "\n",
        "‚úîÔ∏è Bias Detection ‚Äì If neutral words like \"customer\" heavily influence sentiment, the model may have biases.\n",
        "\n",
        "‚úîÔ∏è Choosing the Best Explanation Method ‚Äì Compare different methods and select the most consistent one.\n",
        "\n",
        "‚úîÔ∏è Improving Model Interpretability ‚Äì Use this visualization to explain AI decisions to non-technical stakeholders.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 5: Evaluate the explanations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can evaluate explanations from different explainers initialized during the creation of the `XAIBenchmark` class using the `evaluate_single_sentence` function.\n",
        "\n",
        "- If you provide a **human rationale**, **plausibility metrics** will be calculated.  \n",
        "- If no rationale is provided, plausibility metrics **will not** be included.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Saliency': {'Complexity ‚Üì': 0.1807415634393692,\n",
              "  'Soft Comprehensiveness ‚Üë': 0.6591281890869141,\n",
              "  'Soft Sufficiency ‚Üì': 0.0002086240565404296,\n",
              "  'FAD ‚Üì': 0.0,\n",
              "  'Sparseness ‚Üë': 0.20838111639022827,\n",
              "  'AUTPC ‚Üì': 0.0}}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a=xai_framework.evaluate_single_sentence(sentence, target_label=\"positive\")\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <iframe srcdoc='\n",
              "        <!DOCTYPE html>\n",
              "        <html>\n",
              "        <head>\n",
              "            <title>Metrics Report</title>\n",
              "            <style>\n",
              "                body {\n",
              "                    font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, sans-serif;\n",
              "                    line-height: 1.6;\n",
              "                    padding: 20px;\n",
              "                    background-color: transparent;\n",
              "                    color: #333333;\n",
              "                }\n",
              "                h1 {\n",
              "                    color: #3f51b5;\n",
              "                    border-bottom: 2px solid #009688;\n",
              "                    padding-bottom: 10px;\n",
              "                    margin-top: 0;\n",
              "                }\n",
              "                h2 {\n",
              "                    color: #3f51b5;\n",
              "                    margin-bottom: 8px;\n",
              "                }\n",
              "                h3 {\n",
              "                    color: #009688;\n",
              "                    margin: 12px 0 6px 0;\n",
              "                }\n",
              "                .section {\n",
              "                    margin-bottom: 30px;\n",
              "                }\n",
              "                .metric {\n",
              "                    background: rgba(245, 245, 245, 0.9);\n",
              "                    padding: 16px;\n",
              "                    margin: 12px 0;\n",
              "                    border-radius: 8px;\n",
              "                    border-left: 4px solid #3f51b5;\n",
              "                    backdrop-filter: blur(2px);\n",
              "                }\n",
              "                .explainer {\n",
              "                    background: rgba(255, 255, 255, 0.7);\n",
              "                    padding: 12px;\n",
              "                    margin: 10px 0;\n",
              "                    border-radius: 6px;\n",
              "                    border-left: 3px solid #009688;\n",
              "                }\n",
              "                .score {\n",
              "                    font-weight: 600;\n",
              "                    color: #ff5722;\n",
              "                }\n",
              "                .interpretation {\n",
              "                    white-space: pre-wrap;\n",
              "                    background: rgba(255, 255, 255, 0.5);\n",
              "                    padding: 8px 12px;\n",
              "                    border-radius: 4px;\n",
              "                    margin-top: 8px;\n",
              "                }\n",
              "                .meta {\n",
              "                    color: #009688;\n",
              "                    font-size: 0.9em;\n",
              "                    margin-bottom: 12px;\n",
              "                }\n",
              "                .error {\n",
              "                    color: #d32f2f;\n",
              "                    padding: 10px;\n",
              "                    border-radius: 4px;\n",
              "                    background: rgba(211, 47, 47, 0.1);\n",
              "                }\n",
              "            </style>\n",
              "        </head>\n",
              "        <body>\n",
              "            <h1>Evaluation Metrics Report</h1>\n",
              "        \n",
              "            <div class=\"metric\">\n",
              "                <div class=\"metric-name\">AUTPC ‚Üì</div>\n",
              "                <div class=\"metric-meta\">\n",
              "                    <strong>Definition:</strong> Evaluates explanation faithfulness by measuring performance drop when progressively masking top-salient tokens.<br>\n",
              "                    <strong>Direction:</strong> lower is better | \n",
              "                    <strong>Range:</strong> 0-1 (0 = most faithful, performance drops immediately when critical tokens are masked)\n",
              "                </div>\n",
              "            \n",
              "                <div class=\"explainer\">\n",
              "                    <div class=\"explainer-name\">Saliency</div>\n",
              "                    <div><strong>Score:</strong> 0.000</div>\n",
              "                    <div class=\"interpretation\"><strong>Analysis:</strong> The Saliency method&apos;s score of 0.000 on the AUTPC ‚Üì metric indicates that the performance of the model drops immediately when the most critical tokens are masked, suggesting that the explanation method is highly faithful to the model&apos;s decision-making process. This score is indicative of good performance, as a lower score is better and 0 represents the most faithful explanation. In comparison to the ideal value of 0, the Saliency method&apos;s score is optimal, implying that it perfectly identifies the critical tokens that are essential to the model&apos;s performance.</div>\n",
              "                </div>\n",
              "                </div>\n",
              "            <div class=\"metric\">\n",
              "                <div class=\"metric-name\">Complexity ‚Üì</div>\n",
              "                <div class=\"metric-meta\">\n",
              "                    <strong>Definition:</strong> Measures explanation conciseness by computing the entropy of normalized attribution scores. Lower values indicate simpler explanations focusing on few key features, while higher values suggest distributed feature importance<br>\n",
              "                    <strong>Direction:</strong> lower is better | \n",
              "                    <strong>Range:</strong> 0-1\n",
              "                </div>\n",
              "            \n",
              "                <div class=\"explainer\">\n",
              "                    <div class=\"explainer-name\">Saliency</div>\n",
              "                    <div><strong>Score:</strong> 0.181</div>\n",
              "                    <div class=\"interpretation\"><strong>Analysis:</strong> The Saliency method&apos;s score of 0.181 on the Complexity ‚Üì metric indicates that it provides relatively simple explanations, focusing on a few key features rather than distributing importance across many features. This suggests good performance, as lower values are preferred, indicating concise and targeted explanations. Compared to the ideal value of 0, the Saliency method&apos;s score is relatively close, suggesting that it is effective in providing simple and focused explanations, although there is still some room for improvement towards achieving the most concise explanations possible.</div>\n",
              "                </div>\n",
              "                </div>\n",
              "            <div class=\"metric\">\n",
              "                <div class=\"metric-name\">FAD ‚Üì</div>\n",
              "                <div class=\"metric-meta\">\n",
              "                    <strong>Definition:</strong> Measures performance impact when dropping top-salient tokens, with curve steepness indicating faithfulness.<br>\n",
              "                    <strong>Direction:</strong> lower is better | \n",
              "                    <strong>Range:</strong> 0-1 (0 = perfect alignment with model&apos;s true feature importance)\n",
              "                </div>\n",
              "            \n",
              "                <div class=\"explainer\">\n",
              "                    <div class=\"explainer-name\">Saliency</div>\n",
              "                    <div><strong>Score:</strong> 0.000</div>\n",
              "                    <div class=\"interpretation\"><strong>Analysis:</strong> A score of 0.000 for the Saliency method using the FAD ‚Üì metric indicates that the method perfectly aligns with the model&apos;s true feature importance, as the score is at the lower bound of the range (0-1). This suggests good performance, as a lower score is better and indicates faithfulness to the model&apos;s decision-making process. In comparison to ideal values, a score of 0.000 is ideal, as it represents perfect alignment, meaning the Saliency method is effectively identifying the most important features driving the model&apos;s predictions.</div>\n",
              "                </div>\n",
              "                </div>\n",
              "            <div class=\"metric\">\n",
              "                <div class=\"metric-name\">Soft Comprehensiveness ‚Üë</div>\n",
              "                <div class=\"metric-meta\">\n",
              "                    <strong>Definition:</strong> Measures how much the model&apos;s prediction relies on important tokens by probabilistically perturbing them based on importance scores.<br>\n",
              "                    <strong>Direction:</strong> higher is better | \n",
              "                    <strong>Range:</strong> 0-1 (1 = model completely relies on the important tokens)\n",
              "                </div>\n",
              "            \n",
              "                <div class=\"explainer\">\n",
              "                    <div class=\"explainer-name\">Saliency</div>\n",
              "                    <div><strong>Score:</strong> 0.659</div>\n",
              "                    <div class=\"interpretation\"><strong>Analysis:</strong> The Saliency method&apos;s score of 0.659 on the Soft Comprehensiveness ‚Üë metric indicates that the model&apos;s predictions rely on important tokens to a moderate extent, but not completely. This score suggests good, but not exceptional, performance, as it falls somewhat short of the ideal value of 1, which would indicate that the model&apos;s predictions are entirely based on the important tokens. In comparison to the ideal value, the Saliency method&apos;s score is about 65% of the way to the optimal value, leaving room for improvement in terms of relying on important tokens.</div>\n",
              "                </div>\n",
              "                </div>\n",
              "            <div class=\"metric\">\n",
              "                <div class=\"metric-name\">Soft Sufficiency ‚Üì</div>\n",
              "                <div class=\"metric-meta\">\n",
              "                    <strong>Definition:</strong> Measures how well important tokens preserve the model&apos;s prediction when other tokens are perturbed.<br>\n",
              "                    <strong>Direction:</strong> lower is better | \n",
              "                    <strong>Range:</strong> 0-1 (1 = important tokens perfectly maintain the prediction)\n",
              "                </div>\n",
              "            \n",
              "                <div class=\"explainer\">\n",
              "                    <div class=\"explainer-name\">Saliency</div>\n",
              "                    <div><strong>Score:</strong> 0.000</div>\n",
              "                    <div class=\"interpretation\"><strong>Analysis:</strong> A score of 0.000 on the Soft Sufficiency ‚Üì metric for the Saliency method means that the important tokens identified by this method perfectly preserve the model&apos;s prediction when other tokens are perturbed, indicating that these tokens are crucial for the model&apos;s decision-making process. This suggests good performance, as a lower score is better and 0.000 is the lowest possible score, indicating that the important tokens maintain the prediction flawlessly. In comparison to ideal values, a score of 0.000 is optimal, as it is the lowest possible value on the 0-1 range, where 1 would indicate that the important tokens have no effect on maintaining the prediction.</div>\n",
              "                </div>\n",
              "                </div>\n",
              "            <div class=\"metric\">\n",
              "                <div class=\"metric-name\">Sparseness ‚Üë</div>\n",
              "                <div class=\"metric-meta\">\n",
              "                    <strong>Definition:</strong> Quantifies explanation concentration using the Gini index. Measures whether only highly-attributed features are truly predictive.<br>\n",
              "                    <strong>Direction:</strong> higher is better | \n",
              "                    <strong>Range:</strong> 0-1 (1 = perfectly sparse, 0 = uniform importance across all features)\n",
              "                </div>\n",
              "            \n",
              "                <div class=\"explainer\">\n",
              "                    <div class=\"explainer-name\">Saliency</div>\n",
              "                    <div><strong>Score:</strong> 0.208</div>\n",
              "                    <div class=\"interpretation\"><strong>Analysis:</strong> The score of 0.208 for the Saliency method indicates that the explanation concentration is relatively low, meaning the importance is not highly concentrated on a few features, but rather spread out across many features. This suggests that the method is not performing well in terms of sparseness, as a higher score would indicate that only a few highly-attributed features are truly predictive. Compared to the ideal value of 1, which represents perfectly sparse explanations, the Saliency method&apos;s score is significantly lower, indicating room for improvement in concentrating importance on the most predictive features.</div>\n",
              "                </div>\n",
              "                </div></body></html>'\n",
              "                style=\"width:100%; height:600px; border:none;\"\n",
              "                sandbox=\"allow-scripts\">\n",
              "        </iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from EvaluationMetricsExplainer import EvaluationMetricsExplainer\n",
        "\n",
        "api_key = \"9b305c0291728a7f4ca861254b9bedd3e7785d5360100d93603dd1758931be6d\"\n",
        "explainer = EvaluationMetricsExplainer(api_key=api_key)\n",
        "\n",
        "results = explainer.explain_results(a)\n",
        "json_path, html_path = explainer.save_results(results)\n",
        "\n",
        "# Display in notebook\n",
        "explainer.display_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, you can use each evaluator‚Äôs `compute` function to assess individual metrics.  \n",
        "For **plausibility metrics**, make sure to **add a rationale** to the explanation object before evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.26279761904761906)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ig_exps.rationale= [0, 1, 0, 1, 0,0,0,0,1,0,1,0]\n",
        "iou_f1.compute(ig_exps)\n",
        "auprc.compute(ig_exps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 1: Load the model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 2: Load the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can load any text classification dataset using the `load_fields_from_dataset` function, provided you specify the `LoadDatasetArgs` as follows:  \n",
        "- **`dataset_name`** - The field containing dataset name on huggingface/ \"csv\", \"json\", etc for local files\n",
        "- **`input_text_field`** ‚Äì The field containing the text data.  \n",
        "- **`label_field`** ‚Äì The field containing the class labels.  \n",
        "- **`rationale_field`** (optional) ‚Äì The field marking important tokens in the text.\n",
        "- **`dataset_files`** (optional) - The field containing filepath to a local csv/excel file e.g. [\"healthFC_annotated.csv\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "HuggingFace Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ]
        }
      ],
      "source": [
        "from dataset_loaders.dataset_loader import LoadDatasetArgs,load_fields_from_dataset\n",
        "\n",
        "dataset_args_ = LoadDatasetArgs(\n",
        "    dataset_name=\"eraser-benchmark/movie_rationales\",\n",
        "    text_field=\"review\",\n",
        "    label_field=\"label\",\n",
        "    rationale_field=\"evidences\",\n",
        "    dataset_split=\"test\",\n",
        ")\n",
        "\n",
        "# Load the dataset fields\n",
        "results = load_fields_from_dataset(dataset_args_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(\"there may not be a critic alive who harbors as much affection for shlock monster movies as i do .\\ni delighted in the sneaky - smart entertainment of ron underwood 's big - underground - worm yarn tremors ; i even giggled at last year 's critically - savaged big - underwater - snake yarn anaconda .\\nsomething about these films causes me to lower my inhibitions and return to the saturday afternoons of my youth , spent in the company of ghidrah , the creature from the black lagoon and the blob .\\ndeep rising , a big - undersea - serpent yarn , does n't quite pass the test .\\nsure enough , all the modern monster movie ingredients are in place : a conspicuously multi - ethnic / multi - national collection of bait .\\n.. excuse me , characters ; an isolated location , here a derelict cruise ship in the south china sea ; some comic relief ; a few cgi - enhanced gross - outs ; and at least one big explosion .\\nthere are too - cheesy - to - be - accidental elements , like a sleazy shipping magnate ( anthony heald ) who also appears to have a doctorate in marine biology , or a slinky international jewel thief ( famke janssen ) whose white cotton tank top hides a heart of gold .\\nas it happens , deep rising is noteworthy primarily for the mechanical manner in which it spits out all those ingredients .\\na terrorist crew , led by squinty - eyed mercenary hanover ( wes studi ) and piloted by squinty - eyed boat captain finnegan ( treat williams ) , shows up to loot the cruise ship ; the sea monsters show up to eat the mercenary crew ; a few survivors make it to the closing credits .\\nand up go the lights .\\nit 's hard to work up much enthusiasm for this sort of joyless film - making , especially when a monster moview should make you laugh every time it makes you scream .\\nhere , the laughs are provided almost entirely by kevin\\nj. o'connor , generally amusing as the crew 's fraidy - cat mechanic .\\nwriter / director stephen sommers seems most concerned with creating a tone of action - horror menace -- something over - populated with gore - drenched skeletons , something where the gunfire and special effects are taken a bit too seriously .\\ndeep rising is missing that one unmistakable cue that we 're expected to have a ridiculous good time , not hide our eyes .\\ncase it point , comparing deep rising to its recent cousin anaconda .\\nin deep rising , one of the creature 's victims is regurgitated back into view , partially digested and still alive .\\nhe shrieks in horror at his freakish appearance and pain , in a moment a bit too disturbing to be laughable .\\nin anaconda , we also see a regurgitated victim , partially digested and still alive .\\nhe looks at another character .\\n.. and winks .\\nmake no mistake , deep rising has anaconda beat all to heck when it comes to technical proficiency and pacing .\\nit 's also gloomy , uninspired and not nearly enough fun .\\ni do n't ask much of my monster movies , but i do ask that they act like monster movies .\\nyou do n't have to show me a fantastically impressive , massive beast with tentacles a - flailing .\\njust show me the massive beast burping , and i 'll figure you get the point .\",\n",
              " 0,\n",
              " array(['i even giggled',\n",
              "        'something about these films causes me to lower my inhibitions and return to the saturday afternoons of my',\n",
              "        \"does n't quite pass the test . sure enough\",\n",
              "        'too - cheesy - to - be - accidental',\n",
              "        'noteworthy primarily for the mechanical manner in which',\n",
              "        \"it 's hard to work up much enthusiasm for this sort of joyless film - making , especially when a monster moview should make you laugh every time it makes you scream\",\n",
              "        \"deep rising is missing that one unmistakable cue that we 're expected to have a ridiculous good time , not hide our eyes\",\n",
              "        \"deep rising has anaconda beat all to heck when it comes to technical proficiency and pacing . it 's also gloomy , uninspired and not nearly enough\",\n",
              "        \"i do n't ask much of my monster movies , but i do ask that they act like monster movies . you do n't have to show me a fantastically impressive , massive beast with tentacles a - flailing . just show me the massive beast burping , and i 'll figure you get the\"],\n",
              "       dtype=object))"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_texts= results['text']\n",
        "labels= results['labels']\n",
        "rationales= results['rationales']\n",
        "input_texts[0], labels[0], rationales[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Local CSV File Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ]
        }
      ],
      "source": [
        "# dataset_args_csv = LoadDatasetArgs(\n",
        "#     dataset_name=\"csv\",\n",
        "#     text_field=\"en_text\",\n",
        "#     label_field=\"label\",\n",
        "#     rationale_field=\"en_explanation\",\n",
        "#     dataset_split=\"train\",\n",
        "#     dataset_files=[\"healthFC_annotated.csv\"]\n",
        "# )\n",
        "\n",
        "# # Load the dataset fields\n",
        "# results = load_fields_from_dataset(dataset_args_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected Outputs:\n",
        "\n",
        "- **`input_texts`** ‚Üí An array of strings (text samples).  \n",
        "- **`labels`** ‚Üí An array of strings (corresponding class labels).  \n",
        "- **`rationale`** ‚Üí An array of binary values (`0` or `1`), indicating which tokens in the text are important.  \n",
        "\n",
        "*You can preprocess or postprocess the data to ensure it is in the expected format if needed.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataset_loaders.movie_rationales import MovieRationalesProcessor\n",
        "\n",
        "mv= MovieRationalesProcessor(tokenizer)\n",
        "processed_rationales= mv.process_dataset(input_texts, labels, rationales)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Select a sub-sample of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Select a sub-sample if you want\n",
        "input_texts_sample=results['text'][:2]\n",
        "labels_sample=results['labels'][:2]\n",
        "rationale_sample= processed_rationales[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 3: Generate Explanations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing instance 0\n",
            "Found cached explanation for Saliency\n",
            "Processing instance 1\n",
            "Found cached explanation for Saliency\n",
            "Saved explanations to ../results/scores/movie_04_02.json\n"
          ]
        }
      ],
      "source": [
        "exp_scores= xai_framework.get_feature_importance_for_dataset(input_texts_sample,labels_sample,rationale_sample,output_file=\"../results/scores/movie_04_02.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "file saved in ..\\results\\llm_explanations\\explanations.html\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div style='\n",
              "                # background: #f5f5f5;\n",
              "                padding: 12px;\n",
              "                margin: 12px 0;\n",
              "                border-radius: 4px;\n",
              "                border-left: 4px solid #4e79a7;\n",
              "                color:white;\n",
              "            '>\n",
              "                <h4 style='margin-top:0;color:white;'>\n",
              "                    SALIENCY <span style='color:white'></span>\n",
              "                </h4>\n",
              "                <span style='color:white'> Sentence: [\"there may not be a critic alive who harbors as much affection for shlock monster movies as i do .\\ni delighted in the sneaky - smart entertainment of ron underwood 's big - underground - worm yarn tremors ; i even giggled at last year 's critically - savaged big - underwater - snake yarn anaconda .\\nsomething about these films causes me to lower my inhibitions and return to the saturday afternoons of my youth , spent in the company of ghidrah , the creature from the black lagoon and the blob .\\ndeep rising , a big - undersea - serpent yarn , does n't quite pass the test .\\nsure enough , all the modern monster movie ingredients are in place : a conspicuously multi - ethnic / multi - national collection of bait .\\n.. excuse me , characters ; an isolated location , here a derelict cruise ship in the south china sea ; some comic relief ; a few cgi - enhanced gross - outs ; and at least one big explosion .\\nthere are too - cheesy - to - be - accidental elements , like a sleazy shipping magnate ( anthony heald ) who also appears to have a doctorate in marine biology , or a slinky international jewel thief ( famke janssen ) whose white cotton tank top hides a heart of gold .\\nas it happens , deep rising is noteworthy primarily for the mechanical manner in which it spits out all those ingredients .\\na terrorist crew , led by squinty - eyed mercenary hanover ( wes studi ) and piloted by squinty - eyed boat captain finnegan ( treat williams ) , shows up to loot the cruise ship ; the sea monsters show up to eat the mercenary crew ; a few survivors make it to the closing credits .\\nand up go the lights .\\nit 's hard to work up much enthusiasm for this sort of joyless film - making , especially when a monster moview should make you laugh every time it makes you scream .\\nhere , the laughs are provided almost entirely by kevin\\nj. o'connor , generally amusing as the crew 's fraidy - cat mechanic .\\nwriter / director stephen sommers seems most concerned with creating a tone of action - horror menace -- something over - populated with gore - drenched skeletons , something where the gunfire and special effects are taken a bit too seriously .\\ndeep rising is missing that one unmistakable cue that we 're expected to have a ridiculous good time , not hide our eyes .\\ncase it point , comparing deep rising to its recent cousin anaconda .\\nin deep rising , one of the creature 's victims is regurgitated back into view , partially digested and still alive .\\nhe shrieks in horror at his freakish appearance and pain , in a moment a bit too disturbing to be laughable .\\nin anaconda , we also see a regurgitated victim , partially digested and still alive .\\nhe looks at another character .\\n.. and winks .\\nmake no mistake , deep rising has anaconda beat all to heck when it comes to technical proficiency and pacing .\\nit 's also gloomy , uninspired and not nearly enough fun .\\ni do n't ask much of my monster movies , but i do ask that they act like monster movies .\\nyou do n't have to show me a fantastically impressive , massive beast with tentacles a - flailing .\\njust show me the massive beast burping , and i 'll figure you get the point .\"] </span><br>\n",
              "                <span style='color:white'> Model Label: negative </span>\n",
              "                <pre style='white-space:pre-wrap;margin-bottom:0;'>**1. Calculation of Saliency Scores:**\n",
              "Saliency scores are calculated using the Saliency method, which is a technique used to explain the decisions made by a neural network. The scores are computed by taking the derivative of the model's output with respect to the input features (in this case, the tokens). The derivative represents the change in the output with respect to a small change in the input feature. The scores are then normalized to obtain a measure of the importance of each token.\n",
              "\n",
              "**2. :**\n",
              "To interpret the Saliency scores with respect to the model's decision, we can follow these steps:\n",
              "* **Identify the target class**: The target class is 'negative', which means we are interested in understanding why the model classified the text as 'negative'.\n",
              "* **Look for tokens with high scores**: Tokens with high Saliency scores are more important for the model's decision. In this case, we can see that tokens like '‚ñÅnot', '‚ñÅbe', '‚ñÅa', '‚ñÅcritic', '‚ñÅali', '‚ñÅve', '‚ñÅwho', '‚ñÅhar', '‚ñÅbor', '‚ñÅs' have relatively high scores.\n",
              "* **Analyze the context**: By analyzing the context in which these tokens appear, we can gain insights into why the model classified the text as 'negative'. For example, the presence of words like 'not', 'critic', and 'negative' phrases may indicate that the text is expressing a negative opinion.\n",
              "* **Consider the model's bias**: It's also important to consider the model's bias and whether the Saliency scores are reflecting the model's biases rather than the actual importance of the tokens.\n",
              "\n",
              "By following these steps, we can use the Saliency scores to gain a deeper understanding of why the model classified the text as 'negative' and identify the most important tokens that contributed to this decision.</pre>\n",
              "            </div>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div style='\n",
              "                # background: #f5f5f5;\n",
              "                padding: 12px;\n",
              "                margin: 12px 0;\n",
              "                border-radius: 4px;\n",
              "                border-left: 4px solid #4e79a7;\n",
              "                color:white;\n",
              "            '>\n",
              "                <h4 style='margin-top:0;color:white;'>\n",
              "                    SALIENCY <span style='color:white'></span>\n",
              "                </h4>\n",
              "                <span style='color:white'> Sentence: ['renee zellweger stars as sonia , a young jewish wife and mother frustrated by the constraints of her hasidic community in brooklyn .\\nher husband ( glenn fitzgerald ) is a religious scholar whose all - in - a - day\\'s - work attitude on sex fails to tame the \" fire \" she feels within , as so she confesses to the rebbe ( after hearing her fiery confession , the rebbe suddenly gets frisky with his pleasantly surprised wife -- and dies the next morning ) .\\nsensing her frustration , her husband \\'s brother ( christopher eccleston ) gives her a job in his jewelry brokering business in exchange for raw , passionless sex that just fans sonia \\'s still - burning flame .\\non the job , sonia befriends ramon ( allen payne ) , a cool blast of hunky puerto rican water who does his own jewelry designs when not working as a grunt in an upscale jewelry store .\\ncan fire - taming be far be that far behind for the ever - smoldering sonia ?\\njust about everything in writer - director boaz yakin \\'s rings false , starting with the improbably cast zellweger , who does an adequate enough acting job but simply looks too waspy for the role .\\na better fit would have been julianna margulies , who outshines zellweger as sonia \\'s take - no - crap sister - in - law .\\nsome of sonia \\'s baby steps toward liberation , such as indulging in a non - kosher egg roll in chinatown , come off as silly .\\nyakin attempts to spice up the proceedings with a touch of magical realism -- in the form of the recurring presence of sonia \\'s long - dead brother \\'s ghost -- make the story feel even more trite than it already is .\\n\"\\ni did n\\'t know what to expect .\\nit \\'s like something you chase for so long , but then you do n\\'t know how to react when you get it .\\ni still do n\\'t know how to react .\\n\"\\n--michael\\njordan , on winning his first nba championship in 1991 .\\n..\\nor , my thoughts after meeting him on november 21 , 1997'] </span><br>\n",
              "                <span style='color:white'> Model Label: negative </span>\n",
              "                <pre style='white-space:pre-wrap;margin-bottom:0;'>**1. How are Saliency scores calculated:**\n",
              "\n",
              "Saliency scores are calculated by computing the gradient of the model's output with respect to the input tokens. Specifically, for a given input token, the saliency score is calculated as the absolute value of the gradient of the model's output (i.e., the predicted probability of the target class) with respect to the token's embedding. This gradient represents the change in the model's output when the token's embedding is perturbed. The saliency scores are then normalized to ensure they are comparable across different tokens.\n",
              "\n",
              "**2. How to interpret Saliency scores with respect to the model's decision:**\n",
              "\n",
              "To interpret the Saliency scores, you can follow these steps:\n",
              "\n",
              "* **Identify important tokens:** Tokens with high saliency scores are more important for the model's decision. In this case, the target class is 'negative', so tokens with high scores are likely to contribute to the model's prediction of a negative outcome.\n",
              "* **Understand the direction of influence:** Since Saliency scores are based on gradients, they can be positive or negative. However, in this case, we are only interested in the absolute values of the scores, which represent the magnitude of the influence.\n",
              "* **Analyze the context:** Consider the tokens with high saliency scores in the context of the entire input sequence. Look for patterns, such as phrases or sentences, that may be contributing to the model's decision.\n",
              "* **Compare scores:** Compare the saliency scores of different tokens to understand their relative importance. Tokens with higher scores are more influential in the model's decision.\n",
              "\n",
              "By analyzing the Saliency scores, you can gain insights into which tokens are driving the model's prediction of a negative outcome and how they are contributing to the overall decision.</pre>\n",
              "            </div>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved files: ['..\\\\results\\\\llm_explanations\\\\explanations.json', '..\\\\results\\\\llm_explanations\\\\explanations.html']\n"
          ]
        }
      ],
      "source": [
        "from LLMExplanationGenerator import LLMExplanationGenerator\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "api_key = \"9b305c0291728a7f4ca861254b9bedd3e7785d5360100d93603dd1758931be6d\"\n",
        "explainer = LLMExplanationGenerator(api_key=api_key)\n",
        "\n",
        "# Generate and save explanations (returns both explanations and file paths)\n",
        "explanations, saved_files = explainer.generate_and_save_explanations(\n",
        "    exps=exp_scores,\n",
        "    output_format=\"both\"  # or \"json\"/\"html\"\n",
        ")\n",
        "\n",
        "# Display in notebook\n",
        "explainer.display_explanations(explanations)\n",
        "\n",
        "# Print saved locations\n",
        "print(f\"Saved files: {[str(p) for p in saved_files]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 4: Evaluate Explanations and generate table "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Explainer: Saliency\n",
            "Computing value for Complexity ‚Üì\n",
            "Computing value for Soft Comprehensiveness ‚Üë\n",
            "Computing value for Soft Sufficiency ‚Üì\n",
            "Computing value for FAD ‚Üì\n",
            "Computing value for Sparseness ‚Üë\n",
            "Computing value for AUTPC ‚Üì\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_ba7f3_row0_col0, #T_ba7f3_row0_col1, #T_ba7f3_row0_col2, #T_ba7f3_row0_col3 {\n",
              "  background-color: rgb(247, 100, 100);\n",
              "  color: black;\n",
              "  font-weight: bold;\n",
              "}\n",
              "#T_ba7f3_row0_col4, #T_ba7f3_row0_col5 {\n",
              "  background-color: rgb(197, 100, 197);\n",
              "  color: black;\n",
              "  font-weight: bold;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_ba7f3\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Evaluator</th>\n",
              "      <th id=\"T_ba7f3_level0_col0\" class=\"col_heading level0 col0\" >Soft Comprehensiveness ‚Üë</th>\n",
              "      <th id=\"T_ba7f3_level0_col1\" class=\"col_heading level0 col1\" >Soft Sufficiency ‚Üì</th>\n",
              "      <th id=\"T_ba7f3_level0_col2\" class=\"col_heading level0 col2\" >AUTPC ‚Üì</th>\n",
              "      <th id=\"T_ba7f3_level0_col3\" class=\"col_heading level0 col3\" >FAD ‚Üì</th>\n",
              "      <th id=\"T_ba7f3_level0_col4\" class=\"col_heading level0 col4\" >Complexity ‚Üì</th>\n",
              "      <th id=\"T_ba7f3_level0_col5\" class=\"col_heading level0 col5\" >Sparseness ‚Üë</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Explainer</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_ba7f3_level0_row0\" class=\"row_heading level0 row0\" >Saliency</th>\n",
              "      <td id=\"T_ba7f3_row0_col0\" class=\"data row0 col0\" >0.255827</td>\n",
              "      <td id=\"T_ba7f3_row0_col1\" class=\"data row0 col1\" >0.001541</td>\n",
              "      <td id=\"T_ba7f3_row0_col2\" class=\"data row0 col2\" >0.250000</td>\n",
              "      <td id=\"T_ba7f3_row0_col3\" class=\"data row0 col3\" >1.000000</td>\n",
              "      <td id=\"T_ba7f3_row0_col4\" class=\"data row0 col4\" >0.011881</td>\n",
              "      <td id=\"T_ba7f3_row0_col5\" class=\"data row0 col5\" >0.289792</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x14a3433ed50>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics= xai_framework.compute_evaluation_metrics(exp_scores)\n",
        "xai_framework.create_pivot_table(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <iframe srcdoc='\n",
              "        <!DOCTYPE html>\n",
              "        <html>\n",
              "        <head>\n",
              "            <title>Metrics Report</title>\n",
              "            <style>\n",
              "                body {\n",
              "                    font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, sans-serif;\n",
              "                    line-height: 1.6;\n",
              "                    padding: 20px;\n",
              "                    background-color: transparent;\n",
              "                    color: #333333;\n",
              "                }\n",
              "                h1 {\n",
              "                    color: #3f51b5;\n",
              "                    border-bottom: 2px solid #009688;\n",
              "                    padding-bottom: 10px;\n",
              "                    margin-top: 0;\n",
              "                }\n",
              "                h2 {\n",
              "                    color: #3f51b5;\n",
              "                    margin-bottom: 8px;\n",
              "                }\n",
              "                h3 {\n",
              "                    color: #009688;\n",
              "                    margin: 12px 0 6px 0;\n",
              "                }\n",
              "                .section {\n",
              "                    margin-bottom: 30px;\n",
              "                }\n",
              "                .metric {\n",
              "                    background: rgba(245, 245, 245, 0.9);\n",
              "                    padding: 16px;\n",
              "                    margin: 12px 0;\n",
              "                    border-radius: 8px;\n",
              "                    border-left: 4px solid #3f51b5;\n",
              "                    backdrop-filter: blur(2px);\n",
              "                }\n",
              "                .explainer {\n",
              "                    background: rgba(255, 255, 255, 0.7);\n",
              "                    padding: 12px;\n",
              "                    margin: 10px 0;\n",
              "                    border-radius: 6px;\n",
              "                    border-left: 3px solid #009688;\n",
              "                }\n",
              "                .score {\n",
              "                    font-weight: 600;\n",
              "                    color: #ff5722;\n",
              "                }\n",
              "                .interpretation {\n",
              "                    white-space: pre-wrap;\n",
              "                    background: rgba(255, 255, 255, 0.5);\n",
              "                    padding: 8px 12px;\n",
              "                    border-radius: 4px;\n",
              "                    margin-top: 8px;\n",
              "                }\n",
              "                .meta {\n",
              "                    color: #009688;\n",
              "                    font-size: 0.9em;\n",
              "                    margin-bottom: 12px;\n",
              "                }\n",
              "                .error {\n",
              "                    color: #d32f2f;\n",
              "                    padding: 10px;\n",
              "                    border-radius: 4px;\n",
              "                    background: rgba(211, 47, 47, 0.1);\n",
              "                }\n",
              "            </style>\n",
              "        </head>\n",
              "        <body>\n",
              "            <h1>Evaluation Metrics Report</h1>\n",
              "        \n",
              "            <div class=\"metric\">\n",
              "                <div class=\"metric-name\">AUTPC ‚Üì</div>\n",
              "                <div class=\"metric-meta\">\n",
              "                    <strong>Definition:</strong> Evaluates explanation faithfulness by measuring performance drop when progressively masking top-salient tokens.<br>\n",
              "                    <strong>Direction:</strong> lower is better | \n",
              "                    <strong>Range:</strong> 0-1 (0 = most faithful, performance drops immediately when critical tokens are masked)\n",
              "                </div>\n",
              "            \n",
              "                <div class=\"explainer\">\n",
              "                    <div class=\"explainer-name\">Saliency</div>\n",
              "                    <div><strong>Score:</strong> 0.250</div>\n",
              "                    <div class=\"interpretation\"><strong>Analysis:</strong> The Saliency method&apos;s score of 0.250 on the AUTPC ‚Üì metric indicates that the performance of the model drops gradually as the top-salient tokens are masked, with a moderate level of faithfulness. This score suggests that the explanation method is somewhat effective in identifying critical tokens, but not extremely faithful, as a lower score (closer to 0) would indicate a more immediate performance drop when critical tokens are masked. Compared to the ideal value of 0, which represents the most faithful explanation, a score of 0.250 is relatively high, indicating that there is room for improvement in the explanation method&apos;s faithfulness.</div>\n",
              "                </div>\n",
              "                </div>\n",
              "            <div class=\"metric\">\n",
              "                <div class=\"metric-name\">Complexity ‚Üì</div>\n",
              "                <div class=\"metric-meta\">\n",
              "                    <strong>Definition:</strong> Measures explanation conciseness by computing the entropy of normalized attribution scores. Lower values indicate simpler explanations focusing on few key features, while higher values suggest distributed feature importance<br>\n",
              "                    <strong>Direction:</strong> lower is better | \n",
              "                    <strong>Range:</strong> 0-1\n",
              "                </div>\n",
              "            \n",
              "                <div class=\"explainer\">\n",
              "                    <div class=\"explainer-name\">Saliency</div>\n",
              "                    <div><strong>Score:</strong> 0.012</div>\n",
              "                    <div class=\"interpretation\"><strong>Analysis:</strong> The Saliency method&apos;s score of 0.012 indicates that it provides extremely concise explanations, focusing on a very limited number of key features. This suggests good performance, as lower values are preferred, indicating that the method is able to identify the most important features without distributing importance across many features. Compared to the ideal value of 0, the Saliency method&apos;s score is very close, suggesting that it is performing nearly optimally in terms of explanation conciseness.</div>\n",
              "                </div>\n",
              "                </div>\n",
              "            <div class=\"metric\">\n",
              "                <div class=\"metric-name\">FAD ‚Üì</div>\n",
              "                <div class=\"metric-meta\">\n",
              "                    <strong>Definition:</strong> Measures performance impact when dropping top-salient tokens, with curve steepness indicating faithfulness.<br>\n",
              "                    <strong>Direction:</strong> lower is better | \n",
              "                    <strong>Range:</strong> 0-1 (0 = perfect alignment with model&apos;s true feature importance)\n",
              "                </div>\n",
              "            \n",
              "                <div class=\"explainer\">\n",
              "                    <div class=\"explainer-name\">Saliency</div>\n",
              "                    <div><strong>Score:</strong> 1.000</div>\n",
              "                    <div class=\"interpretation\"><strong>Analysis:</strong> A Saliency method score of 1.000 on the FAD ‚Üì metric indicates that the method&apos;s explanations are completely unfaithful to the model&apos;s true feature importance, as the performance impact when dropping top-salient tokens is maximum. This is not a good performance, as a lower score is desirable, indicating that the method is not effective in identifying the most important features. In comparison to the ideal value of 0, which represents perfect alignment with the model&apos;s true feature importance, a score of 1.000 is the worst possible outcome, suggesting significant room for improvement in the explanation method.</div>\n",
              "                </div>\n",
              "                </div>\n",
              "            <div class=\"metric\">\n",
              "                <div class=\"metric-name\">Soft Comprehensiveness ‚Üë</div>\n",
              "                <div class=\"metric-meta\">\n",
              "                    <strong>Definition:</strong> Measures how much the model&apos;s prediction relies on important tokens by probabilistically perturbing them based on importance scores.<br>\n",
              "                    <strong>Direction:</strong> higher is better | \n",
              "                    <strong>Range:</strong> 0-1 (1 = model completely relies on the important tokens)\n",
              "                </div>\n",
              "            \n",
              "                <div class=\"explainer\">\n",
              "                    <div class=\"explainer-name\">Saliency</div>\n",
              "                    <div><strong>Score:</strong> 0.256</div>\n",
              "                    <div class=\"interpretation\"><strong>Analysis:</strong> The Saliency method&apos;s score of 0.256 on the Soft Comprehensiveness ‚Üë metric indicates that the model&apos;s predictions moderately rely on the important tokens, but there is still a significant amount of reliance on less important tokens. This score suggests mediocre performance, as it falls far from the ideal value of 1, indicating that the model does not completely rely on the important tokens. In comparison to the ideal value of 1, where the model would entirely rely on important tokens, a score of 0.256 is relatively low, suggesting room for improvement in the explanation method&apos;s ability to identify crucial tokens.</div>\n",
              "                </div>\n",
              "                </div>\n",
              "            <div class=\"metric\">\n",
              "                <div class=\"metric-name\">Soft Sufficiency ‚Üì</div>\n",
              "                <div class=\"metric-meta\">\n",
              "                    <strong>Definition:</strong> Measures how well important tokens preserve the model&apos;s prediction when other tokens are perturbed.<br>\n",
              "                    <strong>Direction:</strong> lower is better | \n",
              "                    <strong>Range:</strong> 0-1 (1 = important tokens perfectly maintain the prediction)\n",
              "                </div>\n",
              "            \n",
              "                <div class=\"explainer\">\n",
              "                    <div class=\"explainer-name\">Saliency</div>\n",
              "                    <div><strong>Score:</strong> 0.002</div>\n",
              "                    <div class=\"interpretation\"><strong>Analysis:</strong> A score of 0.002 on the Soft Sufficiency ‚Üì metric means that when important tokens identified by the Saliency method are preserved and other tokens are perturbed, the model&apos;s prediction is almost perfectly maintained, with only a negligible drop in performance. This indicates exceptionally good performance, as a lower score is better and a score close to 0 suggests that the important tokens are highly effective in preserving the model&apos;s prediction. Compared to the ideal value of 0, the Saliency method&apos;s score is extremely close, suggesting that it is nearly optimal in identifying important tokens that maintain the model&apos;s prediction when other tokens are perturbed.</div>\n",
              "                </div>\n",
              "                </div>\n",
              "            <div class=\"metric\">\n",
              "                <div class=\"metric-name\">Sparseness ‚Üë</div>\n",
              "                <div class=\"metric-meta\">\n",
              "                    <strong>Definition:</strong> Quantifies explanation concentration using the Gini index. Measures whether only highly-attributed features are truly predictive.<br>\n",
              "                    <strong>Direction:</strong> higher is better | \n",
              "                    <strong>Range:</strong> 0-1 (1 = perfectly sparse, 0 = uniform importance across all features)\n",
              "                </div>\n",
              "            \n",
              "                <div class=\"explainer\">\n",
              "                    <div class=\"explainer-name\">Saliency</div>\n",
              "                    <div><strong>Score:</strong> 0.290</div>\n",
              "                    <div class=\"interpretation\"><strong>Analysis:</strong> The Saliency method&apos;s score of 0.290 on the Sparseness ‚Üë metric indicates that its explanations are somewhat concentrated, but not extremely so, as it falls closer to the uniform importance end of the spectrum. This score suggests mediocre performance, as it is far from the ideal value of 1, which represents perfectly sparse explanations where only highly-attributed features are truly predictive. In comparison to the ideal value of 1, a score of 0.290 is relatively low, indicating that the Saliency method&apos;s explanations could be improved in terms of concentration and feature importance.</div>\n",
              "                </div>\n",
              "                </div></body></html>'\n",
              "                style=\"width:100%; height:600px; border:none;\"\n",
              "                sandbox=\"allow-scripts\">\n",
              "        </iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from EvaluationMetricsExplainer import EvaluationMetricsExplainer\n",
        "\n",
        "api_key = \"9b305c0291728a7f4ca861254b9bedd3e7785d5360100d93603dd1758931be6d\"\n",
        "explainer = EvaluationMetricsExplainer(api_key=api_key)\n",
        "\n",
        "results = explainer.explain_results(metrics)\n",
        "json_path, html_path = explainer.save_results(results)\n",
        "\n",
        "explainer.display_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also do `Step 3` and `Step 4` together within a single function call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing instance 0\n",
            "Found cached explanation for Saliency\n",
            "Processing instance 1\n",
            "Found cached explanation for Saliency\n",
            "\n",
            "Explainer: Saliency\n",
            "Computing value for Complexity ‚Üì\n",
            "Computing value for Soft Comprehensiveness ‚Üë\n",
            "Computing value for Soft Sufficiency ‚Üì\n",
            "Computing value for FAD ‚Üì\n",
            "Computing value for Sparseness ‚Üë\n",
            "Computing value for AUTPC ‚Üì\n",
            "<pandas.io.formats.style.Styler object at 0x0000014A346152B0>\n"
          ]
        }
      ],
      "source": [
        "eval_metrics= xai_framework.benchmark_dataset(input_texts_sample, labels_sample,rationales=rationale_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_575f8_row0_col0, #T_575f8_row0_col1, #T_575f8_row0_col2, #T_575f8_row0_col3 {\n",
              "  background-color: rgb(247, 100, 100);\n",
              "  color: black;\n",
              "  font-weight: bold;\n",
              "}\n",
              "#T_575f8_row0_col4, #T_575f8_row0_col5 {\n",
              "  background-color: rgb(197, 100, 197);\n",
              "  color: black;\n",
              "  font-weight: bold;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_575f8\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Evaluator</th>\n",
              "      <th id=\"T_575f8_level0_col0\" class=\"col_heading level0 col0\" >Soft Comprehensiveness ‚Üë</th>\n",
              "      <th id=\"T_575f8_level0_col1\" class=\"col_heading level0 col1\" >Soft Sufficiency ‚Üì</th>\n",
              "      <th id=\"T_575f8_level0_col2\" class=\"col_heading level0 col2\" >AUTPC ‚Üì</th>\n",
              "      <th id=\"T_575f8_level0_col3\" class=\"col_heading level0 col3\" >FAD ‚Üì</th>\n",
              "      <th id=\"T_575f8_level0_col4\" class=\"col_heading level0 col4\" >Complexity ‚Üì</th>\n",
              "      <th id=\"T_575f8_level0_col5\" class=\"col_heading level0 col5\" >Sparseness ‚Üë</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Explainer</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_575f8_level0_row0\" class=\"row_heading level0 row0\" >Saliency</th>\n",
              "      <td id=\"T_575f8_row0_col0\" class=\"data row0 col0\" >0.286620</td>\n",
              "      <td id=\"T_575f8_row0_col1\" class=\"data row0 col1\" >0.001537</td>\n",
              "      <td id=\"T_575f8_row0_col2\" class=\"data row0 col2\" >0.250000</td>\n",
              "      <td id=\"T_575f8_row0_col3\" class=\"data row0 col3\" >1.000000</td>\n",
              "      <td id=\"T_575f8_row0_col4\" class=\"data row0 col4\" >0.011881</td>\n",
              "      <td id=\"T_575f8_row0_col5\" class=\"data row0 col5\" >0.289792</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x14a0a0186b0>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xai_framework.create_pivot_table(eval_metrics)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
