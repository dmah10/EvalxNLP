{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to EvalxNLP documentation!","text":"<p>EvalxNLP is a Python framework for benchmarking state-of-the-art feature attribution methods for transformer-based NLP models</p> <p>The framework allows users to:</p> <ul> <li> <p>Visualize and compare Transformers-based models output explanations using various   Ph-FA methods.</p> </li> <li> <p>Use natural language text explanations from LLMs to interpret importance scores and   evaluation metrics for different explainers.</p> </li> <li> <p>Evaluate effectiveness of explainers using a variety of evaluation metrics for faithfulness,   plausibility, and complexity.</p> </li> </ul>"},{"location":"#contents","title":"Contents","text":""},{"location":"#usage","title":"Usage","text":"<ul> <li>Installation</li> <li>Explaining</li> <li>Benchmarking</li> <li>Generating LLM Explanations</li> <li>Advanced Features</li> </ul>"},{"location":"#tutorials","title":"Tutorials","text":"<ul> <li>Sentiment Analysis</li> <li>Hate Speech Detection</li> </ul>"},{"location":"#technical-reference","title":"Technical Reference","text":"<ul> <li>Explainers</li> <li>Metrics</li> <li>Tasks and Datasets</li> <li>LLM Explanations</li> </ul>"},{"location":"reference/explainers/","title":"\ud83e\udde0 Explainers","text":"<p>One goal of EvalxNLP is to enable users to generate diverse explanations using multiple explainability methods. It integrates eight widely recognized post-hoc feature attribution (Ph-FA) explainability methods from the XAI literature.</p> <p>These methods assign importance scores to individual features based on their contribution to the model's prediction. We focus on Ph-FA methods for their interpretability, relevance to end users, and generally efficient computational cost.</p>"},{"location":"reference/explainers/#categories-of-explainers","title":"\ud83e\udde9 Categories of Explainers","text":"<p>Feature attribution methods in EvalxNLP are categorized into:</p> <ul> <li>Perturbation-based methods</li> <li>Gradient-based methods</li> </ul> <p>Below, we provide an overview of the methods included in each category.</p>"},{"location":"reference/explainers/#perturbation-based-methods","title":"\ud83d\udd00 Perturbation-Based Methods","text":"<p>Perturbation-based methods explain model predictions by altering or masking input features and observing the resulting changes in the output.</p>"},{"location":"reference/explainers/#lime","title":"LIME","text":"<p>A model-agnostic method that explains individual predictions by training a local surrogate model.</p> <p>LIME generates explanations by perturbing the input data and measuring how the model's predictions change. Perturbed samples are weighted by their similarity to the original input. A simple, interpretable model\u2014typically linear\u2014is then trained on this weighted data to approximate the local behavior of the original model. LIME aims to balance fidelity (how closely the surrogate approximates the model) and interpretability (how understandable the explanation is to humans).</p>"},{"location":"reference/explainers/#shap-partition-shap","title":"SHAP (Partition SHAP)","text":"<p>A game-theoretic approach to explain feature contributions using Shapley values.</p> <p>SHAP computes the contribution of each input feature by evaluating all possible subsets of features, as proposed in cooperative game theory. Partition SHAP, a variant, improves efficiency by leveraging feature independence, making it scalable for high-dimensional data while preserving theoretical guarantees.</p>"},{"location":"reference/explainers/#shap-i","title":"SHAP-I","text":"<p>An advanced extension of SHAP that captures higher-order feature interactions.</p> <p>SHAP-I extends the original SHAP method to support any-order interactions between features. It incorporates efficient algorithms to compute interaction-aware Shapley values, enabling explanation of complex dependencies in high-dimensional input data.</p>"},{"location":"reference/explainers/#gradient-based-methods","title":"\ud83d\udd01 Gradient-Based Methods","text":"<p>Gradient-based methods derive feature attributions by analyzing the gradients of the model's output with respect to its inputs.</p>"},{"location":"reference/explainers/#saliency","title":"Saliency","text":"<p>A baseline technique that visualizes input feature importance using raw gradients.</p> <p>Saliency methods compute the gradient of the output with respect to each input feature. The absolute values of these gradients highlight which parts of the input most influence the model's prediction.</p>"},{"location":"reference/explainers/#gxi-gradient-input","title":"GxI (Gradient \u00d7 Input)","text":"<p>Enhances gradient information by incorporating input values.</p> <p>GxI multiplies the gradient of the output with respect to each input feature by the corresponding input value. This combination provides a more intuitive measure of each feature's contribution, especially for models where feature magnitudes play a critical role.</p>"},{"location":"reference/explainers/#ig-integrated-gradients","title":"IG (Integrated Gradients)","text":"<p>A principled method that attributes importance by integrating gradients along a path from baseline to input.</p> <p>Integrated Gradients explain a model\u2019s prediction by accumulating gradients as the input transitions from a baseline (e.g., a zero vector) to the actual input. This method satisfies desirable properties such as completeness and provides a mathematically grounded attribution of feature importance.</p>"},{"location":"reference/explainers/#dl-deeplift","title":"DL (DeepLIFT)","text":"<p>Assigns attributions by comparing neuron activations to a reference.</p> <p>DeepLIFT computes the difference between the activations of neurons for an input and a reference baseline. It assigns credit to each input feature for the change in output, using the Rescale Rule. DeepLIFT is often more stable than raw gradients and is computationally more efficient than IG, especially for large datasets and deep architectures.</p>"},{"location":"reference/explainers/#gbp-guided-backpropagation","title":"GBP (Guided Backpropagation)","text":"<p>Enhances interpretability by filtering gradients.</p> <p>Guided Backpropagation modifies the standard backpropagation algorithm to ignore negative gradients during the backward pass. This approach emphasizes input features that positively influence the output, resulting in sharper and more focused explanations.</p> <p>---1~# \ud83e\udde0 Explainers</p> <p>One goal of EvalxNLP is to enable users to generate diverse explanations using multiple explainability methods. It integrates eight widely recognized post-hoc feature attribution (Ph-FA) explainability methods from the XAI literature.</p> <p>These methods assign importance scores to individual features based on their contribution to the model's prediction. We focus on Ph-FA methods for their interpretability, relevance to end users, and generally efficient computational cost.</p>"},{"location":"reference/explainers/#categories-of-explainers_1","title":"\ud83e\udde9 Categories of Explainers","text":"<p>Feature attribution methods in EvalxNLP are categorized into:</p> <ul> <li>Perturbation-based methods</li> <li>Gradient-based methods</li> </ul> <p>Below, we provide an overview of the methods included in each category.</p>"},{"location":"reference/explainers/#perturbation-based-methods_1","title":"\ud83d\udd00 Perturbation-Based Methods","text":"<p>Perturbation-based methods explain model predictions by altering or masking input features and observing the resulting changes in the output.</p>"},{"location":"reference/explainers/#lime_1","title":"LIME","text":"<p>A model-agnostic method that explains individual predictions by training a local surrogate model.</p> <p>LIME generates explanations by perturbing the input data and measuring how the model's predictions change. Perturbed samples are weighted by their similarity to the original input. A simple, interpretable model\u2014typically linear\u2014is then trained on this weighted data to approximate the local behavior of the original model. LIME aims to balance fidelity (how closely the surrogate approximates the model) and interpretability (how understandable the explanation is to humans).</p>"},{"location":"reference/explainers/#shap-partition-shap_1","title":"SHAP (Partition SHAP)","text":"<p>A game-theoretic approach to explain feature contributions using Shapley values.</p> <p>SHAP computes the contribution of each input feature by evaluating all possible subsets of features, as proposed in cooperative game theory. Partition SHAP, a variant, improves efficiency by leveraging feature independence, making it scalable for high-dimensional data while preserving theoretical guarantees.</p>"},{"location":"reference/explainers/#shap-i_1","title":"SHAP-I","text":"<p>An advanced extension of SHAP that captures higher-order feature interactions.</p> <p>SHAP-I extends the original SHAP method to support any-order interactions between features. It incorporates efficient algorithms to compute interaction-aware Shapley values, enabling explanation of complex dependencies in high-dimensional input data.</p>"},{"location":"reference/explainers/#gradient-based-methods_1","title":"\ud83d\udd01 Gradient-Based Methods","text":"<p>Gradient-based methods derive feature attributions by analyzing the gradients of the model's output with respect to its inputs.</p>"},{"location":"reference/explainers/#saliency_1","title":"Saliency","text":"<p>A baseline technique that visualizes input feature importance using raw gradients.</p> <p>Saliency methods compute the gradient of the output with respect to each input feature. The absolute values of these gradients highlight which parts of the input most influence the model's prediction.</p>"},{"location":"reference/explainers/#gxi-gradient-input_1","title":"GxI (Gradient \u00d7 Input)","text":"<p>Enhances gradient information by incorporating input values.</p> <p>GxI multiplies the gradient of the output with respect to each input feature by the corresponding input value. This combination provides a more intuitive measure of each feature's contribution, especially for models where feature magnitudes play a critical role.</p>"},{"location":"reference/explainers/#ig-integrated-gradients_1","title":"IG (Integrated Gradients)","text":"<p>A principled method that attributes importance by integrating gradients along a path from baseline to input.</p> <p>Integrated Gradients explain a model\u2019s prediction by accumulating gradients as the input transitions from a baseline (e.g., a zero vector) to the actual input. This method satisfies desirable properties such as completeness and provides a mathematically grounded attribution of feature importance.</p>"},{"location":"reference/explainers/#dl-deeplift_1","title":"DL (DeepLIFT)","text":"<p>Assigns attributions by comparing neuron activations to a reference.</p> <p>DeepLIFT computes the difference between the activations of neurons for an input and a reference baseline. It assigns credit to each input feature for the change in output, using the Rescale Rule. DeepLIFT is often more stable than raw gradients and is computationally more efficient than IG, especially for large datasets and deep architectures.</p>"},{"location":"reference/explainers/#gbp-guided-backpropagation_1","title":"GBP (Guided Backpropagation)","text":"<p>Enhances interpretability by filtering gradients.</p> <p>Guided Backpropagation modifies the standard backpropagation algorithm to ignore negative gradients during the backward pass. This approach emphasizes input features that positively influence the output, resulting in sharper and more focused explanations.</p>"},{"location":"reference/llm_explanations/","title":"LLM Explanations","text":"<p>Feature attribution explanations, particularly those involving high-dimensional features, can be difficult for lay users to interpret. To address this, our tool integrates a module that leverages Large Language Models (LLMs) to generate natural language descriptions to help users interpret both:</p> <ol> <li>The importance scores from various explanation methods.</li> <li>The evaluation metric scores.</li> </ol>"},{"location":"reference/llm_explanations/#key-benefits","title":"Key Benefits","text":"<ul> <li>Improved Interpretability: Textual descriptions bridge the gap between numerical outputs and actionable insights, helping users better understand model behavior and evaluation outcomes.</li> <li>User-Friendly Accessibility: Natural language explanations lower the barrier for non-technical users, making the framework more approachable without requiring deep expertise in explainability methods or an understanding of the metrics.</li> <li>Enhanced Usability: The combination of visual heatmaps with textual explanations offers a more accessible and holistic view of the model\u2019s decision-making process.</li> </ul> <p>The LLM is integrated into the framework via an API, allowing for seamless generation of textual descriptions whenever needed. This ensures the feature is both flexible and scalable, adapting to various tasks and datasets.</p>"},{"location":"reference/metrics/","title":"\ud83d\udcca Evaluation Metrics","text":"<p>EvalxNLP incorporates a diverse and well-recognized set of properties and metrics from prior research to evaluate post-hoc explanation methods. These metrics are designed to assess explanation quality across three major properties:</p> <ul> <li>Faithfulness</li> <li>Plausibility</li> <li>Complexity</li> </ul> <p>Each metric is custom-implemented, following its original research paper to ensure correctness and theoretical fidelity.</p> <p>(\u2193)/(\u2191) indicates whether lower or higher values are better for that metric.</p>"},{"location":"reference/metrics/#faithfulness","title":"\u2705 Faithfulness","text":"<p>Measures how well the explanations align with the model's actual reasoning.</p>"},{"location":"reference/metrics/#soft-sufficiency","title":"Soft Sufficiency \u2193","text":"<p>Measures how well the most important tokens (based on their importance scores) can retain the model's prediction when other tokens are softly perturbed. It assumes that retaining more elements of important tokens should preserve the model's output, while dropping less important tokens should have minimal impact.</p> <p>A Bernoulli mask is generated, where each token is dropped with a probability proportional to its normalized importance score:</p> \\[ \\mathrm{mask} \\sim \\mathrm{Bernoulli}\\left(\\mathrm{normalized\\_importance\\_scores}\\right) \\] <p>The Suff score is calculated as:</p> \\[ \\mathrm{Suff} = 1 - \\max\\left(0, P_{\\mathrm{full}}(y) - P_{\\mathrm{reduced}}(y)\\right) \\] <p>Where:</p> <ul> <li>\\(P_{\\mathrm{full}}(y)\\) is the model's predicted probability for the original input</li> <li>\\(P_{\\mathrm{reduced}}(y)\\) is the model's predicted probability for the perturbed input</li> </ul> <p>The Suff score is normalized to the range \\([0, 1]\\) using a baseline Suff score (computed by masking all tokens):</p> \\[ \\mathrm{normalized\\_Suff} = \\frac{\\max\\left(0, \\mathrm{Suff} - \\mathrm{baseline\\_Suff}\\right)}{1 - \\mathrm{baseline\\_Suff}} \\] <p>The final score is the average across all instances, with higher values indicating that the model's predictions are less affected by the perturbation of important tokens.</p>"},{"location":"reference/metrics/#soft-comprehensiveness","title":"Soft Comprehensiveness \u2191","text":"<p>It evaluates how much the model's prediction changes when important tokens are softly perturbed using Bernoulli mask. It assumes that heavily perturbing important tokens should significantly affect the model's output, indicating their importance to the prediction.</p> <p>For each instance, the importance scores of tokens are normalized to the range \\([0, 1]\\):</p> \\[ \\mathrm{normalized\\_importance\\_scores} = \\frac{\\mathrm{importance\\_scores} - \\min(\\mathrm{importance\\_scores})}{\\max(\\mathrm{importance\\_scores}) - \\min(\\mathrm{importance\\_scores})} \\] <p>A Bernoulli mask is then generated, where each token is dropped with a probability proportional to \\(1 - \\mathrm{normalized\\_importance\\_scores}\\):</p> \\[ \\mathrm{mask} \\sim \\mathrm{Bernoulli}\\left(1 - \\mathrm{normalized\\_importance\\_scores}\\right) \\] <p>This mask is applied to the token embeddings, creating a perturbed input. The \\(\\mathrm{Comp}\\) score is calculated as the difference between the model's confidence in the original prediction and its confidence after perturbation:</p> \\[ \\mathrm{Comp} = \\max\\left(0, P_{\\mathrm{original}}(y) - P_{\\mathrm{perturbed}}(y)\\right) \\] <p>where:</p> <ul> <li>\\(P_{\\mathrm{original}}(y)\\) is the model's predicted probability for the original input</li> <li>\\(P_{\\mathrm{perturbed}}(y)\\) is the model's predicted probability for the perturbed input</li> </ul> <p>The final \\(\\mathrm{Comp}\\) score is the average across all instances, with higher values indicating that the model relies more heavily on the important tokens.</p>"},{"location":"reference/metrics/#fad-curve-n-auc","title":"FAD curve N-AUC \u2193","text":"<p>Measures the impact of dropping the most salient tokens on model performance, with the steepness of the curve indicating the method's faithfulness. The N-AUC quantifies this steepness, where a higher score reflects better alignment of the attribution method with the model's true feature importance.</p> <p>For each instance, tokens are progressively dropped based on their saliency scores and the model's accuracy is evaluated at different drop percentages (e.g., 0%, 10%, ..., 40%). The saliency scores determine which tokens to replace with a baseline token (<code>[MASK]</code>). The N-AUC is computed over a specified percentage range (0% to 20%) using the trapezoidal rule:</p> \\[ \\mathrm{N\\text{-}AUC} = \\frac{\\mathrm{AUC}}{\\mathrm{max\\_AUC}} \\] <p>where:</p> <ul> <li>\\(\\mathrm{AUC}\\) is the area under the accuracy curve, computed as:</li> </ul> <p>$$   \\mathrm{AUC} = \\int_{x_{\\min}}^{x_{\\max}} y(x) \\, dx   $$</p> <p>where \\(x\\) represents the percentage of tokens dropped, and \\(y(x)\\) represents the model's accuracy at that percentage.</p> <ul> <li>\\(\\mathrm{max\\_AUC}\\) is the maximum possible area, calculated as:</li> </ul> <p>$$   \\mathrm{max_AUC} = (x_{\\max} - x_{\\min}) \\times \\max(y)   $$</p> <p>where \\(x_{\\min}\\) and \\(x_{\\max}\\) are the lower and upper bounds of the percentage range, and \\(\\max(y)\\) is the highest accuracy value in the range.</p> <p>This metric quantifies how much model performance degrades when salient tokens are removed, with lower N-AUC indicating greater reliance on the dropped tokens.</p>"},{"location":"reference/metrics/#autpc","title":"AUTPC \u2193","text":"<p>Area Under the Token Perturbation Curve evaluates the faithfulness of saliency explanations by progressively masking the most important tokens (based on their saliency scores) and measuring the drop in the model's performance. Masking involves replacing tokens with a baseline value (typically <code>[MASK]</code> or a zero vector).</p> <p>Starting with 0% masking and incrementally increasing to 100%, the model\u2019s accuracy is computed at each threshold. The resulting performance curve, plotting accuracy against the percentage of tokens masked, is used to calculate the area under the curve (AUC).</p> <p>This AUTPC value, normalized to the range [0, 1], provides a single metric summarizing how significantly the model relies on the highlighted tokens, with higher values indicating more faithful explanations. A smaller AUC indicates that removing critical features degrades performance faster, demonstrating more faithful explanations. Results are standardized by the number of features for comparability.</p>"},{"location":"reference/metrics/#plausibility","title":"\ud83d\udca1 Plausibility","text":"<p>Measures how well explanations align with human-annotated rationales.</p>"},{"location":"reference/metrics/#iou-f1-score","title":"IOU-F1 Score \u2191","text":"<p>It evaluates the alignment between predicted rationales and ground truth rationales at the span level using the Intersection over Union (IOU) metric. For the i-th instance with predicted rationale \\(S_i^p\\) and ground truth rationale \\(S_i^g\\), the IOU is calculated as:</p> \\[ S_i = \\frac{|S_i^p \\cap S_i^g|}{|S_i^p \\cup S_i^g|} \\] <p>A rationale is considered a match when \\(S_i \\geq 0.5\\). The IOU-F1 score aggregates these matches across all \\(N\\) instances:</p> \\[ \\mathrm{IOU\\text{-}F1} = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}(S_i \\geq 0.5) \\] <p>where \\(\\mathbb{I}\\) is the indicator function: $$ \\mathbb{I}(S_i \\geq 0.5) =  \\begin{cases}  1 &amp; \\text{if } S_i \\geq 0.5 \\ 0 &amp; \\text{otherwise} \\end{cases} $$</p> <p>This metric ranges from 0 to 1, with higher values indicating better alignment between predicted and ground truth rationales. The 0.5 threshold ensures that only substantially overlapping spans are counted as matches.</p>"},{"location":"reference/metrics/#token-level-f1-score","title":"Token-level F1 Score \u2191","text":"<p>The Token F1-score measures the alignment between predicted rationales and ground truth rationales by computing the F1-score, which balances precision and recall. For each instance, the predicted rationale \\(S_i^p\\) is compared to the ground truth rationale \\(S_i^g\\), and the F1-score is calculated based on the overlap of tokens.</p> <p>The F1-score for the i-th instance is defined as:</p> \\[ F1_i = \\frac{2 \\times P_i \\times R_i}{P_i + R_i} \\] <p>where: - Precision \\(P_i\\):   $$   P_i = \\frac{|S_i^p \\cap S_i^g|}{|S_i^p|}   $$ - Recall \\(R_i\\):   $$   R_i = \\frac{|S_i^p \\cap S_i^g|}{|S_i^g|}   $$</p> <p>The final Token F1-score is the average F1-score across all \\(N\\) instances:</p> \\[ \\mathrm{Token\\text{-}F1} = \\frac{1}{N} \\sum_{i=1}^N F1_i \\] <p>For both the Token-F1 score and IOU-F1 score, the top \\(K\\) tokens with positive influence are selected, where \\(K\\) is the average length of the human rationale for the dataset.</p>"},{"location":"reference/metrics/#auprc","title":"AUPRC \u2191","text":"<p>For each instance, the saliency scores are compared to the ground truth rationale mask. The precision-recall curve is computed, and then the area under this curve is calculated. The final AUPRC score is the average of these values across all instances, providing a single metric that quantifies the alignment between predicted saliency and human-annotated rationales, with higher scores indicating better performance.</p>"},{"location":"reference/metrics/#complexity","title":"\ud83d\udd00 Complexity","text":"<p>Measures how interpretable the explanation is by checking sparsity and token focus.</p>"},{"location":"reference/metrics/#complexity_1","title":"Complexity \u2193","text":"<p>It measures the complexity of token-level attributions using Shannon entropy, which quantifies how evenly the importance scores are distributed across tokens. For each instance, the fractional contribution of each token is computed as:</p> \\[ f_j = \\frac{|a_j|}{\\sum_{k=1}^n |a_k|} \\] <p>where: - \\(a_j\\) is the saliency score of the \\(j\\)-th token - \\(n\\) is the total number of tokens</p> <p>The complexity score for the instance is then calculated as the entropy of the fractional contributions:</p> \\[ \\mathrm{Complexity} = -\\frac{1}{n}\\sum_{j=1}^n f_j \\cdot \\log(f_j + \\epsilon) \\] <p>where \\(\\epsilon\\) is a small constant (e.g., \\(10^{-8}\\)) to avoid numerical instability. The final complexity score is the average across all instances, with higher values indicating more evenly distributed attributions (higher complexity) and lower values indicating more concentrated attributions (lower complexity).</p>"},{"location":"reference/metrics/#sparseness","title":"Sparseness \u2191","text":"<p>It measures the sparsity of model attributions using the Gini index, which quantifies how concentrated the importance scores are across features. For each instance, the absolute values of the attributions are sorted in ascending order, and the Gini index is computed as:</p> \\[ \\mathrm{Sparseness} = 1 - 2 \\cdot \\frac{\\sum_{j=1}^n (n - j + 0.5) \\cdot |a_j|}{\\left(\\sum_{j=1}^n |a_j|\\right) \\cdot n} \\] <p>where: - \\(a_j\\) is the \\(j\\)-th sorted attribution value - \\(n\\) is the total number of features (tokens) - \\(|a_j|\\) is the absolute value of the \\(j\\)-th attribution</p> <p>The sparseness score ranges from 0 to 1, where: - 0 indicates dense attributions (importance is evenly distributed across features) - 1 indicates sparse attributions (importance is concentrated on a few features)</p> <p>The final sparseness score is the average across all instances, providing a single metric to evaluate how focused the model is on specific features.</p>"},{"location":"reference/tasks_datasets/","title":"Tasks and Datasets","text":"<p>The framework currently supports the following text classification tasks:</p>"},{"location":"reference/tasks_datasets/#supported-tasks","title":"\ud83e\udde0 Supported Tasks","text":""},{"location":"reference/tasks_datasets/#sentiment-analysis","title":"\ud83d\udcac Sentiment Analysis","text":"<ul> <li>Determines the sentiment expressed in a text, such as positive, negative, or neutral.</li> <li>Commonly used in applications like product reviews, social media analysis, and customer feedback.</li> </ul>"},{"location":"reference/tasks_datasets/#hate-speech-detection","title":"\ud83d\udeab Hate Speech Detection","text":"<ul> <li>Identifies and classifies text containing hate speech, offensive language, or harmful content.</li> <li>Essential for moderating online platforms and ensuring safe digital environments.</li> </ul>"},{"location":"reference/tasks_datasets/#natural-language-inference-nli","title":"\ud83d\udd17 Natural Language Inference (NLI)","text":"<ul> <li>Determines the logical relationship between two sentences (e.g., premise and hypothesis).</li> <li>Tasks include identifying entailment, contradiction, or neutrality between sentences.</li> <li>Useful for applications like question answering, summarization, and reasoning tasks.</li> </ul>"},{"location":"reference/tasks_datasets/#dataset-support","title":"\ud83d\udcda Dataset Support","text":"<p>EvalxNLP includes a representative dataset for each of the supported tasks and allows users to extend the framework with additional classification datasets. All datasets are rationale-annotated, meaning they include human-annotated rationales that highlight the most important words or sentences for a given class label. These rationales enable the evaluation of alignment between model explanations and human understanding.</p>"},{"location":"reference/tasks_datasets/#moviereviews","title":"\ud83c\udfac MovieReviews","text":"<p>Task: Sentiment Analysis Description: Contains 1,000 positive and 1,000 negative movie reviews. Each review includes phrase-level human-annotated rationales that justify the sentiment label.</p>"},{"location":"reference/tasks_datasets/#hatexplain","title":"\ud83d\udce2 HateXplain","text":"<p>Task: Hate Speech Detection Description: Comprises 20,000 posts from Gab and Twitter, annotated with one of three labels: hate speech, offensive, or normal.</p>"},{"location":"reference/tasks_datasets/#e-snli","title":"\ud83d\udcc4 e-SNLI","text":"<p>Task: Natural Language Inference Description: Contains 549,367 examples split into training, validation, and test sets. Each example includes a premise and a hypothesis, annotated with one of three labels: entailment, contradiction, or neutral.</p>"},{"location":"tutorials/hate_speech/","title":"Hate Speech Detection","text":"<p>EvalxNLP supports Hate Speech Detection for both single sentence evaluation and dataset-level benchmarking. This section demonstrates how to use the framework for both cases.</p>"},{"location":"tutorials/hate_speech/#single-sentence","title":"\ud83e\uddea Single Sentence","text":"<p>The following example shows how to generate and evaluate model explanations for a single sentence classified as offensive:</p> <pre><code>from transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom XAIbenchmark import XAIFramework\n\n# Load a hate speech detection model\nmodel_name = \"Hate-speech-CNERG/bert-base-uncased-hatexplain\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n\n# Initialize the explanation framework\nxai_framework = XAIFramework(model, tokenizer)\n\n# Define input and target label\nsentence = \"I hate people from that community\"\nlabel = \"offensive\"\n\n# Generate explanation and visualize\nexps = xai_framework.explain(input_data=sentence, target_label=label)\nxai_framework.visualize(exps)\n\n# Evaluate explanation without a human rationale (optional)\nxai_framework.evaluate_single_sentence(sentence, target_label=\"offensive\")\n</code></pre>"},{"location":"tutorials/hate_speech/#dataset-hatexplain","title":"\ud83d\udcda Dataset (HateXplain)","text":"<p>EvalxNLP also supports dataset-level evaluation using the HateXplain dataset using the built-in class <code>HateSpeechProcessor</code>. The following example demonstrates the full pipeline: generating importance scores, visualizing them, interpreting them with LLM-generated textual explanations, computing evaluation metrics, visualizing these metrics, and finally interpreting them using LLM-based explanations.</p> <pre><code>import sys\nsys.path.append('..')\nfrom XAIbenchmark import XAIFramework\nimport warnings\nimport openpyxl\nimport torch\nwarnings.filterwarnings(\"ignore\")\nimport os\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom dataset_loaders.dataset_loader import LoadDatasetArgs,load_fields_from_dataset\nfrom dataset_loaders.movie_rationales import MovieRationalesProcessor\nfrom LLMExplanationGenerator import LLMExplanationGenerator\nfrom EvaluationMetricsExplainer import EvaluationMetricsExplainer\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\napi_key = \"YOUR_API_KEY\"\n\nmodel_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n\nxai_framework = XAIFramework(model, tokenizer, device=device)\n\ndataset_args_ = LoadDatasetArgs(\n    dataset_name=\"Hate-speech-CNERG/hatexplain\",\n    text_field=\"post_tokens\",\n    label_field=\"annotators\",\n    rationale_field=\"rationales\",\n    dataset_split=\"test\",\n)\n# Load the dataset fields\nresults_hate = load_fields_from_dataset(dataset_args_)\n\ntexts_hate= results_hate['text']\nlabels_hate= results_hate['labels']\nrationales_hate= results_hate['rationales']\n\nhs= HateSpeechProcessor(tokenizer)\nprocessed_input_texts_hate, processed_labels_hate, processed_rationales_hate= hs.process_dataset(input_texts=texts_hate, labels=labels_hate, rationales=rationales_hate)\n\n#Select a sub-sample if you want\ninput_texts_sample=processed_input_texts_hate[:20]\nlabels_sample=processed_labels_hate[:20]\nrationale_sample= processed_rationales_hate[:20]\n\nexp_scores= xai_framework.get_feature_importance_for_dataset(input_texts_sample,labels_sample,rationale_sample,output_file=\"../results/scores/hatespeech_scores.json\")\n\nscores_explainer = LLMExplanationGenerator(api_key=api_key)\n\n# Generate and save explanations (returns both explanations and file paths)\nexplanations, saved_files = scores_explainer.generate_and_save_explanations(\n    exps=exp_scores,\n    output_format=\"both\"  # or \"json\"/\"html\"\n)\n\nscores_explainer.display_explanations(explanations)\n\nmetrics= xai_framework.compute_evaluation_metrics(exp_scores)\nxai_framework.create_pivot_table(metrics,save_path=\"../results/metrics/hatespeech.xlsx\")\n\nmetrics_explainer = EvaluationMetricsExplainer(api_key=api_key)\n\nresults = metrics_explainer.explain_results(metrics)\njson_path, html_path = metrics_explainer.save_results(results)\n\nmetrics_explainer.display_results(results)\n</code></pre>"},{"location":"tutorials/sentiment_analysis/","title":"Sentiment Analysis","text":"<p>You can use EvalxNLP for Sentiment Analysis usecase, on a single sentence, or dataset.</p>"},{"location":"tutorials/sentiment_analysis/#single-sentence","title":"\ud83d\udcac Single Sentence","text":"<p>The following example demonstrates how to use EvalxNLP to explain and evaluate a model's prediction for a single input sentence:</p> <pre><code>from transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom XAIbenchmark import XAIFramework\n\n# Load a pre-trained sentiment classification model\nmodel_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n\n# Initialize EvalxNLP explanation framework\nxai_framework = XAIFramework(model, tokenizer)\n\n# Define input and label\nsentence = \"Worst experience I've ever had!\"\nlabel = \"negative\"\n\n# Generate explanation\nexps = xai_framework.explain(input_data=sentence, target_label=label)\n\n# Visualize the explanation\nxai_framework.visualize(exps)\n\n# Evaluate explanation against a human rationale\nxai_framework.evaluate_single_sentence(\n    sentence,\n    target_label=\"negative\",\n    human_rationale=[1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0]\n)\n</code></pre>"},{"location":"tutorials/sentiment_analysis/#dataset-movie-reviews","title":"\ud83c\udfac Dataset (Movie Reviews)","text":"<p>You can also apply EvalxNLP to a dataset such as Movie Reviews using the built-in class <code>MovieRationalesProcessor</code>. The following example demonstrates the full pipeline: generating importance scores, visualizing them, interpreting them with LLM-generated textual explanations, computing evaluation metrics, visualizing these metrics, and finally interpreting them using LLM-based explanations.</p> <pre><code>import sys\nsys.path.append('..')\nfrom XAIbenchmark import XAIFramework\nimport warnings\nimport openpyxl\nimport torch\nwarnings.filterwarnings(\"ignore\")\nimport os\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom explainers import IntegratedGradientsExplainer, GuidedBackpropExplainer\nfrom evaluators import AUCTPEvaluator, SoftComprehensivenessEvaluator,ComplexityEvaluator, IOUF1Evaluator\nfrom dataset_loaders.dataset_loader import LoadDatasetArgs,load_fields_from_dataset\nfrom dataset_loaders.movie_rationales import MovieRationalesProcessor\nfrom LLMExplanationGenerator import LLMExplanationGenerator\nfrom EvaluationMetricsExplainer import EvaluationMetricsExplainer\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\napi_key = \"YOUR_API_KEY\"\n\nmodel_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n\nig= IntegratedGradientsExplainer(model,tokenizer,device=device)\ngb= GuidedBackpropExplainer(model,tokenizer,device=device)\n\nsc= SoftComprehensivenessEvaluator(model,tokenizer,device=device)\ncx= ComplexityEvaluator(model,tokenizer,device=device)\niou_f1= IOUF1Evaluator(model,tokenizer,device=device)\n\nxai_framework = XAIFramework(model, tokenizer,explainers=[ig,gb], evaluators=[sc,cx,iou_f1], device=device)\n\ndataset_args_ = LoadDatasetArgs(\n    dataset_name=\"eraser-benchmark/movie_rationales\",\n    text_field=\"review\",\n    label_field=\"label\",\n    rationale_field=\"evidences\",\n    dataset_split=\"test\",\n)\n\n# Load the dataset fields\nresults = load_fields_from_dataset(dataset_args_)\ninput_texts= results['text']\nlabels= results['labels']\nrationales= results['rationales']\n\nmv= MovieRationalesProcessor(tokenizer)\nprocessed_rationales= mv.process_dataset(input_texts, labels, rationales)\n\n#Select a sub-sample if you want\ninput_texts_sample=input_texts[:2]\nlabels_sample=labels[:2]\nrationale_sample= processed_rationales[:2]\n\nexp_scores= xai_framework.get_feature_importance_for_dataset(input_texts_sample,labels_sample,rationale_sample,output_file=\"../results/scores/moviereviews_score.json\")\n\nscores_explainer = LLMExplanationGenerator(api_key=api_key)\n\n# Generate and save explanations (returns both explanations and file paths)\nexplanations, saved_files = scores_explainer.generate_and_save_explanations(\n    exps=exp_scores,\n    output_format=\"both\"  # or \"json\"/\"html\"\n)\n\nscores_explainer.display_explanations(explanations)\n\nmetrics= xai_framework.compute_evaluation_metrics(exp_scores)\nxai_framework.create_pivot_table(metrics,save_path=\"../results/metrics/rationales.xlsx\")\n\nmetrics_explainer = EvaluationMetricsExplainer(api_key=api_key)\n\nresults = metrics_explainer.explain_results(metrics)\njson_path, html_path = metrics_explainer.save_results(results)\n\nmetrics_explainer.display_results(results)\n</code></pre>"},{"location":"usage/advanced_usage/","title":"Advanced Features","text":""},{"location":"usage/advanced_usage/#gpu-usage","title":"\u26a1 GPU Usage","text":"<p>To enable GPU acceleration for explanation generation and metric computation, ensure that your model and all framework components are properly initialized on the target device (e.g., GPU or CPU). Below is an example demonstrating how to configure and initialize the framework for GPU usage:</p> <pre><code>from transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom explainers import IntegratedGradientsExplainer\nfrom evaluators import SoftComprehensivenessEvaluator\nimport torch\n\n# Set device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load model and tokenizer\nmodel_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n\n# Initialize explainer and evaluator with device\nig = IntegratedGradientsExplainer(model, tokenizer, device=device)\nsc = SoftComprehensivenessEvaluator(model, tokenizer, device=device)\n\n# Initialize framework\nxai_framework = XAIFramework(\n    model,\n    tokenizer,\n    explainers=[ig],\n    evaluators=[sc],\n    device=device\n)\n</code></pre> <p>This setup ensures that all components run efficiently on the GPU when available, significantly accelerating explanation generation and evaluation.</p>"},{"location":"usage/advanced_usage/#add-custom-objects-to-the-api","title":"\ud83e\udde9 Add Custom Objects to the API","text":"<p>The <code>EvalxNLP</code> framework is fully extensible. You can create and register your own explanation methods and evaluation metrics.</p>"},{"location":"usage/advanced_usage/#add-a-custom-explainer","title":"Add a Custom Explainer","text":"<p>To implement a new explanation method, extend the <code>BaseExplainer</code> abstract class. You must implement the following:</p> <ol> <li><code>NAME</code> (class property): A unique string identifier for your explainer.</li> <li><code>compute_feature_importance()</code>: The core method responsible for computing token-level importance scores.</li> </ol> <pre><code>class MyCustomExplainer(BaseExplainer):\n    NAME = \"my_explainer\"  # Unique identifier\n\n    def __init__(self, model, tokenizer, **kwargs):\n        super().__init__(model, tokenizer, **kwargs)\n        # Add custom initialization parameters if needed\n\n    def compute_feature_importance(\n        self,\n        text: str,\n        target: Union[int, str],\n        target_token: Optional[str] = None,\n        **kwargs\n    ) -&gt; Explanation:\n    \"\"\"Core method implementing the explanation logic\"\"\"\n        # [Implementation details here]\n        return Explanation(\n            text=text,\n            tokens=tokens,\n            scores=scores,\n            explainer=self.NAME,\n            # Additional metadata if required\n        )\n</code></pre>"},{"location":"usage/advanced_usage/#add-a-custom-evaluation-metric","title":"Add a Custom Evaluation Metric","text":"<p>To add a custom evaluation metric, extend the <code>BaseEvaluator</code> abstract class. You must implement:</p> <ol> <li><code>NAME</code> (class property): A unique string identifier for your metric.</li> <li><code>compute()</code>: A method that takes a list of Explanation objects and returns the computed score.</li> </ol> <pre><code>class MyCustomEvaluator(BaseEvaluator):\n    NAME = \"my_metric\"  # Unique identifier\n\n    def __init__(self, model, tokenizer, **kwargs):\n        super().__init__(model, tokenizer, **kwargs)\n        # Add custom initialization parameters if needed\n\n    def compute(\n        self,\n        explanations\n    ):\n        #Add custom implementation here\n        return value\n</code></pre>"},{"location":"usage/benchmarking/","title":"Benchmarking","text":"<p>EvalxNLP allows users to evaluate the quality and reliability of generated explanations and compare multiple explainers across plausibility and faithfulness metrics.</p>"},{"location":"usage/benchmarking/#evaluating-a-single-sentence","title":"\ud83e\uddea Evaluating a Single Sentence","text":""},{"location":"usage/benchmarking/#step-1-evaluate-explanations","title":"Step 1: Evaluate Explanations","text":"<p>Use the <code>evaluate_single_sentence()</code> method from the <code>XAIFramework</code> class to evaluate explanations for a single sentence. You may also provide a human rationale to enable plausibility metric evaluation.</p>"},{"location":"usage/benchmarking/#human-rationale","title":"Human Rationale","text":"<ul> <li>Human rationales are binary lists indicating which tokens are important for the target label.</li> <li>A value of <code>1</code> marks a token as important, and <code>0</code> as not important.</li> <li>If no rationale is provided, plausibility metrics will be skipped.</li> </ul> <pre><code>example = \"Worst experience I've ever had!!\"\nlabel = \"negative\"\nhuman_rationale = [0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0]  # Binary rationale\n\nmetrics = xai_framework.evaluate_single_sentence(\n    sentence=example,\n    target_label=label,\n    human_rationale=human_rationale\n)\n</code></pre>"},{"location":"usage/benchmarking/#step-2-view-results","title":"Step 2: View Results","text":"<p>Visualize the evaluation metrics in a tabular format using the <code>create_pivot_table()</code> function:</p> <pre><code>xai_framework.create_pivot_table(metrics)\n</code></pre> <ul> <li>Metrics are color-coded by property.</li> <li>Darker hues indicate stronger performance.</li> <li>The best-performing score for each metric is bolded for easier comparison.</li> </ul>"},{"location":"usage/benchmarking/#evaluating-a-dataset","title":"\ud83e\uddea Evaluating a Dataset","text":""},{"location":"usage/benchmarking/#step-1-compute-evaluation-metrics","title":"Step 1: Compute Evaluation Metrics","text":"<p>Use the <code>compute_evaluation_metrics()</code> method to calculate explanation quality across a dataset. It takes a list of explanation objects as input (e.g., from <code>get_feature_importance_for_dataset()</code>).</p> <pre><code>metrics = xai_framework.compute_evaluation_metrics(exp_scores)\n</code></pre>"},{"location":"usage/benchmarking/#step-2-visualize-results","title":"Step 2: Visualize Results","text":"<p>Visualize the dataset-level evaluation using the same pivot table function:</p> <pre><code>xai_framework.create_pivot_table(metrics)\n</code></pre>"},{"location":"usage/explaining/","title":"Explaining","text":""},{"location":"usage/explaining/#explaining-single-sentence","title":"\ud83e\udde0 Explaining Single Sentence","text":"<p>EvalxNLP integrates seamlessly with Hugging Face Transformer-based models. You can use any publicly available text classification model or a locally fine-tuned one. This guide walks you through the process of generating explanations for a single sentence.</p>"},{"location":"usage/explaining/#step-1-initialize-the-model-and-tokenizer","title":"Step 1: Initialize the Model and Tokenizer","text":"<p>Begin by loading your pre-trained model and tokenizer. You can choose any model from the Hugging Face Model Hub.</p> <pre><code>from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n</code></pre>"},{"location":"usage/explaining/#step-2-initialize-the-xaiframework-class","title":"Step 2: Initialize the XAIFramework Class","text":"<p>Instantiate the framework with the desired explainers and evaluators. EvalxNLP supports multiple plug-and-play XAI modules.</p> <pre><code>from XAIbenchmark import XAIFramework\nfrom explainers import InputXGradientExplainer, IntegratedGradientsExplainer\nfrom evaluators import AUCTPEvaluator, SoftComprehensivenessEvaluator\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Initialize explainers\nig = IntegratedGradientsExplainer(model, tokenizer)\nixg = InputXGradientExplainer(model, tokenizer, multiply_by_inputs=True)\n\n# Initialize evaluators\nsc = SoftComprehensivenessEvaluator(model, tokenizer)\nauctp = AUCTPEvaluator(model, tokenizer)\n\n# Instantiate the framework\nxai_framework = XAIFramework(\n    model,\n    tokenizer,\n    explainers=[ig, ixg],\n    evaluators=[sc, auctp],\n    device=device\n)\n</code></pre>"},{"location":"usage/explaining/#step-3-generate-explanations","title":"Step 3: Generate Explanations","text":"<p>Provide an input sentence and the corresponding target label to generate post-hoc explanations.</p> <pre><code>example = \"Worst experience I've ever had!\"\nlabel = \"negative\"\n\nexps = xai_framework.explain(input_data=example, target_label=label)\n</code></pre>"},{"location":"usage/explaining/#step-4-visualize-explanations","title":"Step 4: Visualize Explanations","text":"<p>You can visualize the output explanations directly using the built-in visualization tool.</p> <pre><code>xai_framework.visualize(exps)\n</code></pre>"},{"location":"usage/explaining/#explaining-datasets","title":"\ud83d\udcc2 Explaining Datasets","text":"<p>EvalxNLP allows users to explain and benchmark datasets from both the Hugging Face Hub and local files (e.g., CSV or Excel) using the <code>DatasetLoader</code> class. It supports text classification tasks such as Sentiment Analysis, Hate Speech Detection, and Natural Language Inference (NLI).</p>"},{"location":"usage/explaining/#step-1-define-dataset-loading-arguments","title":"Step 1: Define Dataset Loading Arguments","text":"<p>To begin, define the dataset loading configuration using the <code>LoadDatasetArgs</code> data class. The following fields are required:</p> <ul> <li><code>dataset_name</code>: Name of the dataset (e.g., Hugging Face dataset ID or <code>\"csv\"</code>/<code>\"excel\"</code> for local files).</li> <li><code>text_field</code>: Column name containing the input text.</li> <li><code>label_field</code>: Column name containing target labels.</li> </ul> <p>Optional fields: - <code>rationale_field</code>: For plausibility-based evaluation using human rationales. - <code>text_field_2</code>: For NLI tasks, used to distinguish premise and hypothesis.</p>"},{"location":"usage/explaining/#step-2-load-the-dataset","title":"Step 2: Load the Dataset","text":"<p>Once configured, pass the arguments to <code>load_fields_from_dataset()</code> to retrieve the content:</p> <pre><code>from dataset_loaders.dataset_loader import LoadDatasetArgs, load_fields_from_dataset\n\ndataset_args = LoadDatasetArgs(\n    dataset_name=\"Hate-speech-CNERG/hatexplain\",\n    text_field=\"post_tokens\",\n    label_field=\"annotators\",\n    rationale_field=\"rationales\",\n    dataset_split=\"test\"\n)\n\ninput_texts, labels, rationales = load_fields_from_dataset(dataset_args)\n</code></pre> <p>This returns:</p> <ul> <li> <p>input_texts: List of input sentences.</p> </li> <li> <p>labels: List of class labels.</p> </li> <li> <p>rationales: (Optional) Token-level binary importance annotations.</p> </li> </ul>"},{"location":"usage/explaining/#step-3-ensure-correct-format","title":"Step 3: Ensure Correct Format","text":"<p>To use the data with EvalxNLP, ensure each field is in the expected format:</p> <p>Input Texts: List of strings.</p> <p>Labels: List of integers or strings.</p> <p>Rationales: List of binary lists (e.g., [0, 1, 0, 1, 0]).</p>"},{"location":"usage/explaining/#data-post-processing-example","title":"Data Post-processing Example","text":"<p>EvalxNLP provides helper classes to process specific datasets. Here's how to prepare data from the HateXplain dataset:</p> <pre><code>import numpy as np\nfrom dataset_loaders.hatexplain import HateSpeechProcessor\n\nclass HateSpeechProcessor:\n    def __init__(self):\n        pass\n\n    def process_input_texts(self, input_texts):\n        return [\" \".join(text_array) for text_array in input_texts]\n\n    def process_labels(self, labels):\n        return [int(np.bincount(label[\"label\"]).argmax()) for label in labels]\n\n    def process_rationales(self, rationales):\n        return rationales\n\n    def process_dataset(self, input_texts, labels, rationales):\n        processed_input_texts = self.process_input_texts(input_texts)\n        processed_labels = self.process_labels(labels)\n        processed_rationales = self.process_rationales(rationales)\n        return processed_input_texts, processed_labels, processed_rationales\n\n# Process the dataset\nhs = HateSpeechProcessor()\nprocessed_input_texts, processed_labels, processed_rationales = hs.process_dataset(input_texts, labels, rationales)\n</code></pre>"},{"location":"usage/explaining/#optional-subset-selection","title":"(Optional) Subset Selection","text":"<p>You can easily select a subset of samples to speed up evaluation:</p> <pre><code>input_texts_sample = processed_input_texts[:10]\nlabels_sample = processed_labels[:10]\nrationale_sample = processed_rationales[:10]\n</code></pre>"},{"location":"usage/explaining/#step-4-generate-explanations","title":"Step 4: Generate Explanations","text":"<p>Finally, pass the data into the get_feature_importance_for_dataset() function to generate and save explanations:</p> <p><pre><code>exp_scores = xai_framework.get_feature_importance_for_dataset(\n    input_texts_sample,\n    labels_sample,\n    rationale_sample,\n    output_file=\"../results/scores/movie_xx.json\"\n)\n</code></pre> The results include explanation scores per token and can be stored for further analysis or visualization.</p>"},{"location":"usage/installation/","title":"\ud83d\ude80 Installation","text":"<p>This guide walks you through how to install the framework and its dependencies.</p>"},{"location":"usage/installation/#pre-requisites","title":"\u2705 Pre-requisites","text":"<ul> <li>Python 3.10+</li> <li>Tested on Windows 11 64-bit</li> <li>Supports both CPU and GPU</li> </ul>"},{"location":"usage/installation/#install-from-source","title":"\ud83d\udce6 Install from Source","text":"<p>The framework can be installed as follows::</p> <pre><code>git clone https://github.com/kafaite24/EvalxNLP.git\ncd EvalxNLP\npip install -r requirements.txt\n</code></pre> <p>If you don't have pip installed, follow these official guides:</p> <ul> <li>Installing Python</li> <li>Pip Installation Guide</li> </ul>"},{"location":"usage/llm_explanations/","title":"Generating LLM Explanations","text":"<p>EvalxNLP provides powerful tools to interpret explanation scores and evaluation metrics using natural language generated by Large Language Models (LLMs). These human-readable interpretations enhance the transparency and usability of XAI methods.</p> <p>The framework supports integration with Together AI's API, using the <code>meta-llama/Llama-3.3-70B-Instruct-Turbo-Free</code> model by default.</p>"},{"location":"usage/llm_explanations/#prerequisites","title":"\u2705 Prerequisites","text":"<ul> <li>Create an API key from Together AI.</li> <li>Pass your API key and model name to the <code>LLMExplanationGenerator</code> and/or <code>EvaluationMetricsExplainer</code> class(es)</li> </ul>"},{"location":"usage/llm_explanations/#understanding-explanation-scores","title":"\ud83d\udcd8 Understanding Explanation Scores","text":"<p>Use  <code>LLMExplanationGenerator</code> class to interpret importance scores generated from different explainers. <code>generate_and_save_explanations()</code> allows to obtain explanations and save descriptions in JSON, HTML, or both formats. Explanations are saved in the <code>results/explanations/</code> folder.</p> <p>You can also visualize the output within notebook cell using <code>display_explanations()</code> function.</p> <pre><code>from LLMExplanationGenerator import LLMExplanationGenerator\nimport pandas as pd\nfrom IPython.display import display, HTML\n\nmodel_name = \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\napi_key = \"YOUR_API_KEY_HERE\"\n\nexplainer = LLMExplanationGenerator(model_name=model_name, api_key=api_key)\n\nexplanations, saved_files = explainer.generate_and_save_explanations(\n    exps=exps,\n    output_format=\"both\"  # Choose from \"json\", \"html\", or \"both\"\n)\n\n# \ud83d\udcfa Display in notebook\nexplainer.display_explanations(explanations)\n</code></pre>"},{"location":"usage/llm_explanations/#understanding-evaluation-metrics","title":"\ud83d\udcca Understanding Evaluation Metrics","text":"<p>Use <code>EvaluationMetricsExplainer</code> class to interpret evaluation metrics. <code>explain_results()</code> generates explanations and <code>save_results()</code> saves the output in JSON, HTML, or both formats to <code>results/explanations/</code> folder. You can also visualize the output within notebook cell using <code>display_results()</code> function.</p> <pre><code>from EvaluationMetricsExplainer import EvaluationMetricsExplainer\n\napi_key = \"YOUR_API_KEY_HERE\"\nexplainer = EvaluationMetricsExplainer(api_key=api_key)\n\n# Provide results from evaluation functions\nresults = explainer.explain_results(metrics)\n\n# Save results to files\njson_path, html_path = explainer.save_results(results)\n\n# \ud83d\udda5\ufe0f Display in notebook\nexplainer.display_results(results)\n</code></pre> <p>\ud83d\udccd The generated reports include metric definitions, score interpretations, and comparisons across explainers in structured HTML or JSON format.</p>"}]}